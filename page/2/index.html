<!DOCTYPE html>
<html lang="en">
<head>

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no" />
<meta name="author" content="John Doe" />


    
    


<meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">

<link rel="apple-touch-icon" href= "/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">



<link rel="stylesheet" href="/css/style.css">



    <style> .article { opacity: 0;} </style>


<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>Hexo</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: 
    }
</script>


    <script> yiliaConfig.jquery_ui = [false]; </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






<meta name="generator" content="Hexo 7.3.0"></head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/avatar.jpg" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/"></a></h1>
        </hgroup>

        

        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>Menu</li>
                        <li>Tags</li>
                        
                        <li>Friends</li>
                        
                        
                        <li>About Me</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                            <li><a href="/categories/%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/">模型学习</a></li>
                        
                            <li><a href="/categories/%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/">优化技术</a></li>
                        
                            <li><a href="/categories/project/">小试牛刀</a></li>
                        
                            <li><a href="/archives/">归档</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ACGAN/" rel="tag">ACGAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/" rel="tag">BERT</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CycleGAN/" rel="tag">CycleGAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DCGAN/" rel="tag">DCGAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GAN/" rel="tag">GAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/WGAN/" rel="tag">WGAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/WGAN-GP/" rel="tag">WGAN-GP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/code/" rel="tag">code</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A8%A1%E5%9E%8B/" rel="tag">模型</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="tag">线性回归</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="http://moxfive.xyz/">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">专注于前端</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页"></a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/avatar.jpg" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页"></a></h1>
            </hgroup>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/about/">关于我</a></li>
                
                    <li><a href="/categories/%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/">模型学习</a></li>
                
                    <li><a href="/categories/%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/">优化技术</a></li>
                
                    <li><a href="/categories/project/">小试牛刀</a></li>
                
                    <li><a href="/archives/">归档</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="Tags" friends="Friends" about="About Me"/>
</nav>
      <div class="body-wrap">
  
    <article id="post-层归一化" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/23/%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96/" class="article-date">
      <time datetime="2025-03-23T10:37:26.000Z" itemprop="datePublished">2025-03-23</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/23/%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96/">层归一化</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/">优化技术</a>
    </div>


      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-批归一化" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/23/%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96/" class="article-date">
      <time datetime="2025-03-23T10:37:14.000Z" itemprop="datePublished">2025-03-23</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/23/%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96/">批归一化</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h3 id="批量正则化（Batch-Normalization）"><a href="#批量正则化（Batch-Normalization）" class="headerlink" title="批量正则化（Batch Normalization）"></a><strong>批量正则化（Batch Normalization）</strong></h3><p><strong>批量正则化（Batch Normalization，简称BN）</strong> 是一种对神经网络的输入数据进行规范化处理的方法，旨在加速网络训练、提高稳定性，并减少模型对初始化的依赖。在GAN中，批量正则化主要应用于生成器和判别器的训练过程中，以提高其收敛速度和训练的稳定性。</p>
<h4 id="原理："><a href="#原理：" class="headerlink" title="原理："></a><strong>原理</strong>：</h4><p>在深度神经网络的训练中，随着层数的增加，网络的输入可能会经历很多次的线性变换和非线性激活。这样会导致每一层的输入分布发生变化，这个现象叫做<strong>内部协变量偏移（Internal Covariate Shift）</strong>。为了避免这种偏移，<strong>批量正则化</strong>通过对每一层的输入进行标准化，使其均值为0，方差为1，从而加速训练并减少训练的不稳定性。</p>
<p>批量正则化的主要步骤是：</p>
<ol>
<li><strong>计算当前批次数据的均值和方差</strong>。</li>
<li><strong>用均值和方差对数据进行标准化</strong>，使得每个输入特征的均值为0，方差为1。</li>
<li><strong>使用可学习的缩放因子和偏移量</strong>，将标准化后的数据恢复到一定的分布范围内。</li>
</ol>
<p>公式上，批量正则化的过程是：</p>
<p>[<br>\hat{x} &#x3D; \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}<br>]<br>其中：</p>
<ul>
<li>( x ) 是当前批次的数据。</li>
<li>( \mu_B ) 和 ( \sigma_B^2 ) 分别是当前批次数据的均值和方差。</li>
<li>( \epsilon ) 是一个很小的常数，用于避免除以零的情况。</li>
</ul>
<p>然后，标准化后的数据会经过两个可学习的参数：<strong>缩放因子</strong>（( \gamma )）和<strong>偏移量</strong>（( \beta )），以便网络能够根据需要恢复原来的数据分布。</p>
<h4 id="在GAN中的作用："><a href="#在GAN中的作用：" class="headerlink" title="在GAN中的作用："></a><strong>在GAN中的作用</strong>：</h4><ul>
<li><strong>提高训练稳定性</strong>：由于批量正则化对输入数据进行了标准化，网络训练过程中的梯度更新更加稳定，避免了梯度消失或爆炸的问题。</li>
<li><strong>加速收敛</strong>：通过标准化输入数据，使得训练过程更快速地收敛，减少了学习率调节的难度。</li>
<li><strong>缓解模式崩溃</strong>：批量正则化有助于防止生成器只生成某一类样本的模式崩溃问题，因为它提供了更为平滑的训练过程，有助于生成器学习更多样的样本。</li>
</ul>
<p>然而，批量正则化在生成器中的使用会带来一些额外的挑战，尤其是在生成高分辨率图像时，它的效果并不总是理想。因此，研究者们提出了其他的优化方法，像**谱归一化（Spectral Normalization）**等。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/">优化技术</a>
    </div>


      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Wasserstein距离" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/23/Wasserstein%E8%B7%9D%E7%A6%BB/" class="article-date">
      <time datetime="2025-03-23T10:31:23.000Z" itemprop="datePublished">2025-03-23</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/23/Wasserstein%E8%B7%9D%E7%A6%BB/">Wasserstein距离</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h3 id="Wasserstein距离（Wasserstein-Distance）"><a href="#Wasserstein距离（Wasserstein-Distance）" class="headerlink" title="Wasserstein距离（Wasserstein Distance）"></a><strong>Wasserstein距离（Wasserstein Distance）</strong></h3><p><strong>Wasserstein距离</strong>，又被称为<strong>地球移动者距离（Earth Mover’s Distance, EMD）</strong>，是一种衡量两个概率分布之间“距离”的方法。直观来说，它可以看作是从一个分布到另一个分布移动“质量”的最小工作量。</p>
<h4 id="直观解释："><a href="#直观解释：" class="headerlink" title="直观解释："></a><strong>直观解释</strong>：</h4><p>假设你有两个概率分布，分别表示两个不同的“山脊”（或者“堆积”）。Wasserstein距离试图计算，把一堆质量从一个山脊移动到另一个山脊所需要的最小“成本”。其中，每个“成本”都是根据移动质量的距离来计算的。可以把它看作是把一堆土从一个地方搬到另一个地方所需要的最少工作量。</p>
<h4 id="公式："><a href="#公式：" class="headerlink" title="公式："></a><strong>公式</strong>：</h4><p>Wasserstein距离通常定义为<strong>最小传输成本</strong>。对于两个分布 (P) 和 (Q)，它的计算可以用下面的公式表示：</p>
<p>[<br>W(P, Q) &#x3D; \inf_{\gamma \in \Gamma(P, Q)} \mathbb{E}_{(x,y) \sim \gamma} [| x - y |]<br>]</p>
<p>其中：</p>
<ul>
<li>( \Gamma(P, Q) ) 是所有满足边际条件 (P) 和 (Q) 的联合分布（即对每个 (x) 和 (y) 在分布 (P) 和 (Q) 下的配对情况）。</li>
<li>( | x - y | ) 是计算在空间中从 (x) 到 (y) 的距离。</li>
</ul>
<p><strong>Wasserstein距离</strong>实际上衡量的是从一个分布到另一个分布的最小“成本”或“运输工作量”。</p>
<h4 id="优势："><a href="#优势：" class="headerlink" title="优势："></a><strong>优势</strong>：</h4><ul>
<li><strong>良好的梯度性质</strong>：与Kullback-Leibler散度等传统方法不同，Wasserstein距离在分布之间有更平滑的过渡。它不容易出现梯度消失问题，因此它在训练过程中更加稳定。</li>
<li><strong>可用性</strong>：Wasserstein距离不仅适用于连续分布，还可以适用于离散分布，这使得它在处理一些复杂的数据分布时特别有用。</li>
</ul>
<h4 id="在GAN中的应用："><a href="#在GAN中的应用：" class="headerlink" title="在GAN中的应用："></a><strong>在GAN中的应用</strong>：</h4><p>在生成对抗网络中，**Wasserstein GAN（WGAN）**引入了Wasserstein距离作为优化目标，解决了传统GAN中的训练不稳定问题。传统GAN在训练时存在判别器输出饱和和梯度消失的问题，而WGAN通过Wasserstein距离的引入，使得生成器和判别器的学习更加稳定，并且生成效果更好。</p>
<hr>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/">优化技术</a>
    </div>


      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-KL散度" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/23/KL%E6%95%A3%E5%BA%A6/" class="article-date">
      <time datetime="2025-03-23T10:31:11.000Z" itemprop="datePublished">2025-03-23</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/23/KL%E6%95%A3%E5%BA%A6/">KL散度</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h3 id="KL散度（Kullback-Leibler-Divergence）"><a href="#KL散度（Kullback-Leibler-Divergence）" class="headerlink" title="KL散度（Kullback-Leibler Divergence）"></a><strong>KL散度（Kullback-Leibler Divergence）</strong></h3><p><strong>KL散度</strong>，又叫<strong>Kullback-Leibler信息量散度</strong>，是一种衡量两个概率分布之间差异的度量。它定义了从分布 (Q) 到分布 (P) 的信息损失，或者说，如果我们用分布 (Q) 来近似真实分布 (P)，则产生的误差。</p>
<h4 id="直观解释："><a href="#直观解释：" class="headerlink" title="直观解释："></a><strong>直观解释</strong>：</h4><p>KL散度衡量的是使用概率分布 (Q) 来代替概率分布 (P) 时，所丧失的信息量。它并不是对称的，即 (D_{KL}(P \parallel Q)) 不等于 (D_{KL}(Q \parallel P))。可以把KL散度理解为，给定一个真实的分布 (P)，如果我们用分布 (Q) 来近似它，那么KL散度就是衡量这种近似误差的大小。</p>
<h4 id="公式："><a href="#公式：" class="headerlink" title="公式："></a><strong>公式</strong>：</h4><p>对于离散的概率分布 (P) 和 (Q)，KL散度的公式为：</p>
<p>[<br>D_{KL}(P \parallel Q) &#x3D; \sum_{i} P(i) \log \frac{P(i)}{Q(i)}<br>]</p>
<p>对于连续的概率分布 (P) 和 (Q)，KL散度的公式为：</p>
<p>[<br>D_{KL}(P \parallel Q) &#x3D; \int p(x) \log \frac{p(x)}{q(x)} , dx<br>]</p>
<p>其中：</p>
<ul>
<li>(P(i)) 和 (Q(i)) 分别是离散分布 (P) 和 (Q) 在样本点 (i) 上的概率。</li>
<li>(p(x)) 和 (q(x)) 是连续分布 (P) 和 (Q) 在样本点 (x) 上的概率密度。</li>
</ul>
<h4 id="优势："><a href="#优势：" class="headerlink" title="优势："></a><strong>优势</strong>：</h4><ul>
<li><strong>易于理解和计算</strong>：KL散度的计算相对简单，并且能够直接衡量分布之间的信息丧失。</li>
<li><strong>适用于实际应用</strong>：在许多实际机器学习任务中（例如变分推理和自编码器），KL散度经常作为目标函数来进行优化。</li>
</ul>
<h4 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a><strong>缺点</strong>：</h4><ul>
<li><strong>不对称</strong>：KL散度不对称，即 (D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P))，这可能会导致某些情况下的评估偏差。</li>
<li><strong>无法处理零概率问题</strong>：如果分布 (Q) 中某些事件的概率为零，而分布 (P) 中该事件的概率不为零，KL散度会出现无穷大，这在某些情况下会带来计算上的困难。</li>
</ul>
<h4 id="在GAN中的应用："><a href="#在GAN中的应用：" class="headerlink" title="在GAN中的应用："></a><strong>在GAN中的应用</strong>：</h4><p>KL散度在原始GAN的训练过程中并没有直接作为损失函数，但它与生成模型中的**变分自编码器（VAE）**相关。VAE在训练时使用KL散度来衡量潜在变量的分布与标准正态分布之间的差异，因此KL散度在生成模型中经常被用来优化潜在空间的结构。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/">优化技术</a>
    </div>


      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Inception-Score" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/23/Inception-Score/" class="article-date">
      <time datetime="2025-03-23T10:28:51.000Z" itemprop="datePublished">2025-03-23</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/23/Inception-Score/">Inception Score</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h3 id="Inception-Score-IS"><a href="#Inception-Score-IS" class="headerlink" title="Inception Score (IS)"></a><strong>Inception Score (IS)</strong></h3><p><strong>Inception Score</strong>（简称IS）是衡量生成模型（例如GAN）生成图像质量的一个重要指标。其核心思想是衡量生成图像的“清晰度”和“多样性”。</p>
<h4 id="计算方式："><a href="#计算方式：" class="headerlink" title="计算方式："></a><strong>计算方式</strong>：</h4><ul>
<li><strong>清晰度</strong>：生成的图像是否能被分类网络（通常是Inception网络）准确地分类。如果生成的图像属于某个类别并且网络能够自信地给出一个高概率，那么说明图像的内容清晰，容易被识别。</li>
<li><strong>多样性</strong>：如果生成的图像种类非常丰富，分类网络的输出应该会具有较大的分布差异。换句话说，生成图像的类别不应该过于集中在某一类。</li>
</ul>
<p>具体地，IS是通过以下步骤计算的：</p>
<ol>
<li>先通过Inception网络对生成的每一张图片进行分类，得到每张图片的预测概率分布。</li>
<li>然后计算这些预测概率分布的<strong>KL散度</strong>（Kullback-Leibler divergence），也就是衡量不同生成图像之间的多样性。</li>
<li>最后将这些多样性和图像的清晰度结合起来，得到最终的得分。</li>
</ol>
<h4 id="公式："><a href="#公式：" class="headerlink" title="公式："></a><strong>公式</strong>：</h4><p>Inception Score的计算公式为：</p>
<p>[<br>IS &#x3D; \exp\left(\mathbb{E}<em>x[D</em>{KL}(p(y|x) || p(y))]\right)<br>]</p>
<p>其中，(p(y|x)) 是给定生成图像 (x) 后的类别概率分布，(p(y)) 是所有生成图像类别的均匀分布。</p>
<h4 id="优点："><a href="#优点：" class="headerlink" title="优点："></a><strong>优点</strong>：</h4><ul>
<li>计算简单，直接利用了现有的预训练分类网络（如Inception v3）进行评估。</li>
<li>对生成图像的清晰度和多样性都有一定的衡量。</li>
</ul>
<h4 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a><strong>缺点</strong>：</h4><ul>
<li><strong>依赖Inception网络</strong>：Inception Score的好坏很大程度上取决于Inception网络的能力。网络的训练集、分类的能力和网络的泛化能力可能影响最终评估的准确性。</li>
<li><strong>局限性</strong>：IS没有直接考虑生成样本与真实数据之间的分布差异，可能导致一些细微的生成质量问题无法被反映出来。</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/">优化技术</a>
    </div>


      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-FID" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/23/FID/" class="article-date">
      <time datetime="2025-03-23T10:28:35.000Z" itemprop="datePublished">2025-03-23</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/23/FID/">FID</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h3 id="Frechet-Inception-Distance-FID"><a href="#Frechet-Inception-Distance-FID" class="headerlink" title="Fréchet Inception Distance (FID)"></a><strong>Fréchet Inception Distance (FID)</strong></h3><p><strong>FID</strong> 是一个更为复杂且更为精细的评估生成图像质量的指标。它的灵感来源于计算生成数据和真实数据在特征空间的距离。相较于Inception Score，FID更关注生成样本和真实样本之间的统计差异。</p>
<h4 id="计算方式："><a href="#计算方式：" class="headerlink" title="计算方式："></a><strong>计算方式</strong>：</h4><ul>
<li>通过Inception网络提取生成图像和真实图像的特征（通常是在网络的某一层中提取激活值，像是倒数第二层的输出）。</li>
<li>对这些特征进行统计建模，通常假设它们符合高斯分布。</li>
<li>然后计算<strong>生成图像特征和真实图像特征的Fréchet距离</strong>（即均值和协方差矩阵的差异）。</li>
</ul>
<p><strong>Fréchet距离</strong>本质上衡量的是两组数据的高斯分布之间的差异。FID越小，表示生成样本和真实数据越相似。</p>
<h4 id="FID公式："><a href="#FID公式：" class="headerlink" title="FID公式："></a><strong>FID公式</strong>：</h4><p>假设我们从生成图像和真实图像中分别得到特征均值和协方差：</p>
<ul>
<li>( \mu_r, \Sigma_r ) 是真实数据的均值和协方差。</li>
<li>( \mu_g, \Sigma_g ) 是生成数据的均值和协方差。</li>
</ul>
<p>那么，FID的计算公式为：</p>
<p>[<br>FID &#x3D; \left| \mu_g - \mu_r \right|^2 + \text{Tr}\left( \Sigma_g + \Sigma_r - 2 \left( \Sigma_g \Sigma_r \right)^{1&#x2F;2} \right)<br>]</p>
<p>其中，( \mu_g ) 和 ( \mu_r ) 是生成图像和真实图像的特征均值，( \Sigma_g ) 和 ( \Sigma_r ) 是它们的协方差矩阵。</p>
<h4 id="优点："><a href="#优点：" class="headerlink" title="优点："></a><strong>优点</strong>：</h4><ul>
<li><strong>更符合人类评判</strong>：相比于IS，FID更能捕捉生成图像与真实图像的高层次统计差异，更符合人类对图像质量的直觉感知。</li>
<li><strong>不依赖类别</strong>：FID不关心生成图像属于哪一类，而是看其是否能够生成真实数据分布中的多样性，这使得它比IS更为全面。</li>
</ul>
<h4 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a><strong>缺点</strong>：</h4><ul>
<li><strong>计算更复杂</strong>：FID需要计算特征的均值和协方差，计算过程比Inception Score要复杂。</li>
<li><strong>依赖于Inception网络</strong>：虽然FID对生成图像的评价比IS更为全面，但它仍然依赖于一个预训练的Inception网络。如果该网络没有覆盖到某些特定的图像类型，可能会影响评估的准确性。</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/">优化技术</a>
    </div>


      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-关于-线性回归-的笔记与思考" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/23/%E5%85%B3%E4%BA%8E-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E7%9A%84%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%80%9D%E8%80%83/" class="article-date">
      <time datetime="2025-03-23T10:03:13.000Z" itemprop="datePublished">2025-03-23</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/23/%E5%85%B3%E4%BA%8E-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E7%9A%84%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%80%9D%E8%80%83/">关于 线性回归 的笔记与思考</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/">模型学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%A8%A1%E5%9E%8B/" rel="tag">模型</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="tag">线性回归</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-关于-GAN-及其衍生模型的笔记与思考" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/23/%E5%85%B3%E4%BA%8E-GAN-%E5%8F%8A%E5%85%B6%E8%A1%8D%E7%94%9F%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%80%9D%E8%80%83/" class="article-date">
      <time datetime="2025-03-23T10:01:06.000Z" itemprop="datePublished">2025-03-23</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/23/%E5%85%B3%E4%BA%8E-GAN-%E5%8F%8A%E5%85%B6%E8%A1%8D%E7%94%9F%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%80%9D%E8%80%83/">关于 GAN 及其衍生模型的笔记与思考</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="关于-GAN"><a href="#关于-GAN" class="headerlink" title="关于 GAN"></a>关于 GAN</h1><p><em>从单模型到 <strong>“左右互搏术”</strong> 的对抗式模型，<strong>GAN</strong> 的对抗式的思路注定了它的革命性意义</em></p>
<h3 id="GAN之前的思路"><a href="#GAN之前的思路" class="headerlink" title="GAN之前的思路"></a>GAN之前的思路</h3><p>在 <strong>GAN</strong> 的横空出世之前，生成模型的研究主要依赖于传统的概率图模型和最大似然估计等方法。生成模型的基本目标是通过学习训练数据的分布，来生成类似于训练数据的新样本。</p>
<p>但在当时，主流的生成模型往往依赖于一种显式建模的方式，通常使用自回归模型（如HMM、VAE等），通过明确的概率分布和参数估计进行训练。虽然这些方法可以实现某些生成任务，但始终存在一些局限性：</p>
<ol>
<li>生成样本的质量不高：自回归模型常常生成模糊、不清晰的样本。</li>
<li>难以捕捉数据的复杂性：例如，图像生成任务中，数据的高维度特性使得这些方法很难有效捕捉到所有复杂的细节。</li>
</ol>
<p>在这些挑战面前，生成模型的研究面临瓶颈，直到GAN的问世，一场生成模型的革命才悄然拉开帷幕。</p>
<h3 id="GAN的思路"><a href="#GAN的思路" class="headerlink" title="GAN的思路"></a>GAN的思路</h3><p><em>深谙“左右互搏术”，G_model 与 D_model 既是对手，亦是良友</em></p>
<p><strong>GAN</strong> 的核心思想是 <strong>“对抗”</strong>，生成器和判别器在对抗式的博弈中 <strong>相互优化</strong>。</p>
<p>生成器的目标是产生尽可能逼真的假样本，尽量“骗过”判别器；而判别器的目标是区分生成样本和真实样本。两者通过不断“斗智斗勇”的过程，最终达到一种平衡：生成器能够生成与真实样本几乎无差别的图像，而判别器无法轻易分辨。</p>
<p>这一创新的思路彻底改变了生成模型的训练方式，从参数化的显式建模转向了隐式的对抗博弈。这种“左右互搏”的博弈式优化，不仅在图像生成、视频生成等领域取得了突破性进展，也为其他机器学习任务提供了新的启示。</p>
<p>GAN的优点：</p>
<ol>
<li>生成能力强大：与传统生成模型相比，GAN能够生成更加逼真和复杂的数据。</li>
<li>无需明确建模分布：生成器通过与判别器的对抗训练，不需要事先假设数据分布，因此可以更灵活地适应复杂的数据集。</li>
<li>适应性广泛：无论是图像、音频还是文本，GAN都可以被应用到各类生成任务中，展现出了超强的泛化能力。</li>
</ol>
<p>但尽管GAN如此强大，其训练过程中却充满了挑战，这也为后续的研究带来了许多值得思考的问题。</p>
<h3 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h3><p>尽管GAN具有强大的生成能力，但其训练和调优相对复杂，存在以下挑战：​</p>
<ol>
<li><p><strong>模式崩溃（Mode Collapse）</strong><br><em>为了成功逃避判别器的严厉审查，生成器将为了成功而成功</em><br><em>趋利避害，本能也</em></p>
<ul>
<li>如果有一天，不需要学习很多知识，扩展自己的能力，只需要每天做着简单，轻松，重复的劳动，就可以获得名誉和金钱，万人瞩目，那么绝大多数人将趋之若鹜，无人扩展自己的能力。</li>
<li>如果生成器只需要成功的模拟出一种图像的逼真生成，骗过判别器，那么生成器也会“懒得”去学习其他种类图像的生成逻辑，只会生成这一种来糊弄判别器。</li>
</ul>
<p>这就是 <strong>模式崩溃</strong></p>
<blockquote>
<p>模型崩溃是指机器学习模型由于在另一个模型（包括其自身的先前版本）的输出上进行未经整理的训练而产生错误，从而逐渐退化的一种现象。<br> Shumailov 等人 创造了这个术语，并描述了退化的两个具体阶段：早期模型崩溃和晚期模型崩溃。在早期模型崩溃阶段，模型开始丢失分布尾部的信息–主要影响少数数据。后来的研究强调，早期模型崩溃很难察觉，因为整体性能可能看起来有所改善，而模型在少数数据上的性能却有所下降。<br>         ——来自维基百科</p>
</blockquote>
<p> 也就是说，​生成器可能只能生成有限种类的数据，而忽略了数据集中的其他多样性。</p>
<p> 为了解决这一问题，后续研究者提出了许多改进方法，如使用批量正则化或采用多生成器架构。​</p>
</li>
<li><p><strong>训练不稳定：</strong></p>
<p> <em>为了与判别器斗智斗勇，生成器不得不全力以赴，但判别器的挑剔和苛刻，使得整个训练过程充满了矛盾与张力。</em></p>
<ul>
<li>假设你每天都要与一位非常严格的评审竞争，如何让他相信你的作品既完美又无可挑剔？但问题是，评审的标准变幻莫测，且总是在不断提高。你试图改进，但他总是能找出新的瑕疵，甚至有时会让你陷入困境，难以找到一个合适的平衡点。最终，你可能陷入一场无休止的噩梦中，无法突破。</li>
</ul>
<p> 这正是生成器和判别器之间的关系。生成器希望产生尽可能接近真实数据的假样本，而判别器则不断提高自己的标准，试图识别这些假样本。两者的竞争如果没有良好的平衡，可能会导致训练过程的不稳定。训练可能会早期收敛，但结果却远非理想，生成器并没有学到足够的生成策略，或者根本没有学到如何应对判别器的挑战。</p>
<p> 训练不稳定是GAN训练中的一个普遍问题。为了避免生成器和判别器之间的学习速率失衡，研究者提出了WGAN（Wasserstein GAN），它通过引入 <strong>“Wasserstein距离”</strong> 来缓解这种不稳定性，使得优化过程更加平滑且容易收敛。此外，合理的超参数调整，尤其是学习率和优化器选择，也能在一定程度上改善这一问题，但是这往往是漫长的尝试。</p>
</li>
<li><p><strong>难以评估：</strong><br> <em>生成器的目标是创造看似真实的样本，而判别器的职责是让生成的样本无法再隐藏在真实数据的“伪装”下。但这场较量最终的标准究竟是什么？</em></p>
<ul>
<li>假设你是一个画家，创作了一幅作品，你的作品看起来栩栩如生。现在，评审团的任务是评判你作品的艺术价值。问题是，他们无法简单通过传统的评分标准来评价你的作品，因为艺术的美学标准常常难以量化。是否只通过他们的主观评价，还是找到一种能量化的工具来衡量作品的质量呢？</li>
</ul>
<p>在GAN中，评估生成模型的质量也是一个复杂的问题。生成器的目标是生成看起来非常真实的数据，而判别器则试图分辨出这些“假冒伪劣”的样本。因此，传统的损失函数难以准确地评估生成器的表现，因为损失函数可能无法捕捉到生成数据与真实数据之间微妙的差别。想象你在画布上做出的每一笔都应该尽可能与真实世界的数据匹配，如何保证每一笔都完美无瑕？</p>
<p>为了解决这个问题，研究者们提出了一些新的评估方法，如 <strong>Fréchet Inception Distance（FID）</strong>，它通过比较生成样本和真实样本在 Inception 模型特征空间中的分布差异来量化生成数据的质量。这种方法能够 <strong>更加客观</strong> 地衡量生成样本与真实数据之间的距离，避免仅依赖人工评估的主观性。<br>除此之外，还有诸如 <strong>Inception Score</strong> 等评估方法，试图用更精细的方式来捕捉生成样本的质量。</p>
</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>GAN 通过引入生成器与判别器的 <strong>对抗式博弈</strong>，极大推动了生成模型的进步，使得机器能够生成几乎与真实数据无异的样本。然而，这种创新的思路也带来了新的挑战：模式崩溃、训练不稳定、评估困难等问题，成为GAN进一步发展的瓶颈。</p>
<p>但正是这些挑战推动了GAN及其变种模型（如WGAN、WGAN-GP等）的不断演进，解决方案也在逐步落地。<br>从 <strong>Wasserstein距离</strong> 到 <strong>Fréchet Inception Distance</strong>，从 <strong>批量正则化</strong> 到 <strong>多生成器架构</strong>，我们已经看到研究者们为克服这些问题所做出的巨大努力。</p>
<p>随着这些挑战的逐步解决，GAN无疑将在未来的机器学习领域中继续发挥其巨大的潜力。</p>
<hr>
<h1 id="关于-GAN-的衍生模型"><a href="#关于-GAN-的衍生模型" class="headerlink" title="关于 GAN 的衍生模型"></a>关于 GAN 的衍生模型</h1><h3 id="DCGAN"><a href="#DCGAN" class="headerlink" title="DCGAN"></a>DCGAN</h3><p><em>创新的 <strong>GAN</strong> + 卓越的 <strong>CNN</strong> &#x3D; 创新卓越的<strong>DCGAN</strong></em></p>
<p>从简单的全连接神经网络架构走向了更复杂的深度卷积网络架构</p>
<p>DCGAN通过深度卷积网络增强了生成器与判别器的能力，尤其是生成器的表现得到了极大改进。相比于传统GAN，DCGAN用卷积层替代了全连接层，这一改动使得生成器能够有效捕捉到图像的空间结构和细节特征。<br>DCGAN的一个关键创新就是使用 <strong>去卷积（Deconvolution）操作</strong>，让生成器能够从潜在空间映射到高维数据空间。</p>
<p>效果：</p>
<ol>
<li><strong>图像生成质量提高：</strong> 通过卷积结构的引入，DCGAN能够生成更加清晰、自然的图像，尤其在面部图像生成和自然场景图像生成方面取得了突破。</li>
<li><strong>训练更为稳定：</strong> DCGAN相比传统GAN，训练过程中的稳定性得到了大幅提升，减少了许多困扰传统GAN的梯度消失问题。</li>
</ol>
<h3 id="ACGAN"><a href="#ACGAN" class="headerlink" title="ACGAN"></a>ACGAN</h3><p><em>我不仅让你生成，我还得让你知道你生成的是什么</em></p>
<p>ACGAN的提出，是为了让生成器的输出不再局限于仅生成真实感的图像，而是能够在生成图像的同时，控制图像的类别或标签。这一创新解决了传统GAN在生成任务中 <strong>缺乏可控性的问题</strong>。</p>
<p><strong>ACGAN</strong> 在原始 <strong>GAN</strong> 的基础上，引入了一个 <strong>辅助分类器</strong>，生成器不仅根据随机噪声生成图像，同时也根据附加的类别标签生成特定类型的图像。<br>判别器则变得更加复杂，它不仅需要判断样本的真实性，还需要预测样本的类别。这种设计使得 <strong>ACGAN</strong> 能够在生成的过程中引入条件信息，从而控制生成图像的标签。</p>
<p>效果：</p>
<ol>
<li><strong>多样性控制：</strong> ACGAN使得生成器能够根据输入的类别标签生成对应的图像，广泛应用于有标签数据的生成任务，例如生成特定类别的动物、植物图像等。</li>
<li><strong>提升了生成样本的可控性：</strong> 生成器不仅追求图像的真实性，也能够有效地根据需求生成多种不同类型的样本。</li>
</ol>
<h3 id="WGAN"><a href="#WGAN" class="headerlink" title="WGAN"></a>WGAN</h3><p><em>你这数学原理有bug ！</em></p>
<p><strong>WGAN</strong> 的提出，打破了传统GAN在训练过程中经常出现的 <strong>梯度消失与训练不稳定问题</strong>。</p>
<p>WGAN的核心创新在于引入了 <strong>Wasserstein距离</strong>，代替了传统GAN中使用的 <strong>JS散度</strong>。Wasserstein距离具有更好的数学性质，能够提供更稳定的训练信号，特别是在生成分布与真实分布差异较大时，它能够避免出现梯度消失的情况。同时，WGAN的判别器不再是二分类器，而是一个判别评分器，用于衡量样本的真实性。</p>
<p>为了优化训练过程，WGAN采用了 <strong>权重剪切技术</strong>，将判别器的权重限制在一定范围内，避免了权重过大导致的训练不稳定。</p>
<p>效果：</p>
<ol>
<li><strong>训练过程更平稳：</strong> 引入Wasserstein距离的WGAN，不仅训练过程更加稳定，而且能够应对更复杂的生成任务，特别是在生成高质量图像时表现优异。</li>
<li><strong>解决了梯度消失问题：</strong> WGAN通过Wasserstein距离有效缓解了GAN中常见的梯度消失问题，使得生成器和判别器的优化过程更加顺畅。</li>
</ol>
<h3 id="WGAN-GP"><a href="#WGAN-GP" class="headerlink" title="WGAN-GP"></a>WGAN-GP</h3><p><em>粗暴的裁剪是比不上优雅的梯度惩罚的</em></p>
<p><strong>WGAN-GP</strong> 是对WGAN的一种优化，它引入了<strong>梯度惩罚</strong>（Gradient Penalty）机制，取代了WGAN中的权重剪切。梯度惩罚的加入进一步提升了模型的稳定性，并解决了WGAN中权重剪切可能带来的副作用。</p>
<p>WGAN-GP通过对判别器的梯度进行惩罚，确保其梯度的平滑性。与WGAN中的权重剪切不同，梯度惩罚使得优化过程更加细致，能够避免生成器和判别器之间的不平衡。其损失函数中加入了梯度惩罚项：<br>[<br>L &#x3D; D(x) - D(G(z)) + \lambda \cdot \mathbb{E}[\left( |\nabla_{\hat{x}} D(\hat{x})|_2 - 1 \right)^2]<br>]</p>
<p>其中，(\hat{x}) 是生成器输出的线性插值样本，(\lambda) 为梯度惩罚的权重。</p>
<p>效果：</p>
<ol>
<li><strong>进一步提升训练稳定性</strong>：通过梯度惩罚，WGAN-GP避免了权重剪切可能带来的负面影响，进一步提升了训练过程的稳定性。</li>
<li><strong>高质量生成</strong>：WGAN-GP生成的样本质量更高，尤其在图像生成领域表现出色，能够生成细节更加丰富、逼真度更高的图像。</li>
</ol>
<h3 id="CycleGAN"><a href="#CycleGAN" class="headerlink" title="CycleGAN"></a>CycleGAN</h3><p><em>循环交叉，交叉循环</em></p>
<p>CycleGAN 作为一种无监督学习的生成对抗网络，特别适用于图像到图像的转换任务，而无需成对的数据。CycleGAN通过引入循环一致性损失，使得模型能够在没有标签数据的情况下，实现不同领域之间的图像转换。</p>
<p>CycleGAN使用两个生成器和两个判别器。生成器一负责将源域图像转换为目标域图像，另一个生成器则将目标域图像转换回源域图像。关键在于循环一致性损失，通过确保转换回来的图像能够尽可能还原原图，保证生成图像的质量和一致性。</p>
<p>其目标是：</p>
<ol>
<li>生成器G将源域图像转换为目标域图像。</li>
<li>生成器F将目标域图像转换为源域图像。</li>
<li>循环一致性损失,保证 $𝐺(𝐹(𝑥))≈𝑥$ 和 $𝐹(𝐺(𝑦))≈𝑦$</li>
</ol>
<p>效果：</p>
<ol>
<li><strong>无监督图像转换：</strong> CycleGAN可以在没有成对数据的情况下，进行风格迁移、图像合成等任务，应用广泛。</li>
<li><strong>图像合成和风格迁移：</strong> 无论是将夏季图像转换为冬季图像，还是将一张照片转换为油画风格，CycleGAN都能表现出色。</li>
</ol>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/">模型学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GAN/" rel="tag">GAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%A8%A1%E5%9E%8B/" rel="tag">模型</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-关于-Transformer-的笔记与思考" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/23/%E5%85%B3%E4%BA%8E-Transformer-%E7%9A%84%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%80%9D%E8%80%83/" class="article-date">
      <time datetime="2025-03-23T09:59:01.000Z" itemprop="datePublished">2025-03-23</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/23/%E5%85%B3%E4%BA%8E-Transformer-%E7%9A%84%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%80%9D%E8%80%83/">关于 Transformer 的笔记与思考</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="关于-Transformer"><a href="#关于-Transformer" class="headerlink" title="关于 Transformer"></a>关于 Transformer</h1><p><em>上下文理解大师</em></p>
<p>人类理解一句话里的一个词的时候，绝对不会脱离了文本，一定是结合上下文，才能分得清主谓宾，定状补，一词多义，熟词生义，阅读理解，完形填空……这种全局性的理解，正是 Transformer 架构的精髓所在。</p>
<p>Transformer 模型由编码器和解码器两部分组成，核心在于完全基于 <strong>自注意力机制</strong>。它让模型在处理某个词汇时，关注输入序列中的所有其他词汇，从而捕捉全局依赖关系。这就像是在读小说时，你不仅关注当前章节，还时刻留意其他章节的剧情，以获得全面的理解。</p>
<p>摒弃了传统的 CNN、RNN 的结构（虽然后续研究发现 Transformer 和 CNN、RNN 从某种意义上是等价的），Transformer 可以利用可以并行计算的特性，大幅度提升了计算处理的速度。</p>
<h4 id="Transformer架构设计"><a href="#Transformer架构设计" class="headerlink" title="Transformer架构设计"></a>Transformer架构设计</h4><p><img src="/img/Transformer.png" alt="Transformer的架构"></p>
<p>Transformer 的特殊架构（如上图）主要由以下几个重要组成部分：</p>
<ol>
<li><p><strong>输入嵌入（Input Embedding）</strong><br>传统的模型无法直接处理离散数据（如单词或字符 ID），因此需要将输入的每个 token（例如单词）映射为连续的向量。这个映射过程通过 <strong>输入嵌入矩阵</strong> 实现，最终让模型能够理解每个单词的语义。通过训练，嵌入矩阵会自动调整，使得词向量之间的相似度反映其语义上的接近程度。</p>
</li>
<li><p><strong>位置编码（Positional Encoding）</strong><br>由于 Transformer 完全摒弃了 RNN 和 CNN 的序列结构，它无法像传统模型那样自然而然地获得序列中单词的位置信息。因此，Transformer 引入了位置编码，通过正弦和余弦函数为每个单词的向量加上唯一的位置信息。这种方式可以帮助模型识别词汇在序列中的位置顺序，使得模型能够理解句子的结构。</p>
</li>
<li><p><strong>自注意力机制（Self-Attention）</strong><br>自注意力机制是 Transformer 的核心，它使得模型在处理每个单词时，能够同时考虑到输入序列中其他所有单词的影响。在每个 token 的表示中，不仅包含了当前位置的单词信息，还包括了整个句子中相关词汇的信息。通过自注意力机制，模型能够学习到更丰富的上下文关系。</p>
</li>
<li><p><strong>多头注意力机制（Multi-Head Attention）</strong><br>多头注意力的引入使得 Transformer 能够从多个子空间（多个角度）并行地进行注意力计算，增强了模型在学习不同类型依赖关系方面的能力。它让模型不仅仅关注到局部的关系，而是能够从多个层次去理解输入序列。</p>
</li>
<li><p><strong>前馈神经网络（Feed-Forward Network）</strong><br>在每个编码器和解码器的层中，除了自注意力机制外，还会有一个前馈神经网络，它主要由两个线性变换和一个非线性激活函数（如 ReLU）构成。通过这种方式，模型能够捕捉到更复杂的特征表示。</p>
</li>
<li><p><strong>残差连接和层归一化（Residual Connection &amp; Layer Normalization）</strong><br>为了防止深层网络中的梯度消失或爆炸问题，Transformer 在每一层都采用了 <strong>残差连接</strong> 和 <strong>层归一化</strong>。残差连接确保信息在网络中的有效传递，而层归一化帮助模型的训练更稳定、收敛更快。</p>
</li>
</ol>
<h4 id="Transformer的优势"><a href="#Transformer的优势" class="headerlink" title="Transformer的优势"></a>Transformer的优势</h4><ol>
<li><strong>并行计算</strong>：与传统的 RNN 和 CNN 不同，Transformer 可以一次性处理整个序列，大大提高了计算效率。</li>
<li><strong>长距离依赖建模</strong>：通过自注意力机制，Transformer 能够捕捉到序列中任意两位置之间的依赖关系，从而解决了 RNN 无法有效建模长距离依赖的问题。</li>
<li><strong>高效训练</strong>：由于 Transformer 结构的简洁性和并行性，它能够大幅提高训练的速度，尤其是在处理大规模数据时。</li>
</ol>
<p>Transformer 模型的核心优势在于能够通过自注意力机制捕捉长距离的依赖关系，并且通过并行计算极大地提升了处理速度。这使得它成为了自然语言处理（NLP）领域的革命性突破，并成为许多现代预训练语言模型（如 BERT、GPT）架构的基础。</p>
<h1 id="关于-Transformer-的变体"><a href="#关于-Transformer-的变体" class="headerlink" title="关于 Transformer 的变体"></a>关于 Transformer 的变体</h1><h3 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h3><p><em>你做完型填空的时候也不会只看一边，对吧</em></p>
<p>假如现在的任务目标不再是翻译，而是纯粹的语言理解（语义情感分析，问答），那么完全不需要 <strong>Decoder</strong> ，只需要 <strong>Encoder</strong> 就已经能够充分完成任务了。</p>
<p><strong>BERT</strong> 基于 <strong>Transformer</strong> 的编码器部分，完全摒弃了解码器，主要用于理解输入序列的上下文信息。BERT 的输入是一个句子或一对句子，它通过以下两种方式来获取丰富的上下文信息：</p>
<ol>
<li><strong>Masked Language Model（MLM）：</strong><br> 为了进行预训练，<strong>BERT</strong> 会随机遮蔽输入中的一部分单词，通过上下文信息来预测这些被遮蔽的单词。这种训练方式确保模型能有效学习到每个词在上下文中的角色。主要目标是学会词元之间的关系。</li>
<li><strong>Next Sentence Prediction（NSP）：</strong><br> <strong>BERT</strong> 还通过预测句子间的关系来进一步增强理解能力。它随机选取两个句子，判断第二个句子是否是第一个句子的下一句，从而训练模型捕捉句子间的语义关系。主要目标是学会句间的关系。</li>
</ol>
<p>BERT的独特之处还在于其 <strong>双向编码能力</strong>。</p>
<p>既然是纯碎的语言理解，那么就 <strong>没必要单向阅读</strong> 了吧，所以这里引入了 <strong>双向理解</strong> 的思路，这样显然而且试验证明的确能提高模型性能。</p>
<p>传统的语言模型（如 GPT）是单向的，只能从左到右或从右到左理解文本，而 BERT 通过双向的方式同时捕捉前后文的关系，从而更好地理解词汇的含义。</p>
<h2 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h2><p><em>你说话的时候总得想着你上一句是什么吧</em></p>
<p>理解一句话的意思不仅仅是拼凑单词的意义，而是要在理解词汇的同时，还能生成合乎逻辑且富有创意的内容。这正是 <strong>GPT（Generative Pretrained Transformer）</strong> 模型的核心能力。它不仅能理解输入的文本，还能在此基础上进行创造性地生成输出。</p>
<p>GPT 是基于 Transformer 架构的一个衍生模型，最重要的特点是完全依赖 <strong>自回归模型</strong>，即通过前文的单词逐步生成后续单词，这使得它在生成连贯且自然的文本时具有无与伦比的优势。</p>
<p>和 <strong>BERT</strong> 相对，<strong>GPT</strong> 反而是只聚焦于 <strong>解码器</strong>，它通过连续生成预测下一个词汇来实现文本生成。在每一个词生成之前，都要把上文已经有的文本进行处理，经过若干解码器的堆叠处理，然后得到最可能的下一个词，如此循环往复，最终得到完整的生成文本。</p>
<p>GPT 的关键特点是自回归生成过程。它从一个初始的输入开始，逐步预测下一个词汇，然后把这个词汇作为新的输入加入到序列中继续预测下一个词汇，直到生成完整的句子或段落。</p>
<p>GPT 的成功在于通过大量数据的预训练，使得模型能够在生成文本时，不仅理解词汇，还能够创作出合理且自然的语言。这让它成为当前自然语言处理领域的重要突破之一。</p>
<h2 id="Reformer"><a href="#Reformer" class="headerlink" title="Reformer"></a>Reformer</h2><p><em>深度压缩与效率的化身</em></p>
<p>当句子一长，token增多，那么如果对于每一个 token 都需要对其他所有的 token 计算注意力得分，这显然是一个相当低效的过程。</p>
<p>其实想一下，对于每一个 token 而言，对于它比较重要的 token 也没多少，其实不需要对于其他 <strong>每一个</strong> token <strong>都</strong> 计算注意力（计算出来的注意力得分权重为0.0000000000001要你有啥用）。如果能将相似注意力的 token 放在一起，只在他们之间计算注意力得分，然后加权求和，就很好的缩小了问题的规模，这就是 <strong>Reformer</strong> 了</p>
<p><strong>Reformer架构设计</strong><br>Reformer 的设计理念是将 <strong>注意力机制</strong> 和 <strong>压缩存储</strong> 结合起来，从而使得计算效率和内存使用得到了极大的提升。它的关键创新点包括：</p>
<ol>
<li>局部敏感哈希<br>Reformer用 <strong>局部敏感哈希（LSH）</strong> 来替代传统的 <strong>全局</strong> 自注意力计算。这种方式将注意力计算限制在邻近的词汇之间，避免了传统自注意力计算中计算量过大的问题，从而有效减少了计算复杂度。</li>
<li>可逆残差网络<br>为了降低内存占用，Reformer 引入了 <strong>可逆残差网络</strong> 的概念。这意味着在计算时，<strong>不需要保留中间层的所有输出</strong>，而是通过反向传递过程来恢复它们，这大大减少了内存的使用。</li>
<li>分块计算<br>Reformer 对输入数据进行分块处理，每次只处理小块的局部信息，而不是一次性处理整个序列。这样可以进一步降低计算开销，尤其是在处理超大规模数据时。</li>
</ol>
<p>有了如上的改进，可以在保证原 Transformer 的性能的基础上：</p>
<ul>
<li>更低的内存消耗：通过局部敏感哈希和可逆残差网络，Reformer 在处理大规模数据时显著减少了内存使用，提升了训练效率。</li>
<li>高效处理长序列：Reformer 在处理长序列时，通过分块计算避免了全局计算的瓶颈，使得它能够在有限资源下处理更长的文本。</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/">模型学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%A8%A1%E5%9E%8B/" rel="tag">模型</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-测试博客" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/23/%E6%B5%8B%E8%AF%95%E5%8D%9A%E5%AE%A2/" class="article-date">
      <time datetime="2025-03-23T07:52:13.000Z" itemprop="datePublished">2025-03-23</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/23/%E6%B5%8B%E8%AF%95%E5%8D%9A%E5%AE%A2/">测试博客</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="Profile"><a href="#Profile" class="headerlink" title="Profile"></a>Profile</h1><p>这是我的 <code>hexo</code> 测试网页</p>
<h1 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h1><ul>
<li>Name: 张淞博</li>
<li>Age: 18</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2025 John Doe
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="A fast, simple &amp; powerful blog framework">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="Another simple and elegant theme for Hexo  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="Site Visitors"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="Page Hits"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





<div class="scroll" id="scroll">
    <a href="#" title="Back to Top"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="Comments"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="Go to Bottom"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>