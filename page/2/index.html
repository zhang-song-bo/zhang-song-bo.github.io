<!DOCTYPE html>
<html lang="en">
<head>

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no" />
<meta name="author" content="John Doe" />


    
    


<meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">

<link rel="apple-touch-icon" href= "/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">



<link rel="stylesheet" href="/css/style.css">



    <style> .article { opacity: 0;} </style>


<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>Hexo</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: 
    }
</script>


    <script> yiliaConfig.jquery_ui = [false]; </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






<meta name="generator" content="Hexo 7.3.0"></head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/avatar.jpg" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/"></a></h1>
        </hgroup>

        

        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>Menu</li>
                        <li>Tags</li>
                        
                        <li>Friends</li>
                        
                        
                        <li>About Me</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                            <li><a href="/categories/%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/">模型学习</a></li>
                        
                            <li><a href="/categories/%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/">优化技术</a></li>
                        
                            <li><a href="/categories/project/">小试牛刀</a></li>
                        
                            <li><a href="/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E8%AE%B0/">生活随记</a></li>
                        
                            <li><a href="/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/">论文学习</a></li>
                        
                            <li><a href="/archives/">归档</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ACGAN/" rel="tag">ACGAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/" rel="tag">BERT</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CycleGAN/" rel="tag">CycleGAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DCGAN/" rel="tag">DCGAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GAN/" rel="tag">GAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Multi-Agent/" rel="tag">Multi-Agent</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/" rel="tag">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TF-IDF/" rel="tag">TF-IDF</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/WGAN/" rel="tag">WGAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/WGAN-GP/" rel="tag">WGAN-GP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/agent/" rel="tag">agent</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/code/" rel="tag">code</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pix2pix/" rel="tag">pix2pix</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/" rel="tag">优化技术</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96/" rel="tag">关键词提取</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/" rel="tag">分类模型</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/" rel="tag">分类算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%BE%E5%83%8F%E7%BF%BB%E8%AF%91/" rel="tag">图像翻译</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%BE%E5%83%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/" rel="tag">图像风格迁移</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" rel="tag">支持向量机</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90/" rel="tag">文本分析</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A8%A1%E5%9E%8B/" rel="tag">模型</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="tag">线性回归</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" rel="tag">逻辑回归</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="http://moxfive.xyz/">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">专注于前端</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页"></a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/avatar.jpg" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页"></a></h1>
            </hgroup>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/about/">关于我</a></li>
                
                    <li><a href="/categories/%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/">模型学习</a></li>
                
                    <li><a href="/categories/%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/">优化技术</a></li>
                
                    <li><a href="/categories/project/">小试牛刀</a></li>
                
                    <li><a href="/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E8%AE%B0/">生活随记</a></li>
                
                    <li><a href="/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/">论文学习</a></li>
                
                    <li><a href="/archives/">归档</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="Tags" friends="Friends" about="About Me"/>
</nav>
      <div class="body-wrap">
  
    <article id="post-关于-Transformer-的笔记与思考" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/20/%E5%85%B3%E4%BA%8E-Transformer-%E7%9A%84%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%80%9D%E8%80%83/" class="article-date">
      <time datetime="2025-03-20T09:59:01.000Z" itemprop="datePublished">2025-03-20</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/20/%E5%85%B3%E4%BA%8E-Transformer-%E7%9A%84%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%80%9D%E8%80%83/">关于 Transformer 的笔记与思考</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="关于-Transformer"><a href="#关于-Transformer" class="headerlink" title="关于 Transformer"></a>关于 Transformer</h1><p><em>上下文理解大师</em></p>
<p>人类理解一句话里的一个词的时候，绝对不会脱离了文本，一定是结合上下文，才能分得清主谓宾，定状补，一词多义，熟词生义，阅读理解，完形填空……这种全局性的理解，正是 Transformer 架构的精髓所在。</p>
<p>Transformer 模型由编码器和解码器两部分组成，核心在于完全基于 <strong>自注意力机制</strong>。它让模型在处理某个词汇时，关注输入序列中的所有其他词汇，从而捕捉全局依赖关系。这就像是在读小说时，你不仅关注当前章节，还时刻留意其他章节的剧情，以获得全面的理解。</p>
<p>摒弃了传统的 CNN、RNN 的结构（虽然后续研究发现 Transformer 和 CNN、RNN 从某种意义上是等价的），Transformer 可以利用可以并行计算的特性，大幅度提升了计算处理的速度。</p>
<h4 id="Transformer架构设计"><a href="#Transformer架构设计" class="headerlink" title="Transformer架构设计"></a>Transformer架构设计</h4><p><img src="/img/Transformer.png" alt="Transformer的架构"></p>
<p>Transformer 的特殊架构（如上图）主要由以下几个重要组成部分：</p>
<ol>
<li><p><strong>输入嵌入（Input Embedding）</strong><br>传统的模型无法直接处理离散数据（如单词或字符 ID），因此需要将输入的每个 token（例如单词）映射为连续的向量。这个映射过程通过 <strong>输入嵌入矩阵</strong> 实现，最终让模型能够理解每个单词的语义。通过训练，嵌入矩阵会自动调整，使得词向量之间的相似度反映其语义上的接近程度。</p>
</li>
<li><p><strong>位置编码（Positional Encoding）</strong><br>由于 Transformer 完全摒弃了 RNN 和 CNN 的序列结构，它无法像传统模型那样自然而然地获得序列中单词的位置信息。因此，Transformer 引入了位置编码，通过正弦和余弦函数为每个单词的向量加上唯一的位置信息。这种方式可以帮助模型识别词汇在序列中的位置顺序，使得模型能够理解句子的结构。</p>
</li>
<li><p><strong>自注意力机制（Self-Attention）</strong><br>自注意力机制是 Transformer 的核心，它使得模型在处理每个单词时，能够同时考虑到输入序列中其他所有单词的影响。在每个 token 的表示中，不仅包含了当前位置的单词信息，还包括了整个句子中相关词汇的信息。通过自注意力机制，模型能够学习到更丰富的上下文关系。</p>
</li>
<li><p><strong>多头注意力机制（Multi-Head Attention）</strong><br>多头注意力的引入使得 Transformer 能够从多个子空间（多个角度）并行地进行注意力计算，增强了模型在学习不同类型依赖关系方面的能力。它让模型不仅仅关注到局部的关系，而是能够从多个层次去理解输入序列。</p>
</li>
<li><p><strong>前馈神经网络（Feed-Forward Network）</strong><br>在每个编码器和解码器的层中，除了自注意力机制外，还会有一个前馈神经网络，它主要由两个线性变换和一个非线性激活函数（如 ReLU）构成。通过这种方式，模型能够捕捉到更复杂的特征表示。</p>
</li>
<li><p><strong>残差连接和层归一化（Residual Connection &amp; Layer Normalization）</strong><br>为了防止深层网络中的梯度消失或爆炸问题，Transformer 在每一层都采用了 <strong>残差连接</strong> 和 <strong>层归一化</strong>。残差连接确保信息在网络中的有效传递，而层归一化帮助模型的训练更稳定、收敛更快。</p>
</li>
</ol>
<h6 id="对于翻译任务"><a href="#对于翻译任务" class="headerlink" title="对于翻译任务"></a>对于翻译任务</h6><p>对于学习的数据，应该是一一配对的句子对(两种不同的语言)<br>对于模型而言，需要分别将句子对的两个视作 <strong>源语言</strong> 和 <strong>目标语言</strong></p>
<p>因为模型无法学习非数值的数据，所以需要对于文本数据进行词嵌入处理，把句子变成tokens，再把tokens变成可学习的向量(维度为embed_dim)。</p>
<ul>
<li><p>对于编码器</p>
<ol>
<li>分词处理：<br> 首先，使用分词技术将输入句子拆分为若干个token（分割策略有讲究，可以是基于词的划分，也可以是基于子词或字符的划分等）。分词后的结果是一个形状为 (batch_size, seq_len) 的张量，其中每个token都被映射为其对应的ID。</li>
<li>词嵌入和位置编码：<br> 接下来，通过词嵌入（Input Embedding）将token ID映射为对应的高维稠密向量，得到形状为 (batch_size, seq_len, embed_dim) 的数据。这些嵌入向量是通过查找表学习得到的。<br> 然后，使用位置编码（Positional Encoding）对输入嵌入进行增强。位置编码通过正余弦函数生成，形状为 (batch_size, seq_len, embed_dim)，用于注入序列中每个token的位置信息。位置编码会加到输入嵌入中，从而让模型能够感知每个token在序列中的相对位置。</li>
<li>进入自注意力（Self-Attention）层：<br> 数据进入自注意力层后，首先通过三个全连接层(embed_dim, d_model)分别计算(即 $X @ W_Q$ )得到查询（Q）、键（K）、值（V）矩阵(batch_size, seq_len, d_model)。这些矩阵的参数会通过后续的梯度下降优化学习得到。<br> 将三个矩阵分割成 head 个头，得到形状为 (batch_size，head, seq_len, d_model&#x2F;head) 的子注意力头<br> 对每个token的查询向量Q与其他token的键向量K进行点积计算，得到注意力得分(batch_size，head, seq_len, seq_len)。通过缩放因子（根号下 d_k）对得分进行缩放，再通过softmax操作将得分转换为概率分布。<br> 接着，使用该概率分布对值向量V进行加权求和，得到一个融合了上下文信息的新的表示((batch_size，head, seq_len, d_model&#x2F;head))。</li>
<li>还原维度:<br> 将注意力子头的数据相融合得到(batch_size, seq_len, d_model)，再经过全连接层 $W_o$(d_model, d_model),得到（batch_size, seq_len，d_model）</li>
<li>残差连接与层归一化：<br> 将注意力层的输出与输入数据进行残差连接，然后进行层归一化（Layer Normalization）。残差连接有助于缓解深层网络中的梯度消失问题，而层归一化则有助于提升训练的稳定性，数据形状不变。</li>
<li>前馈神经网络（MLP）：<br> 接着，经过一个前馈神经网络（MLP）。这个网络通常由两层全连接层组成，其中中间层的大小通常是输入的4倍，最终得到的数据形状为（batch_size, seq_len，d_model）。前馈神经网络的作用是进一步非线性转换每个token的表示。</li>
<li>再次残差连接与层归一化：<br> 前馈神经网络的输出会再次与输入数据进行残差连接，并进行层归一化处理，确保模型在深层网络中保持有效的信息流动和稳定的训练过程。</li>
<li>完成一次Encoder处理：<br> 这样，我们就完成了一个Transformer编码器层的处理。</li>
<li>重复堆叠多个编码器层：<br> 上述的编码器层结构会堆叠若干次，每一层都包括自注意力、前馈神经网络和层归一化等组件。通过多层堆叠，模型能够逐步捕捉更复杂的特征和语义信息。</li>
</ol>
</li>
<li><p>对于解码器</p>
<ol>
<li><p>目标序列的输入<br> 解码器的输入是目标语言的 token 序列，首先进行分词处理，得到形状(batch_size, target_seq_len) （每个 token 被映射为 ID）</p>
</li>
<li><p>词嵌入与位置编码<br> 通过词嵌入（Input Embedding）将 token ID 映射为高维向量：</p>
<ul>
<li>形状：(batch_size, target_seq_len, d_model)</li>
</ul>
<p> 添加位置编码（Positional Encoding），用于注入序列中的位置信息：</p>
<ul>
<li>形状：(batch_size, target_seq_len, d_model)</li>
</ul>
<p> 位置编码与词嵌入逐元素相加，形成解码器的输入向量。</p>
</li>
<li><p>掩码多头自注意力（Masked Multi-Head Self-Attention）<br> 作用：防止解码器看到未来 token，以确保自回归生成的正确性。<br> 计算过程：</p>
<ul>
<li>计算 Q, K, V<br>   目标序列的输入经过 W_Q, W_K, W_V 三个全连接层后，分别得到Q, K, V<br>   形状：(batch_size, target_seq_len, d_model)</li>
<li>分割成 head 个独立注意力头：<br>   形状：(batch_size, head, target_seq_len, d_k)，其中 d_k &#x3D; d_model &#x2F; head</li>
<li>计算注意力得分</li>
</ul>
</li>
</ol>
</li>
</ul>
<p>Q @ K^T &#x2F; sqrt(d_k)，得到：<br>形状：(batch_size, head, target_seq_len, target_seq_len)<br>        - 应用 Mask，将未来 token 的得分置为 -∞，再经过 Softmax 归一化。<br>        - 计算注意力输出<br>使用 Softmax 权重加权求和 V，得到：<br>形状：(batch_size, head, target_seq_len, d_k)<br>         - 拼接多个头，经过 W_o 变换，恢复：<br>形状：(batch_size, target_seq_len, d_model)<br>        - 残差连接与层归一化<br>形状：(batch_size, target_seq_len, d_model)<br>4. 编码器-解码器交叉注意力（Encoder-Decoder Attention）<br>作用：让解码器能够参考编码器的输出，与源序列进行交互。<br>计算过程：</p>
<ul>
<li>计算 Q, K, V<br>Q 来自上一层解码器的输出，形状：(batch_size, target_seq_len, d_model)<br>K, V 来自编码器的最终输出，形状：(batch_size, source_seq_len, d_model)</li>
<li>计算注意力得分<br>$Q @ K^T &#x2F; sqrt(d_k)$，得到：<br>形状：(batch_size, head, target_seq_len, source_seq_len)<br>经过 Softmax 归一化，生成注意力权重。</li>
<li>计算注意力输出<br>使用 Softmax 权重加权求和 V，得到：<br>形状：(batch_size, head, target_seq_len, d_k)</li>
<li>拼接多个头，经过 W_o 变换：<br>形状：(batch_size, target_seq_len, d_model)</li>
<li>残差连接与层归一化<br>形状：(batch_size, target_seq_len, d_model)</li>
</ul>
<ol start="5">
<li><p>前馈神经网络（Feed-Forward Network, FFN）<br>作用：进一步变换特征，提高表示能力。<br>计算过程：</p>
<ul>
<li>经过第一层全连接层，将维度扩展至 4 * d_model，并使用 ReLU 激活：<br>形状：(batch_size, target_seq_len, 4 * d_model)</li>
<li>经过第二层全连接层，将维度降回 d_model：<br>形状：(batch_size, target_seq_len, d_model)</li>
<li>残差连接与层归一化<br>形状：(batch_size, target_seq_len, d_model)</li>
</ul>
</li>
<li><p>解码器堆叠多个层<br>解码器的上述结构会堆叠 N 层（标准 Transformer 取 N&#x3D;6）。<br>通过多层堆叠，模型能够逐步捕捉复杂的特征，并有效建模目标序列的上下文关系。</p>
</li>
<li><p>线性层 + Softmax（输出层）<br>解码器的最终输出传入 全连接层（Linear），投影至词汇表大小：<br>形状：(batch_size, target_seq_len, vocab_size)<br>经过 Softmax 归一化，得到每个 token 在词汇表上的概率分布：<br>形状：(batch_size, target_seq_len, vocab_size)</p>
</li>
<li><p>自回归生成（Inference 阶段）<br>训练时，整个目标序列同时输入，计算并行进行。<br>但推理时（生成文本），解码器采用 自回归（Autoregressive） 方式：<br>先输入起始 token（例如 &lt;BOS&gt;），生成第一个 token。<br>将已生成的 token 作为新输入，继续生成下一个 token。<br>依次重复，直到生成 &lt;EOS&gt; 或达到最大长度。</p>
</li>
</ol>
<h6 id="对于文本情感分类任务"><a href="#对于文本情感分类任务" class="headerlink" title="对于文本情感分类任务"></a>对于文本情感分类任务</h6><p>事实上这个任务完全不需要解码器的存在，因为是分类任务而不是生成任务</p>
<p>所以只需要编码器的堆叠，最终通过全连接层进行二分类即可</p>
<h4 id="Transformer的优势"><a href="#Transformer的优势" class="headerlink" title="Transformer的优势"></a>Transformer的优势</h4><ol>
<li><strong>并行计算</strong>：与传统的 RNN 和 CNN 不同，Transformer 可以一次性处理整个序列，大大提高了计算效率。</li>
<li><strong>长距离依赖建模</strong>：通过自注意力机制，Transformer 能够捕捉到序列中任意两位置之间的依赖关系，从而解决了 RNN 无法有效建模长距离依赖的问题。</li>
<li><strong>高效训练</strong>：由于 Transformer 结构的简洁性和并行性，它能够大幅提高训练的速度，尤其是在处理大规模数据时。</li>
</ol>
<p>Transformer 模型的核心优势在于能够通过自注意力机制捕捉长距离的依赖关系，并且通过并行计算极大地提升了处理速度。这使得它成为了自然语言处理（NLP）领域的革命性突破，并成为许多现代预训练语言模型（如 BERT、GPT）架构的基础。</p>
<h1 id="关于-Transformer-的变体"><a href="#关于-Transformer-的变体" class="headerlink" title="关于 Transformer 的变体"></a>关于 Transformer 的变体</h1><h3 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h3><p><em>你做完型填空的时候也不会只看一边，对吧</em></p>
<p>假如现在的任务目标不再是翻译，而是纯粹的语言理解（语义情感分析，问答），那么完全不需要 <strong>Decoder</strong> ，只需要 <strong>Encoder</strong> 就已经能够充分完成任务了。</p>
<p><strong>BERT</strong> 基于 <strong>Transformer</strong> 的编码器部分，完全摒弃了解码器，主要用于理解输入序列的上下文信息。BERT 的输入是一个句子或一对句子，它通过以下两种方式来获取丰富的上下文信息：</p>
<ol>
<li><strong>Masked Language Model（MLM）：</strong><br> 为了进行预训练，<strong>BERT</strong> 会随机遮蔽输入中的一部分单词，通过上下文信息来预测这些被遮蔽的单词。这种训练方式确保模型能有效学习到每个词在上下文中的角色。主要目标是学会词元之间的关系。</li>
<li><strong>Next Sentence Prediction（NSP）：</strong><br> <strong>BERT</strong> 还通过预测句子间的关系来进一步增强理解能力。它随机选取两个句子，判断第二个句子是否是第一个句子的下一句，从而训练模型捕捉句子间的语义关系。主要目标是学会句间的关系。</li>
</ol>
<p>BERT的独特之处还在于其 <strong>双向编码能力</strong>。</p>
<p>既然是纯碎的语言理解，那么就 <strong>没必要单向阅读</strong> 了吧，所以这里引入了 <strong>双向理解</strong> 的思路，这样显然而且试验证明的确能提高模型性能。</p>
<p>传统的语言模型（如 GPT）是单向的，只能从左到右或从右到左理解文本，而 BERT 通过双向的方式同时捕捉前后文的关系，从而更好地理解词汇的含义。</p>
<h2 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h2><p><em>你说话的时候总得想着你上一句是什么吧</em></p>
<p>理解一句话的意思不仅仅是拼凑单词的意义，而是要在理解词汇的同时，还能生成合乎逻辑且富有创意的内容。这正是 <strong>GPT（Generative Pretrained Transformer）</strong> 模型的核心能力。它不仅能理解输入的文本，还能在此基础上进行创造性地生成输出。</p>
<p>GPT 是基于 Transformer 架构的一个衍生模型，最重要的特点是完全依赖 <strong>自回归模型</strong>，即通过前文的单词逐步生成后续单词，这使得它在生成连贯且自然的文本时具有无与伦比的优势。</p>
<p>和 <strong>BERT</strong> 相对，<strong>GPT</strong> 反而是只聚焦于 <strong>解码器</strong>，它通过连续生成预测下一个词汇来实现文本生成。在每一个词生成之前，都要把上文已经有的文本进行处理，经过若干解码器的堆叠处理，然后得到最可能的下一个词，如此循环往复，最终得到完整的生成文本。</p>
<p>GPT 的关键特点是自回归生成过程。它从一个初始的输入开始，逐步预测下一个词汇，然后把这个词汇作为新的输入加入到序列中继续预测下一个词汇，直到生成完整的句子或段落。</p>
<p>GPT 的成功在于通过大量数据的预训练，使得模型能够在生成文本时，不仅理解词汇，还能够创作出合理且自然的语言。这让它成为当前自然语言处理领域的重要突破之一。</p>
<h2 id="Reformer"><a href="#Reformer" class="headerlink" title="Reformer"></a>Reformer</h2><p><em>深度压缩与效率的化身</em></p>
<p>当句子一长，token增多，那么如果对于每一个 token 都需要对其他所有的 token 计算注意力得分，这显然是一个相当低效的过程。</p>
<p>其实想一下，对于每一个 token 而言，对于它比较重要的 token 也没多少，其实不需要对于其他 <strong>每一个</strong> token <strong>都</strong> 计算注意力（计算出来的注意力得分权重为0.0000000000001要你有啥用）。如果能将相似注意力的 token 放在一起，只在他们之间计算注意力得分，然后加权求和，就很好的缩小了问题的规模，这就是 <strong>Reformer</strong> 了</p>
<p><strong>Reformer架构设计</strong><br>Reformer 的设计理念是将 <strong>注意力机制</strong> 和 <strong>压缩存储</strong> 结合起来，从而使得计算效率和内存使用得到了极大的提升。它的关键创新点包括：</p>
<ol>
<li>局部敏感哈希<br>Reformer用 <strong>局部敏感哈希（LSH）</strong> 来替代传统的 <strong>全局</strong> 自注意力计算。这种方式将注意力计算限制在邻近的词汇之间，避免了传统自注意力计算中计算量过大的问题，从而有效减少了计算复杂度。</li>
<li>可逆残差网络<br>为了降低内存占用，Reformer 引入了 <strong>可逆残差网络</strong> 的概念。这意味着在计算时，<strong>不需要保留中间层的所有输出</strong>，而是通过反向传递过程来恢复它们，这大大减少了内存的使用。</li>
<li>分块计算<br>Reformer 对输入数据进行分块处理，每次只处理小块的局部信息，而不是一次性处理整个序列。这样可以进一步降低计算开销，尤其是在处理超大规模数据时。</li>
</ol>
<p>有了如上的改进，可以在保证原 Transformer 的性能的基础上：</p>
<ul>
<li>更低的内存消耗：通过局部敏感哈希和可逆残差网络，Reformer 在处理大规模数据时显著减少了内存使用，提升了训练效率。</li>
<li>高效处理长序列：Reformer 在处理长序列时，通过分块计算避免了全局计算的瓶颈，使得它能够在有限资源下处理更长的文本。</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/">模型学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%A8%A1%E5%9E%8B/" rel="tag">模型</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-CycleGAN-实现莫奈风格画作转化为真实照片" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/18/CycleGAN-%E5%AE%9E%E7%8E%B0%E8%8E%AB%E5%A5%88%E9%A3%8E%E6%A0%BC%E7%94%BB%E4%BD%9C%E8%BD%AC%E5%8C%96%E4%B8%BA%E7%9C%9F%E5%AE%9E%E7%85%A7%E7%89%87/" class="article-date">
      <time datetime="2025-03-18T14:26:19.000Z" itemprop="datePublished">2025-03-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/18/CycleGAN-%E5%AE%9E%E7%8E%B0%E8%8E%AB%E5%A5%88%E9%A3%8E%E6%A0%BC%E7%94%BB%E4%BD%9C%E8%BD%AC%E5%8C%96%E4%B8%BA%E7%9C%9F%E5%AE%9E%E7%85%A7%E7%89%87/">CycleGAN 实现莫奈风格画作转化为真实照片</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">使用 CycleGAN 实现莫奈风格图像转化为真实照片风格图片</span></span><br><span class="line"><span class="string">在同一根目录下需要：</span></span><br><span class="line"><span class="string">    datasets/monet2photo/trainA 文件夹存放莫奈风格画像</span></span><br><span class="line"><span class="string">    datasets/monet2photo/trainB 文件夹存放真实风景图片</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> ImageFolder</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置参数</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line">epochs = <span class="number">1000</span></span><br><span class="line">input_channel_size = <span class="number">3</span></span><br><span class="line">output_channel_size = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">os.makedirs(<span class="string">&quot;output_cyclegan&quot;</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">256</span>, <span class="number">256</span>)),  <span class="comment"># 统一图片大小，方便处理</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))  <span class="comment"># RGB 图片归一化</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">dataset_A = ImageFolder(root=<span class="string">&quot;datasets/monet2photo/trainA&quot;</span>, transform=transform)  <span class="comment"># 莫奈画像</span></span><br><span class="line">dataset_B = ImageFolder(root=<span class="string">&quot;datasets/monet2photo/trainB&quot;</span>, transform=transform)  <span class="comment"># 真实照片</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataloader_A = DataLoader(dataset_A, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">dataloader_B = DataLoader(dataset_B, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 残差块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResnetBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv_block = nn.Sequential(</span><br><span class="line">            nn.ReflectionPad2d(<span class="number">1</span>), <span class="comment"># 反射填充，缓解填充带来的差异</span></span><br><span class="line">            nn.Conv2d(dim, dim, kernel_size=<span class="number">3</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.InstanceNorm2d(dim), <span class="comment"># 实例归一化</span></span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.ReflectionPad2d(<span class="number">1</span>),</span><br><span class="line">            nn.Conv2d(dim, dim, kernel_size=<span class="number">3</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.InstanceNorm2d(dim)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> x + <span class="variable language_">self</span>.conv_block(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成器</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_channel_size, output_channel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.main = nn.Sequential(</span><br><span class="line">            nn.ReflectionPad2d(<span class="number">3</span>),</span><br><span class="line">            nn.Conv2d(input_channel_size, <span class="number">64</span>, kernel_size=<span class="number">7</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>), <span class="comment"># (4,3,256,256)-&gt;(4,64,256,256) 感受野（7*7）</span></span><br><span class="line">            nn.InstanceNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),<span class="comment"># (4,64,256,256)-&gt;(4,128,128,128) 感受野（9*9）</span></span><br><span class="line">            nn.InstanceNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),<span class="comment"># (4,128,128,128)-&gt;(4,256,64,64) 感受野（13*13）</span></span><br><span class="line">            nn.InstanceNorm2d(<span class="number">256</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            ResnetBlock(<span class="number">256</span>),</span><br><span class="line">            ResnetBlock(<span class="number">256</span>),</span><br><span class="line">            ResnetBlock(<span class="number">256</span>),</span><br><span class="line">            ResnetBlock(<span class="number">256</span>), <span class="comment"># (4,256,64,64)-&gt;(4,256,64,64) 感受野（13*13）</span></span><br><span class="line"></span><br><span class="line">            nn.ConvTranspose2d(<span class="number">256</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, output_padding=<span class="number">1</span>, bias=<span class="literal">False</span>),<span class="comment"># (4,256,64,64)-&gt;(4,128,128,128) 感受野（17*17）</span></span><br><span class="line">            nn.InstanceNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.ConvTranspose2d(<span class="number">128</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, output_padding=<span class="number">1</span>, bias=<span class="literal">False</span>),<span class="comment"># (4,128,128,128)-&gt;(4,64,256,256) 感受野（25*25）</span></span><br><span class="line">            nn.InstanceNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.ReflectionPad2d(<span class="number">3</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, output_channel_size, kernel_size=<span class="number">7</span>, padding=<span class="number">0</span>),<span class="comment"># (4,64,128,128)-&gt;(4,3,256,256) 感受野（31*31）</span></span><br><span class="line">            nn.Tanh()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.main(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 判别器</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Discriminator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_channel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.model = nn.Sequential(</span><br><span class="line">            nn.Conv2d(input_channel_size, <span class="number">64</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>), <span class="comment"># (4,3,256,256)-&gt;(4,64,128,128)</span></span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, <span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>), <span class="comment"># (4,64,128,128)-&gt;(4,128,64,64)</span></span><br><span class="line">            nn.InstanceNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, <span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>), <span class="comment"># (4,128,64,64)-&gt;(4,256,32,32)</span></span><br><span class="line">            nn.InstanceNorm2d(<span class="number">256</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, <span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">1</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>) <span class="comment"># (4,256,32,32)-&gt;(4,1,16,16)</span></span><br><span class="line">            <span class="comment"># 使用更为高级的 PatchGAN 判别器而不是普通的判别器设计，这样设计能让模型更加关注局部区域的真实程度，这正对应了图像风格转化的任务特性</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.model(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">netG = Generator(input_channel_size, output_channel_size).to(device) <span class="comment"># A-&gt;B</span></span><br><span class="line">netF = Generator(output_channel_size, input_channel_size).to(device) <span class="comment"># B-&gt;A</span></span><br><span class="line">netD_A = Discriminator(input_channel_size).to(device) <span class="comment"># 检验图像是否是真实的A</span></span><br><span class="line">netD_B = Discriminator(output_channel_size).to(device) <span class="comment"># 检验图像是否是真实的B</span></span><br><span class="line"></span><br><span class="line">criterion_GAN = nn.MSELoss()</span><br><span class="line">criterion_cycle = nn.L1Loss()</span><br><span class="line"></span><br><span class="line">optimizer_GF = optim.Adam(<span class="built_in">list</span>(netG.parameters()) + <span class="built_in">list</span>(netF.parameters()), lr=lr, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>)) <span class="comment"># 由于循环一致性的设计，需要将 G 和 F 两个模型绑定在一起联合优化，保证相同的优化速率和效果</span></span><br><span class="line">optimizer_D_A = optim.Adam(netD_A.parameters(), lr=lr, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line">optimizer_D_B = optim.Adam(netD_B.parameters(), lr=lr, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练循环</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> i, ((real_A, _), (real_B, _)) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(dataloader_A, dataloader_B)):</span><br><span class="line">        real_A = real_A.to(device)</span><br><span class="line">        real_B = real_B.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练生成器</span></span><br><span class="line">        optimizer_GF.zero_grad()</span><br><span class="line">            <span class="comment"># 生成转化之后的图像</span></span><br><span class="line">        fake_B = netG(real_A)</span><br><span class="line">        fake_A = netF(real_B)</span><br><span class="line">        valid_B = torch.ones_like(netD_B(fake_B))</span><br><span class="line">        valid_A = torch.ones_like(netD_A(fake_A))</span><br><span class="line">            <span class="comment"># 计算损失</span></span><br><span class="line">                <span class="comment"># 计算对抗性损失</span></span><br><span class="line">        loss_G_GAN = criterion_GAN(netD_B(fake_B), valid_B)</span><br><span class="line">        loss_F_GAN = criterion_GAN(netD_A(fake_A), valid_A)</span><br><span class="line">                <span class="comment"># 计算循环一致性损失</span></span><br><span class="line">        recovered_A = netF(fake_B) <span class="comment"># 将生成出的虚假图像 B 转化回去</span></span><br><span class="line">        recovered_B = netG(fake_A) <span class="comment"># 将生成出的虚假图像 A 转化回去</span></span><br><span class="line">        loss_cycle_A = criterion_cycle(recovered_A, real_A)</span><br><span class="line">        loss_cycle_B = criterion_cycle(recovered_B, real_B)</span><br><span class="line">        total_cycle_loss = (loss_cycle_A + loss_cycle_B) * <span class="number">10</span> <span class="comment"># 10 是权重</span></span><br><span class="line">            <span class="comment"># 计算总损失值</span></span><br><span class="line">        total_G_loss = loss_G_GAN + loss_F_GAN + total_cycle_loss</span><br><span class="line">        total_G_loss.backward()</span><br><span class="line">        optimizer_GF.step()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练判别器</span></span><br><span class="line">            <span class="comment"># 训练判别器 A</span></span><br><span class="line">        real_output_A = netD_A(real_A) <span class="comment"># 来自真实的图像</span></span><br><span class="line">        fake_output_A = netD_A(fake_A.detach()) <span class="comment"># 来自虚假的图像，细节阻断梯度传播，防止波及生成fake_A 的 netF</span></span><br><span class="line">        loss_D_real_A = criterion_GAN(real_output_A, torch.ones_like(real_output_A)) <span class="comment"># 应接近于 1</span></span><br><span class="line">        loss_D_fake_A = criterion_GAN(fake_output_A, torch.zeros_like(fake_output_A)) <span class="comment"># 应接近于 0</span></span><br><span class="line">        loss_D_A = <span class="number">0.5</span> * (loss_D_real_A + loss_D_fake_A) <span class="comment"># 0.5 代表取均值</span></span><br><span class="line">        loss_D_A.backward()</span><br><span class="line">        optimizer_D_A.step()</span><br><span class="line">            <span class="comment"># 训练判别器 B</span></span><br><span class="line">            <span class="comment"># 与 A 类似</span></span><br><span class="line">        real_output_B = netD_B(real_B)</span><br><span class="line">        fake_output_B = netD_B(fake_B.detach())</span><br><span class="line">        loss_D_real_B = criterion_GAN(real_output_B, torch.ones_like(real_output_B))</span><br><span class="line">        loss_D_fake_B = criterion_GAN(fake_output_B, torch.zeros_like(fake_output_B))</span><br><span class="line">        loss_D_B = <span class="number">0.5</span> * (loss_D_real_B + loss_D_fake_B)</span><br><span class="line">        loss_D_B.backward()</span><br><span class="line">        optimizer_D_B.step()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>] Batch <span class="subst">&#123;i&#125;</span> G_loss: <span class="subst">&#123;total_G_loss.item():<span class="number">.4</span>f&#125;</span> D_A: <span class="subst">&#123;loss_D_A.item():<span class="number">.4</span>f&#125;</span> D_B: <span class="subst">&#123;loss_D_B.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="comment"># 保存生成器参数，方便加载使用</span></span><br><span class="line">            torch.save(netG.state_dict(), <span class="string">&quot;generator_monet2real.pth&quot;</span>)  <span class="comment"># monet-&gt;real 参数</span></span><br><span class="line">            torch.save(netF.state_dict(), <span class="string">&quot;generator_real2monet.pth&quot;</span>)  <span class="comment"># real-&gt;monet 参数</span></span><br></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/project/">project</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CycleGAN/" rel="tag">CycleGAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GAN/" rel="tag">GAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/code/" rel="tag">code</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-CycleGAN" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/18/CycleGAN/" class="article-date">
      <time datetime="2025-03-18T13:30:00.000Z" itemprop="datePublished">2025-03-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/18/CycleGAN/">CycleGAN 详解</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>CycleGAN 是一种无监督图像到图像转换（Image-to-Image Translation）模型，能够在不需要成对训练数据的情况下实现高质量的风格转换。该方法由 Jun-Yan Zhu 等人在 2017 年提出，并在风格迁移、图像修复、医学影像处理等领域有着广泛的应用。</p>
<h1 id="1-CycleGAN-简介"><a href="#1-CycleGAN-简介" class="headerlink" title="1. CycleGAN 简介"></a>1. CycleGAN 简介</h1><p>CycleGAN（Cycle-Consistent Generative Adversarial Networks）主要解决<strong>未配对数据</strong>（Unpaired Data）的图像转换问题。例如，我们可以使用 CycleGAN 在不需要成对的”马”和”斑马”图片的情况下，将一匹马的图像转换为斑马风格，反之亦然。</p>
<p>相比于 Pix2Pix 这样的有监督方法（需要成对数据），CycleGAN 的最大特点是<strong>无监督学习</strong>，它使用循环一致性损失（Cycle Consistency Loss）来确保图像转换的可逆性。</p>
<h1 id="2-CycleGAN-主要结构"><a href="#2-CycleGAN-主要结构" class="headerlink" title="2. CycleGAN 主要结构"></a>2. CycleGAN 主要结构</h1><p>CycleGAN 由两个 GAN 组成，每个 GAN 负责将一种风格转换为另一种：</p>
<ul>
<li><strong>生成器 G（X → Y）</strong>：将域 X（如马的图片）转换为域 Y（如斑马的图片）。</li>
<li><strong>生成器 F（Y → X）</strong>：将域 Y 的图像转换回域 X。</li>
<li><strong>判别器 D_X</strong>：判断给定的 X 域图像是真实的还是由 F 生成的。</li>
<li><strong>判别器 D_Y</strong>：判断给定的 Y 域图像是真实的还是由 G 生成的。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/junyanz/CycleGAN/master/imgs/teaser.jpg"></p>
<p>CycleGAN 的关键点在于<strong>循环一致性损失</strong>，它确保如果我们将图像从 X → Y，再从 Y → X，得到的图像应该与原始 X 类似。</p>
<h1 id="3-CycleGAN-训练过程"><a href="#3-CycleGAN-训练过程" class="headerlink" title="3. CycleGAN 训练过程"></a>3. CycleGAN 训练过程</h1><p>CycleGAN 采用对抗训练框架，同时优化两个目标：</p>
<h3 id="3-1-对抗损失（Adversarial-Loss）"><a href="#3-1-对抗损失（Adversarial-Loss）" class="headerlink" title="3.1 对抗损失（Adversarial Loss）"></a>3.1 对抗损失（Adversarial Loss）</h3><p>CycleGAN 继承了标准 GAN 的损失，使得生成器 G 生成的图像尽可能真实：</p>
<p>$$ L_{GAN}(G, D_Y, X, Y) &#x3D; \mathbb{E}<em>{y \sim p</em>{data}(y)} [\log D_Y(y)] + \mathbb{E}<em>{x \sim p</em>{data}(x)} [\log (1 - D_Y(G(x)))] $$</p>
<p>类似地，F 也有自己的 GAN 损失：</p>
<p>$$ L_{GAN}(F, D_X, Y, X) &#x3D; \mathbb{E}<em>{x \sim p</em>{data}(x)} [\log D_X(x)] + \mathbb{E}<em>{y \sim p</em>{data}(y)} [\log (1 - D_X(F(y)))] $$</p>
<h3 id="3-2-循环一致性损失（Cycle-Consistency-Loss）"><a href="#3-2-循环一致性损失（Cycle-Consistency-Loss）" class="headerlink" title="3.2 循环一致性损失（Cycle Consistency Loss）"></a>3.2 循环一致性损失（Cycle Consistency Loss）</h3><p>为了确保 G(X) 能够转换回 X，我们引入循环一致性损失：</p>
<p>$$ L_{cycle}(G, F) &#x3D; \mathbb{E}<em>{x \sim p</em>{data}(x)} [||F(G(x)) - x||<em>1] + \mathbb{E}</em>{y \sim p_{data}(y)} [||G(F(y)) - y||_1] $$</p>
<h3 id="3-3-全损失函数"><a href="#3-3-全损失函数" class="headerlink" title="3.3 全损失函数"></a>3.3 全损失函数</h3><p>综合以上损失，CycleGAN 的最终目标函数为：</p>
<p>$$ L(G, F, D_X, D_Y) &#x3D; L_{GAN}(G, D_Y, X, Y) + L_{GAN}(F, D_X, Y, X) + \lambda L_{cycle}(G, F) $$</p>
<p>其中，( \lambda ) 是权重参数，控制循环一致性损失的重要程度。</p>
<h1 id="4-CycleGAN-的应用场景"><a href="#4-CycleGAN-的应用场景" class="headerlink" title="4. CycleGAN 的应用场景"></a>4. CycleGAN 的应用场景</h1><p><strong>风格转换</strong>：如将照片转换为油画风格，或者将真实图像转换为动漫风格。<br><strong>图像增强</strong>：如提高医学影像的质量或将黑白照片转换为彩色。<br><strong>域适应</strong>：用于将数据从一个领域（Domain）映射到另一个领域。<br><strong>图像修复</strong>：在去雾、去噪声等任务中表现良好。</p>
<h1 id="5-CycleGAN-的优缺点"><a href="#5-CycleGAN-的优缺点" class="headerlink" title="5. CycleGAN 的优缺点"></a>5. CycleGAN 的优缺点</h1><h3 id="5-1-优点"><a href="#5-1-优点" class="headerlink" title="5.1 优点"></a>5.1 优点</h3><ul>
<li><strong>无需成对数据</strong>，适用于无监督图像转换任务。</li>
<li><strong>效果自然</strong>，生成图像更加逼真。</li>
<li><strong>结构简单</strong>，训练方法类似于标准 GAN。</li>
</ul>
<h3 id="5-2-缺点"><a href="#5-2-缺点" class="headerlink" title="5.2 缺点"></a>5.2 缺点</h3><ul>
<li><strong>容易模式崩溃（Mode Collapse）</strong>，导致生成的图像缺乏多样性。</li>
<li><strong>训练不稳定</strong>，对超参数（如学习率、循环损失权重等）敏感。</li>
<li><strong>转换不一定完全准确</strong>，对于复杂场景可能生成伪影（Artifacts）。</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/">模型学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CycleGAN/" rel="tag">CycleGAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GAN/" rel="tag">GAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%9B%BE%E5%83%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/" rel="tag">图像风格迁移</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-关于-线性回归-的笔记与思考" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/18/%E5%85%B3%E4%BA%8E-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E7%9A%84%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%80%9D%E8%80%83/" class="article-date">
      <time datetime="2025-03-18T10:03:13.000Z" itemprop="datePublished">2025-03-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/18/%E5%85%B3%E4%BA%8E-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E7%9A%84%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%80%9D%E8%80%83/">关于 线性回归 的笔记与思考</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/">模型学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%A8%A1%E5%9E%8B/" rel="tag">模型</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="tag">线性回归</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-WGAN-WGAN-GP" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/16/WGAN-WGAN-GP/" class="article-date">
      <time datetime="2025-03-16T12:45:00.000Z" itemprop="datePublished">2025-03-16</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/16/WGAN-WGAN-GP/">WGAN与WGAN-GP</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="省流"><a href="#省流" class="headerlink" title="省流"></a>省流</h1><p>*生成对抗网络（GAN）在图像生成等任务上表现出色，但传统的 GAN 训练存在诸多问题，如<strong>模式崩溃（Mode Collapse）</strong>、<strong>梯度消失或爆炸</strong>等。为了解决这些问题，WGAN（Wasserstein GAN）及其改进版本 WGAN-GP（WGAN with Gradient Penalty）被提出，使得 GAN 的训练更加稳定，并能够生成质量更高的样本。*</p>
<p>传统的GAN模型的很多问题来自于它的损失函数的数学缺陷上。使用原损失函数其实是等价于去优化一个JS散度，假如两个分布完全没有重合的情况下，损失函数值是一个常数log2，这个时候梯度为0，这一点对于模型训练是致命的，会导致梯度消失的问题，那么如何修改这个损失函数，才能使得缓解这个问题呢，所以研究人员提出了使用 Wasserstein 距离来替代原设计。</p>
<p>从直观上来看，Wasserstein 好像就是将原损失函数的取对数的操作取消掉，成为</p>
<ul>
<li><strong>批评器的损失函数</strong>：<br>$$ L_D &#x3D; E[D(X)] - E[D(G(Z))] $$</li>
<li><strong>生成器的损失函数</strong>：<br>$$ L_G &#x3D; -E[D(G(Z))] $$</li>
</ul>
<p>为了使得这一替换有效，模型必须满足 1-李普希兹 函数条件，这一点需要使用 <strong>梯度裁剪</strong> 或 <strong>梯度惩罚</strong> 的操作。</p>
<p>最终证明效果很好。</p>
<h1 id="WGAN-简介"><a href="#WGAN-简介" class="headerlink" title="WGAN 简介"></a>WGAN 简介</h1><p>WGAN（Wasserstein GAN）由 Martin Arjovsky 等人在 2017 年提出，并在论文《Wasserstein GAN》中进行了详细介绍。WGAN 通过引入<strong>Wasserstein 距离（也称 Earth Mover’s Distance, EMD）</strong> 来度量真实分布和生成分布之间的差距，从而改善训练稳定性。</p>
<h2 id="WGAN-的改进点"><a href="#WGAN-的改进点" class="headerlink" title="WGAN 的改进点"></a>WGAN 的改进点</h2><p>相较于传统 GAN，WGAN 主要有以下改进：</p>
<ol>
<li><strong>引入 Wasserstein 距离</strong>：<ul>
<li>传统 GAN 使用 JS 散度（Jensen-Shannon Divergence）度量真实分布和生成分布的差异，但容易导致梯度消失。</li>
<li>WGAN 采用 Wasserstein 距离（EMD），它可以提供更稳定的训练信号。</li>
</ul>
</li>
<li><strong>去掉 Sigmoid 及交叉熵损失</strong>：<ul>
<li>WGAN 直接使用神经网络的输出值作为衡量分布距离的指标，不再使用 Sigmoid 激活函数。</li>
</ul>
</li>
<li><strong>使用权重裁剪（Weight Clipping）</strong>：<ul>
<li>WGAN 要求判别器（Critic）的参数保持在一定范围（如 [-0.01, 0.01]），以满足 1-Lipschitz 条件。</li>
</ul>
</li>
<li><strong>不再使用传统的判别器（Discriminator），改用批评器（Critic）</strong>：<ul>
<li>批评器不再输出样本是真假，而是输出 Wasserstein 距离的估计值。</li>
</ul>
</li>
</ol>
<h1 id="WGAN-结构"><a href="#WGAN-结构" class="headerlink" title="WGAN 结构"></a>WGAN 结构</h1><h2 id="生成器（Generator）"><a href="#生成器（Generator）" class="headerlink" title="生成器（Generator）"></a>生成器（Generator）</h2><ul>
<li>生成器的结构与普通 GAN 类似，输入为随机噪声 <code>Z</code>，输出为生成的图像 <code>G(Z)</code>。</li>
<li>目标是最小化 Wasserstein 距离，使生成样本的分布接近真实分布。</li>
</ul>
<h2 id="批评器（Critic）"><a href="#批评器（Critic）" class="headerlink" title="批评器（Critic）"></a>批评器（Critic）</h2><ul>
<li>取代传统判别器，输入为真实图像 <code>X</code> 或生成图像 <code>G(Z)</code>。</li>
<li>输出一个<strong>任意实数</strong>，其值用于衡量真实数据和生成数据的 Wasserstein 距离。</li>
<li>训练时优化 Wasserstein 损失，使得批评器可以正确衡量两者的距离。</li>
</ul>
<h2 id="WGAN-损失函数"><a href="#WGAN-损失函数" class="headerlink" title="WGAN 损失函数"></a>WGAN 损失函数</h2><ul>
<li><strong>批评器的损失函数</strong>：<br>$$ L_D &#x3D; E[D(X)] - E[D(G(Z))] $$</li>
<li><strong>生成器的损失函数</strong>：<br>$$ L_G &#x3D; -E[D(G(Z))] $$</li>
</ul>
<h1 id="WGAN-GP-简介"><a href="#WGAN-GP-简介" class="headerlink" title="WGAN-GP 简介"></a>WGAN-GP 简介</h1><p>虽然 WGAN 通过权重裁剪保证了 1-Lipschitz 条件，但这种方法存在一定的弊端，如<strong>梯度消失、参数空间受限</strong>。为了解决这些问题，Gulrajani 等人提出了 WGAN-GP（WGAN with Gradient Penalty），用**梯度惩罚（Gradient Penalty）**取代权重裁剪。</p>
<h2 id="WGAN-GP-的改进点"><a href="#WGAN-GP-的改进点" class="headerlink" title="WGAN-GP 的改进点"></a>WGAN-GP 的改进点</h2><ol>
<li><strong>去掉权重裁剪，改用梯度惩罚</strong>：<ul>
<li>直接限制批评器的梯度范数，使其满足 1-Lipschitz 条件。</li>
</ul>
</li>
<li><strong>更稳定的训练</strong>：<ul>
<li>由于不再强制限制参数范围，模型可以更自由地学习复杂分布。</li>
</ul>
</li>
</ol>
<h2 id="WGAN-GP-的损失函数"><a href="#WGAN-GP-的损失函数" class="headerlink" title="WGAN-GP 的损失函数"></a>WGAN-GP 的损失函数</h2><ul>
<li><p><strong>批评器的损失函数</strong>：<br>$$ L_D &#x3D; E[D(X)] - E[D(G(Z))] + \lambda E[(||\nabla_{\hat{x}} D(\hat{x})||_2 - 1)^2] $$<br>其中：</p>
<ul>
<li>( \lambda ) 是梯度惩罚项的权重，一般设为 10。</li>
<li>( \hat{x} ) 是真实样本和生成样本之间的插值。</li>
<li>目标是让梯度范数接近 1，以满足 1-Lipschitz 条件。</li>
</ul>
</li>
<li><p><strong>生成器的损失函数</strong>（与 WGAN 相同）：<br>$$ L_G &#x3D; -E[D(G(Z))] $$</p>
</li>
</ul>
<h1 id="WGAN-GP-训练过程"><a href="#WGAN-GP-训练过程" class="headerlink" title="WGAN-GP 训练过程"></a>WGAN-GP 训练过程</h1><ol>
<li><strong>批评器训练</strong><ul>
<li>使用真实数据 <code>X</code> 和生成数据 <code>G(Z)</code> 计算 Wasserstein 距离。</li>
<li>计算梯度惩罚项，调整批评器的参数。</li>
</ul>
</li>
<li><strong>生成器训练</strong><ul>
<li>生成器更新参数，使 <code>D(G(Z))</code> 尽可能大，即最小化 Wasserstein 距离。</li>
</ul>
</li>
<li><strong>交替训练</strong><ul>
<li>通常训练 <strong>5 次批评器</strong>，再训练 <strong>1 次生成器</strong>。</li>
</ul>
</li>
</ol>
<h1 id="WGAN-和-WGAN-GP-的对比"><a href="#WGAN-和-WGAN-GP-的对比" class="headerlink" title="WGAN 和 WGAN-GP 的对比"></a>WGAN 和 WGAN-GP 的对比</h1><table>
<thead>
<tr>
<th></th>
<th>WGAN</th>
<th>WGAN-GP</th>
</tr>
</thead>
<tbody><tr>
<td>Lipschitz 约束</td>
<td>权重裁剪</td>
<td>梯度惩罚</td>
</tr>
<tr>
<td>训练稳定性</td>
<td>相对稳定</td>
<td>更加稳定</td>
</tr>
<tr>
<td>适用于高维数据</td>
<td>可能受限</td>
<td>适用于更复杂的数据</td>
</tr>
<tr>
<td>生成质量</td>
<td>良好</td>
<td>更高质量</td>
</tr>
</tbody></table>
<h1 id="WGAN-WGAN-GP-的应用"><a href="#WGAN-WGAN-GP-的应用" class="headerlink" title="WGAN &#x2F; WGAN-GP 的应用"></a>WGAN &#x2F; WGAN-GP 的应用</h1><p>WGAN 及 WGAN-GP 由于训练稳定，广泛应用于<strong>图像生成、风格迁移、数据增强</strong>等任务。例如：</p>
<ul>
<li><strong>高质量人脸生成</strong>（如 CelebA 数据集）</li>
<li><strong>医学影像合成</strong>（用于补充训练数据）</li>
<li><strong>图像超分辨率</strong></li>
</ul>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>WGAN 通过 Wasserstein 距离提高了 GAN 训练的稳定性，而 WGAN-GP 进一步优化了 Lipschitz 约束，使得训练更加稳定，生成效果更好。两者的主要改进点包括：</p>
<ul>
<li>WGAN <strong>引入 Wasserstein 距离，改进损失函数</strong>，但仍需权重裁剪。</li>
<li>WGAN-GP <strong>用梯度惩罚替代权重裁剪，提高稳定性</strong>。</li>
<li>WGAN-GP 适用于更复杂的数据分布，效果更优。</li>
</ul>
<p>WGAN 和 WGAN-GP 的提出极大地推动了 GAN 的发展，后续许多生成模型（如 StyleGAN）都在其基础上进行改进。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/">模型学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GAN/" rel="tag">GAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/WGAN/" rel="tag">WGAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/WGAN-GP/" rel="tag">WGAN-GP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-DCGAN" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/16/DCGAN/" class="article-date">
      <time datetime="2025-03-16T12:30:00.000Z" itemprop="datePublished">2025-03-16</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/16/DCGAN/">DCGAN</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>生成对抗网络（GAN）是一种强大的生成模型，而深度卷积生成对抗网络（DCGAN, Deep Convolutional GAN）是GAN的一个改进版本，它引入了**深度卷积神经网络（CNN）**来增强图像生成的能力，使得生成的图像更加清晰、稳定。</p>
<h1 id="DCGAN-简介"><a href="#DCGAN-简介" class="headerlink" title="DCGAN 简介"></a>DCGAN 简介</h1><p>DCGAN 由 Radford 等人在 2015 年提出，并在论文《Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks》中详细介绍。它主要改进了 GAN 的网络结构，通过使用卷积层和去卷积层（反卷积层）来替代传统的全连接层，从而提高生成器的表现力和稳定性。</p>
<h2 id="DCGAN-的改进点"><a href="#DCGAN-的改进点" class="headerlink" title="DCGAN 的改进点"></a>DCGAN 的改进点</h2><p>相较于标准 GAN，DCGAN 主要有以下改进：</p>
<ol>
<li><strong>使用卷积层</strong> 取代全连接层，使网络能够更好地学习图像的空间特征。</li>
<li><strong>使用批归一化（Batch Normalization）</strong> 来稳定训练，防止梯度消失或爆炸。</li>
<li><strong>去掉池化层（Pooling）</strong>，改用步长为 2 的卷积和反卷积操作来实现下采样和上采样。</li>
<li><strong>在生成器中使用 ReLU 激活函数</strong>（最后一层使用 Tanh），提高梯度流动性。</li>
<li><strong>在判别器中使用 LeakyReLU 激活函数</strong>，避免梯度完全消失。</li>
</ol>
<h1 id="DCGAN-结构"><a href="#DCGAN-结构" class="headerlink" title="DCGAN 结构"></a>DCGAN 结构</h1><p>DCGAN 仍然由**生成器（Generator）<strong>和</strong>判别器（Discriminator）**组成，它们的架构有所调整，以更好地适应图像数据。</p>
<h2 id="生成器（Generator）"><a href="#生成器（Generator）" class="headerlink" title="生成器（Generator）"></a>生成器（Generator）</h2><p>生成器的任务是将一个随机噪声向量 <code>Z</code> 转换为逼真的图像。</p>
<h3 id="生成器的结构"><a href="#生成器的结构" class="headerlink" title="生成器的结构"></a>生成器的结构</h3><ul>
<li>输入：随机噪声 <code>Z</code>（通常服从标准正态分布）。</li>
<li>通过一系列**转置卷积（反卷积）**层，将低维噪声转换为高维图像。</li>
<li>每一层使用<strong>批归一化（Batch Normalization）</strong>，避免训练不稳定。</li>
<li>隐藏层使用 <strong>ReLU 激活函数</strong>，最后一层使用 <strong>Tanh 激活函数</strong>，确保输出像素值在 <code>[-1,1]</code> 之间。</li>
</ul>
<h3 id="生成器的优化目标"><a href="#生成器的优化目标" class="headerlink" title="生成器的优化目标"></a>生成器的优化目标</h3><p>生成器的目标是欺骗判别器，使其认为生成的图像是真实的，即最小化以下损失函数：</p>
<p>$$<br>L_G &#x3D; -E[\log D(G(Z))]<br>$$</p>
<h2 id="判别器（Discriminator）"><a href="#判别器（Discriminator）" class="headerlink" title="判别器（Discriminator）"></a>判别器（Discriminator）</h2><p>判别器的任务是区分输入图像是真实的还是由生成器生成的。</p>
<h3 id="判别器的结构"><a href="#判别器的结构" class="headerlink" title="判别器的结构"></a>判别器的结构</h3><ul>
<li>输入：一张图像（可能是真实的，也可能是生成的）。</li>
<li>通过<strong>多个卷积层</strong>提取特征，每一层都使用 LeakyReLU 激活函数。</li>
<li>最后通过<strong>全连接层 + Sigmoid 激活函数</strong>，输出 <code>0</code> 或 <code>1</code>，表示假图像或真图像。</li>
</ul>
<h3 id="判别器的优化目标"><a href="#判别器的优化目标" class="headerlink" title="判别器的优化目标"></a>判别器的优化目标</h3><p>判别器的目标是正确区分真实图像 <code>X</code> 和生成图像 <code>G(Z)</code>，即最大化以下损失函数：</p>
<p>$$<br>L_D &#x3D; -E[\log D(X)] - E[\log(1 - D(G(Z)))]<br>$$</p>
<h1 id="DCGAN-训练过程"><a href="#DCGAN-训练过程" class="headerlink" title="DCGAN 训练过程"></a>DCGAN 训练过程</h1><p>训练 DCGAN 需要交替优化生成器和判别器，使二者不断进步，最终生成高质量的图像。</p>
<h2 id="训练步骤"><a href="#训练步骤" class="headerlink" title="训练步骤"></a>训练步骤</h2><ol>
<li><p><strong>判别器训练</strong></p>
<ul>
<li>使用真实图像 <code>X</code> 计算 <code>D(X)</code>，并最大化 <code>log(D(X))</code>。</li>
<li>使用生成器生成的图像 <code>G(Z)</code> 计算 <code>D(G(Z))</code>，并最大化 <code>log(1 - D(G(Z)))</code>。</li>
<li>计算损失 <code>L_D</code>，更新判别器参数。</li>
</ul>
</li>
<li><p><strong>生成器训练</strong></p>
<ul>
<li>生成器生成图像 <code>G(Z)</code>。</li>
<li>计算 <code>D(G(Z))</code>，希望让判别器将其判定为真实，即最大化 <code>log(D(G(Z)))</code>。</li>
<li>计算损失 <code>L_G</code>，更新生成器参数。</li>
</ul>
</li>
<li><p><strong>交替训练</strong></p>
<ul>
<li>通常先训练判别器几步，再训练生成器一步。</li>
<li>经过多个 epoch 的训练后，生成器可以生成高质量的图像。</li>
</ul>
</li>
</ol>
<h2 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h2><ul>
<li><strong>使用批归一化（BatchNorm）</strong>：避免训练不稳定。</li>
<li><strong>调整学习率</strong>：通常使用 Adam 优化器，学习率设为 0.0002。</li>
<li><strong>避免判别器过强</strong>：如果判别器训练得太好，生成器可能无法学习到有效的特征。</li>
<li><strong>使用标签平滑（Label Smoothing）</strong>：真实样本标签用 <code>0.9</code> 代替 <code>1.0</code>，避免梯度消失问题。</li>
</ul>
<h1 id="DCGAN-的应用"><a href="#DCGAN-的应用" class="headerlink" title="DCGAN 的应用"></a>DCGAN 的应用</h1><p>DCGAN 被广泛应用于图像生成任务，如：</p>
<ul>
<li><strong>人脸生成</strong>：如 CelebA 数据集训练的 DCGAN 可生成逼真的人脸。</li>
<li><strong>动漫风格生成</strong>：使用 DCGAN 训练动漫数据集，可生成风格化的角色。</li>
<li><strong>数据增强</strong>：可以用于补充稀缺数据，提高模型的泛化能力。</li>
<li><strong>艺术创作</strong>：可用于生成风格化的艺术作品。</li>
</ul>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>DCGAN 通过引入深度卷积网络（CNN）结构，使 GAN 训练更加稳定，生成的图像更加逼真。其主要改进包括：</p>
<ul>
<li><strong>去掉全连接层，使用卷积层和反卷积层</strong>。</li>
<li><strong>批归一化稳定训练，提高模型收敛性</strong>。</li>
<li><strong>改进激活函数，使用 ReLU 和 LeakyReLU 提高梯度流动性</strong>。</li>
<li><strong>去掉池化层，使用步长控制特征提取和生成过程</strong>。</li>
</ul>
<p>由于这些改进，DCGAN 在图像生成任务上表现优秀，并成为后续许多生成模型（如 StyleGAN、BigGAN）的基础。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/">模型学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DCGAN/" rel="tag">DCGAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GAN/" rel="tag">GAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-ACGAN" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/16/ACGAN/" class="article-date">
      <time datetime="2025-03-16T11:58:41.000Z" itemprop="datePublished">2025-03-16</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/16/ACGAN/">ACGAN</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>在之前的博客中，我们介绍了通用的GAN代码实现，它能够生成图像，但无法生成<strong>指定类别</strong>的图像。ACGAN（Auxiliary Classifier GAN）弥补了这一缺陷，通过在生成器和判别器中引入类别信息，使模型能够生成<strong>特定类别</strong>的数据。</p>
<p>ACGAN的生成器<strong>接收一个随机噪声和一个图像标签作为输入</strong>，并输出一张符合该类别的图像。而判别器不仅要判断输入图像的真假，还要<strong>同时输出图像的类别标签</strong>。</p>
<h1 id="ACGAN简介"><a href="#ACGAN简介" class="headerlink" title="ACGAN简介"></a>ACGAN简介</h1><p>ACGAN的最大特点是<strong>既能生成图像，又能进行分类</strong>。它是对传统GAN的扩展，由Ian Goodfellow等人在2014年提出的GAN基础上进一步发展而来。ACGAN通过<strong>引入条件控制</strong>，使生成过程受到额外信息的指导，从而能够生成具有特定属性或风格的数据。</p>
<h2 id="ACGAN的原理"><a href="#ACGAN的原理" class="headerlink" title="ACGAN的原理"></a>ACGAN的原理</h2><p>ACGAN的基本原理依然基于GAN的对抗训练框架，包括一个<strong>生成器（Generator）<strong>和一个</strong>判别器（Discriminator）</strong>，但两者都接受额外的类别信息（Conditional Information），例如类别标签、文本描述等。</p>
<ul>
<li><p><strong>生成器（Generator）</strong></p>
<ul>
<li>接收随机噪声向量 <code>Z</code> 和条件信息 <code>C</code>（如类别标签）。</li>
<li>结合 <code>Z</code> 和 <code>C</code> 生成符合指定类别的数据样本。</li>
</ul>
</li>
<li><p><strong>判别器（Discriminator）</strong></p>
<ul>
<li>输入一张图像，判断其是真实的还是生成的。</li>
<li>额外输出该图像的类别标签。</li>
</ul>
</li>
</ul>
<h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><ul>
<li><strong>对抗训练</strong>：生成器不断优化，使其生成的样本足够真实，能够欺骗判别器；判别器则不断学习，以更准确地区分真实数据与生成数据。</li>
<li><strong>损失函数</strong>：<ul>
<li><strong>生成器的损失</strong> &#x3D; GAN 损失 + 分类损失（生成样本的类别正确性）。</li>
<li><strong>判别器的损失</strong> &#x3D; 真实样本的真假判断损失 + 生成样本的真假判断损失 + 分类损失。</li>
</ul>
</li>
</ul>
<p>这种设计不仅提高了GAN的生成质量，还能确保生成的样本具有正确的类别信息。</p>
<h1 id="ACGAN的关键机制——辅助分类器（Auxiliary-Classifier）"><a href="#ACGAN的关键机制——辅助分类器（Auxiliary-Classifier）" class="headerlink" title="ACGAN的关键机制——辅助分类器（Auxiliary Classifier）"></a>ACGAN的关键机制——辅助分类器（Auxiliary Classifier）</h1><p>GAN的传统机制可以概括为：输入随机噪声，输出伪造样本。然而，这样的生成方式缺乏约束，就像火车没有轨道一样，生成结果的方向不可控。为了解决这个问题，研究者提出了CGAN（Conditional GAN），通过向GAN添加辅助标签，使生成过程更加精准。</p>
<p>ACGAN是CGAN的扩展，<strong>在判别器中增加了辅助分类器（Auxiliary Classifier）</strong>，用于预测输入数据的类别信息。这使得模型不仅能够判断数据真假，还能进一步分类，提高生成数据的可控性。</p>
<h2 id="辅助分类器的结构"><a href="#辅助分类器的结构" class="headerlink" title="辅助分类器的结构"></a>辅助分类器的结构</h2><p>辅助分类器是一种神经网络组件，通常嵌入判别器中，负责预测输入数据的类别。其一般结构如下：</p>
<ol>
<li><strong>输入层</strong>：接收图像数据及其类别标签（或其他条件信息）。</li>
<li><strong>特征提取层</strong>：使用卷积层（对于图像数据）或全连接层（对于其他数据）提取特征。</li>
<li><strong>条件融合层</strong>：在某些模型中，特征与条件信息会进一步融合（如拼接、元素乘法或注意力机制）。</li>
<li><strong>分类层</strong>：采用全连接层输出类别预测结果，通常使用 softmax 计算类别概率分布。</li>
<li><strong>损失函数</strong>：使用交叉熵损失衡量预测类别与真实类别之间的差距。</li>
</ol>
<h2 id="判别器的整体结构"><a href="#判别器的整体结构" class="headerlink" title="判别器的整体结构"></a>判别器的整体结构</h2><p>ACGAN的判别器由两个分支组成：</p>
<ul>
<li><strong>主分类器分支</strong>：判断输入数据是真实的还是生成的。</li>
<li><strong>辅助分类器分支</strong>：预测输入数据的类别。</li>
</ul>
<p>判别器的总损失是主分类器损失与辅助分类器损失的加权组合，从而保证它既能正确区分真假数据，又能准确分类。</p>
<h1 id="ACGAN的条件分类实现过程"><a href="#ACGAN的条件分类实现过程" class="headerlink" title="ACGAN的条件分类实现过程"></a>ACGAN的条件分类实现过程</h1><p>ACGAN的核心目标是通过条件信息控制数据的生成过程，使模型能够按照给定的类别生成特定的数据。</p>
<h2 id="生成器（Generator）"><a href="#生成器（Generator）" class="headerlink" title="生成器（Generator）"></a>生成器（Generator）</h2><ol>
<li><strong>输入</strong>：<ul>
<li>随机噪声向量 <code>Z</code>（通常来自高斯分布）。</li>
<li>条件向量 <code>C</code>（通常是 one-hot 编码的类别标签）。</li>
</ul>
</li>
<li><strong>条件嵌入</strong>：<ul>
<li><code>C</code> 通过嵌入层转换为与 <code>Z</code> 维度相同的向量。</li>
<li><code>Z</code> 和 <code>C</code> 进行合并，形成新的输入向量。</li>
</ul>
</li>
<li><strong>生成数据</strong>：<ul>
<li>通过深度神经网络（如卷积层、反卷积层等）生成数据样本。</li>
</ul>
</li>
</ol>
<h2 id="判别器（Discriminator）"><a href="#判别器（Discriminator）" class="headerlink" title="判别器（Discriminator）"></a>判别器（Discriminator）</h2><ol>
<li><strong>输入</strong>：<ul>
<li>真实数据样本或生成器生成的数据样本。</li>
<li>条件向量 <code>C</code>。</li>
</ul>
</li>
<li><strong>特征提取</strong>：<ul>
<li>通过多个卷积层提取特征。</li>
</ul>
</li>
<li><strong>两个输出分支</strong>：<ul>
<li><strong>真假判断分支</strong>：判断样本是真实的还是生成的。</li>
<li><strong>辅助分类器分支</strong>：预测样本的类别。</li>
</ul>
</li>
<li><strong>损失函数</strong>：<ul>
<li><strong>对抗损失</strong>：用于真假判断。</li>
<li><strong>分类损失</strong>：用于类别预测。</li>
</ul>
</li>
</ol>
<h2 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h2><ol>
<li><strong>判别器训练</strong><ul>
<li>使用真实数据计算对抗损失和分类损失。</li>
<li>使用生成数据计算对抗损失和分类损失。</li>
<li>计算总损失并优化判别器。</li>
</ul>
</li>
<li><strong>生成器训练</strong><ul>
<li>生成数据，试图欺骗判别器，使其误判为真实数据。</li>
<li>计算生成样本的分类损失。</li>
<li>计算总损失并优化生成器。</li>
</ul>
</li>
<li><strong>交替训练</strong><ul>
<li>通常先更新判别器几次，再更新生成器一次。</li>
</ul>
</li>
</ol>
<h2 id="训练优化建议"><a href="#训练优化建议" class="headerlink" title="训练优化建议"></a>训练优化建议</h2><ul>
<li><strong>条件信息合并方式</strong>：直接拼接或使用注意力机制。</li>
<li><strong>损失权重调整</strong>：对抗损失和分类损失需要适当平衡。</li>
<li><strong>训练稳定性</strong>：调整学习率、批量大小等超参数，提高训练稳定性。</li>
</ul>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>ACGAN通过辅助分类器的引入，使得生成器能够生成<strong>带有特定类别属性</strong>的数据，同时保持GAN的对抗训练优势。这使得ACGAN在<strong>图像生成、风格迁移、数据增强等领域</strong>具备广泛的应用价值。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/">模型学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ACGAN/" rel="tag">ACGAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GAN/" rel="tag">GAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%A8%A1%E5%9E%8B/" rel="tag">模型</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-SVM" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/15/SVM/" class="article-date">
      <time datetime="2025-03-15T13:45:00.000Z" itemprop="datePublished">2025-03-15</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/15/SVM/">支持向量机（SVM）详解</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>支持向量机（Support Vector Machine, SVM）是一种常用于分类和回归的机器学习算法，因其优秀的泛化能力和数学理论基础，在数据挖掘、文本分类、人脸识别等任务中广泛应用。</p>
<h1 id="1-SVM-基本概念"><a href="#1-SVM-基本概念" class="headerlink" title="1. SVM 基本概念"></a>1. SVM 基本概念</h1><p>SVM 的核心思想是找到一个最优的<strong>超平面（Hyperplane）</strong>，用于划分数据，使不同类别的数据点尽可能分开，并最大化<strong>间隔（Margin）</strong>。</p>
<ul>
<li><strong>超平面</strong>：在二维空间中，超平面是一条直线；在三维空间中，它是一个平面；在更高维度的空间中，仍称之为超平面。</li>
<li><strong>支持向量（Support Vectors）</strong>：距离超平面最近的点，这些点决定了最优超平面的位置。</li>
<li><strong>间隔（Margin）</strong>：支持向量到超平面的最小距离，SVM 试图最大化间隔，以提高泛化能力。</li>
</ul>
<h1 id="2-SVM-的数学原理"><a href="#2-SVM-的数学原理" class="headerlink" title="2. SVM 的数学原理"></a>2. SVM 的数学原理</h1><p>假设我们的数据集是线性可分的，给定训练样本 ( (x_i, y_i) )，其中 ( x_i ) 是特征向量，( y_i \in {-1, 1} ) 表示类别标签。</p>
<p>超平面的方程可以表示为：<br>[ w \cdot x + b &#x3D; 0 ]<br>其中 ( w ) 是法向量，决定了超平面的方向，( b ) 是偏置项。</p>
<p>为了找到最优超平面，我们需要最大化间隔 ( \frac{2}{||w||} )，等价于最小化 ( ||w||^2 )，同时保证所有样本点被正确分类：</p>
<p>[ y_i (w \cdot x_i + b) \geq 1, \quad \forall i ]</p>
<p>这就是<strong>硬间隔 SVM</strong> 的优化问题。</p>
<h3 id="2-1-软间隔-SVM"><a href="#2-1-软间隔-SVM" class="headerlink" title="2.1 软间隔 SVM"></a>2.1 软间隔 SVM</h3><p>在现实数据中，可能存在部分噪声数据，使得严格的线性可分难以实现。因此，我们引入<strong>松弛变量</strong> ( \xi_i ) 来允许一定的误分类：</p>
<p>[ \min \frac{1}{2} ||w||^2 + C \sum \xi_i ]</p>
<p>其中 ( C ) 是超参数，控制间隔最大化与误分类的权衡。</p>
<h3 id="2-2-核函数（Kernel-Trick）"><a href="#2-2-核函数（Kernel-Trick）" class="headerlink" title="2.2 核函数（Kernel Trick）"></a>2.2 核函数（Kernel Trick）</h3><p>当数据是非线性可分时，SVM 使用<strong>核函数（Kernel Function）</strong> 将数据映射到高维特征空间，使其在高维空间中线性可分。</p>
<p>常见核函数包括：</p>
<ul>
<li><strong>线性核函数</strong>：( K(x_i, x_j) &#x3D; x_i \cdot x_j )</li>
<li><strong>多项式核函数</strong>：( K(x_i, x_j) &#x3D; (x_i \cdot x_j + c)^d )</li>
<li><strong>高斯径向基核（RBF 核）</strong>：( K(x_i, x_j) &#x3D; e^{-\gamma ||x_i - x_j||^2} )</li>
</ul>
<h1 id="3-SVM-的优缺点"><a href="#3-SVM-的优缺点" class="headerlink" title="3. SVM 的优缺点"></a>3. SVM 的优缺点</h1><p>✅ <strong>优点</strong>：</p>
<ul>
<li>适用于高维数据，尤其是文本分类等任务。</li>
<li>通过核技巧处理非线性问题。</li>
<li>具有良好的泛化能力。</li>
</ul>
<p>❌ <strong>缺点</strong>：</p>
<ul>
<li>对于大规模数据集，训练时间较长。</li>
<li>需要选择合适的核函数，否则可能导致过拟合。</li>
<li>对噪声较敏感。</li>
</ul>
<h1 id="4-SVM-的-Python-实现示例"><a href="#4-SVM-的-Python-实现示例" class="headerlink" title="4. SVM 的 Python 实现示例"></a>4. SVM 的 Python 实现示例</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X, y = iris.data, iris.target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练 SVM 模型</span></span><br><span class="line">clf = SVC(kernel=<span class="string">&#x27;rbf&#x27;</span>, C=<span class="number">1.0</span>, gamma=<span class="string">&#x27;scale&#x27;</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测 &amp; 评估</span></span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy:&quot;</span>, accuracy_score(y_test, y_pred))</span><br></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/">模型学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SVM/" rel="tag">SVM</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/" rel="tag">分类算法</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" rel="tag">支持向量机</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-逻辑回归" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/14/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" class="article-date">
      <time datetime="2025-03-14T14:15:00.000Z" itemprop="datePublished">2025-03-14</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/14/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/">逻辑回归（Logistic Regression）详解</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>逻辑回归（Logistic Regression）是一种常见的分类算法，尽管名字中带有“回归”，但它主要用于<strong>二分类问题</strong>，也可以扩展到多分类任务。</p>
<h1 id="1-逻辑回归的基本原理"><a href="#1-逻辑回归的基本原理" class="headerlink" title="1. 逻辑回归的基本原理"></a>1. 逻辑回归的基本原理</h1><p>逻辑回归的核心思想是使用**逻辑函数（Sigmoid）**将线性回归的输出转换为概率值：</p>
<p>[<br>\sigma(z) &#x3D; \frac{1}{1 + e^{-z}}<br>]</p>
<p>其中，( z ) 是线性回归的结果：</p>
<p>[<br>z &#x3D; w_1x_1 + w_2x_2 + … + w_nx_n + b<br>]</p>
<p>通过 Sigmoid 函数，逻辑回归可以输出一个介于 0 和 1 之间的概率值，并根据设定的阈值（通常为 0.5）将样本分类为 0 或 1。</p>
<h1 id="2-逻辑回归的决策边界"><a href="#2-逻辑回归的决策边界" class="headerlink" title="2. 逻辑回归的决策边界"></a>2. 逻辑回归的决策边界</h1><p>逻辑回归的决策边界是由权重和偏置确定的超平面。例如：</p>
<ul>
<li>在<strong>二维</strong>数据中，决策边界是<strong>一条直线</strong>。</li>
<li>在<strong>三维</strong>数据中，决策边界是<strong>一个平面</strong>。</li>
<li>在更高维数据中，决策边界是<strong>一个超平面</strong>。</li>
</ul>
<h1 id="3-逻辑回归的损失函数"><a href="#3-逻辑回归的损失函数" class="headerlink" title="3. 逻辑回归的损失函数"></a>3. 逻辑回归的损失函数</h1><p>为了优化逻辑回归模型，我们使用<strong>对数损失函数（Log Loss）</strong>，即<strong>交叉熵损失（Cross-Entropy Loss）</strong>：</p>
<p>[<br>L &#x3D; - \frac{1}{m} \sum_{i&#x3D;1}^{m} [ y_i \log \hat{y}_i + (1 - y_i) \log (1 - \hat{y}_i) ]<br>]</p>
<p>其中：</p>
<ul>
<li>( y_i ) 是真实标签（0 或 1）。</li>
<li>( \hat{y}_i ) 是预测的概率值。</li>
<li>( m ) 是样本总数。</li>
</ul>
<h1 id="4-逻辑回归的优化方法"><a href="#4-逻辑回归的优化方法" class="headerlink" title="4. 逻辑回归的优化方法"></a>4. 逻辑回归的优化方法</h1><p>逻辑回归使用**梯度下降（Gradient Descent）**来最小化损失函数，更新权重 ( w ) 和偏置 ( b )：</p>
<p>[<br>w &#x3D; w - \alpha \frac{\partial L}{\partial w}, \quad b &#x3D; b - \alpha \frac{\partial L}{\partial b}<br>]</p>
<p>其中 ( \alpha ) 是学习率。</p>
<p>常见优化方法：</p>
<ul>
<li><strong>批量梯度下降（BGD）</strong></li>
<li><strong>随机梯度下降（SGD）</strong></li>
<li><strong>小批量梯度下降（Mini-batch SGD）</strong></li>
</ul>
<h1 id="5-逻辑回归的-Python-实现"><a href="#5-逻辑回归的-Python-实现" class="headerlink" title="5. 逻辑回归的 Python 实现"></a>5. 逻辑回归的 Python 实现</h1><p>使用 <code>scikit-learn</code> 进行逻辑回归建模：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据集</span></span><br><span class="line">X, y = make_classification(n_samples=<span class="number">1000</span>, n_features=<span class="number">2</span>, random_state=<span class="number">42</span>)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练逻辑回归模型</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;模型准确率: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h1 id="6-逻辑回归的优缺点"><a href="#6-逻辑回归的优缺点" class="headerlink" title="6. 逻辑回归的优缺点"></a>6. 逻辑回归的优缺点</h1><p><strong>优点</strong>：</p>
<ul>
<li>计算简单，易于实现。</li>
<li>训练速度快，适用于大规模数据。</li>
<li>结果可解释性强，能提供特征权重。</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>只能处理<strong>线性可分</strong>问题，无法解决复杂非线性关系。</li>
<li>对异常值较敏感。</li>
<li>不能自动进行特征选择，可能需要手动筛选。</li>
</ul>
<h1 id="7-逻辑回归的扩展"><a href="#7-逻辑回归的扩展" class="headerlink" title="7. 逻辑回归的扩展"></a>7. 逻辑回归的扩展</h1><ul>
<li><strong>多分类逻辑回归（Softmax 回归）</strong>：用于多类别分类任务。</li>
<li><strong>正则化逻辑回归（L1&#x2F;L2 正则化）</strong>：防止过拟合。</li>
<li><strong>带核的逻辑回归（Kernel Logistic Regression）</strong>：适用于非线性数据。</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/">模型学习</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/" rel="tag">分类模型</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" rel="tag">逻辑回归</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-WGAN-WGAN-GP-实现手写数字生成" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/13/WGAN-WGAN-GP-%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E7%94%9F%E6%88%90/" class="article-date">
      <time datetime="2025-03-13T14:26:19.000Z" itemprop="datePublished">2025-03-13</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/13/WGAN-WGAN-GP-%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E7%94%9F%E6%88%90/">WGAN/WGAN-GP 实现手写数字生成</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><ol>
<li>本次挑战使用的MNIST手写数字数据集，包含60,000张28x28的灰度图像，分为10个类别（数字0-9）。此数据集将用于训练你的生成对抗网络。</li>
<li>你的任务是使用DCGAN模型，对该数据集进行图像生成。具体要求如下：<ol>
<li>数据集下载：请下载MNIST数据集，并确保数据集中包含训练集和测试集。</li>
<li>数据预处理：将图像数据进行必要的预处理，使其适合于DCGAN模型的训练。</li>
<li>模型训练：搭建DCGAN模型，并利用训练数据集进行训练，调整模型参数，尝试生成高质量的数字图像。</li>
<li>模型评估：在训练过程中，监控生成图像的质量，并可视化不同训练阶段生成的图像。</li>
</ol>
</li>
</ol>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><h4 id="WGAN"><a href="#WGAN" class="headerlink" title="WGAN"></a>WGAN</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torchvision.utils <span class="keyword">as</span> vutils</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">lr = <span class="number">0.00005</span>  <span class="comment"># WGAN采用较小的学习率</span></span><br><span class="line">noise_dim = <span class="number">100</span></span><br><span class="line">epochs = <span class="number">20</span></span><br><span class="line">channel_size = <span class="number">1</span></span><br><span class="line">critic_iter = <span class="number">5</span>  <span class="comment"># 判别器训练次数</span></span><br><span class="line">weight_clip = <span class="number">0.01</span>  <span class="comment"># 权重裁剪范围</span></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">os.makedirs(<span class="string">&quot;output_wgan&quot;</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Grayscale(num_output_channels=<span class="number">1</span>),  <span class="comment"># 修改默认的图像通道数</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 ImageFolder 读取数据</span></span><br><span class="line">dataset = datasets.ImageFolder(root=<span class="string">&#x27;data/mnist_jpg&#x27;</span>, transform=transform)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    基于卷积层的生成器</span></span><br><span class="line"><span class="string">    和 DCGAN 相同</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, noise_dim, channel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.main = nn.Sequential(</span><br><span class="line">            nn.ConvTranspose2d(noise_dim, <span class="number">128</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.ConvTranspose2d(<span class="number">128</span>, <span class="number">64</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.ConvTranspose2d(<span class="number">64</span>, channel_size, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.main(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Discriminator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    基于卷积层的判别器</span></span><br><span class="line"><span class="string">    和 DCGAN 相同</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.main = nn.Sequential(</span><br><span class="line">            nn.Conv2d(channel_size, <span class="number">64</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.flatten = nn.Flatten()</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(<span class="number">128</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.main(<span class="built_in">input</span>)</span><br><span class="line">        x = <span class="variable language_">self</span>.flatten(x)</span><br><span class="line">        output = <span class="variable language_">self</span>.fc(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型,优化器</span></span><br><span class="line">netG = Generator(noise_dim, channel_size).to(device)</span><br><span class="line">netD = Discriminator(channel_size).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 原论文建议使用 RMSprop 优化器</span></span><br><span class="line">optimizerD = optim.RMSprop(netD.parameters(), lr=lr)</span><br><span class="line">optimizerG = optim.RMSprop(netG.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> i, (data, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        real_imgs = data.to(device)</span><br><span class="line">        batch_size = real_imgs.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 判别器训练</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(critic_iter):</span><br><span class="line">            netD.zero_grad()</span><br><span class="line">            noise = torch.randn(batch_size, noise_dim, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">            fake_imgs = netG(noise)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算 Wasserstein 损失</span></span><br><span class="line">            lossD = -netD(real_imgs).mean() + netD(fake_imgs.detach()).mean()</span><br><span class="line">            lossD.backward()</span><br><span class="line">            optimizerD.step()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 权重裁剪</span></span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> netD.parameters():</span><br><span class="line">                p.data.clamp_(-weight_clip, weight_clip)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成器训练</span></span><br><span class="line">        netG.zero_grad()</span><br><span class="line">        fake_imgs = netG(noise)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算 Wasserstein 损失</span></span><br><span class="line">        lossG = -netD(fake_imgs).mean()</span><br><span class="line">        lossG.backward()</span><br><span class="line">        optimizerG.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>] Batch <span class="subst">&#123;i&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(dataloader)&#125;</span> Loss_D: <span class="subst">&#123;lossD.item():<span class="number">.4</span>f&#125;</span> Loss_G: <span class="subst">&#123;lossG.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存生成结果</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        fixed_noise = torch.randn(<span class="number">16</span>, noise_dim, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">        fake = netG(fixed_noise)</span><br><span class="line">    vutils.save_image(fake, <span class="string">f&quot;output_wgan/fake_samples_epoch_<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>.png&quot;</span>, normalize=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h4 id="WGAN-GP"><a href="#WGAN-GP" class="headerlink" title="WGAN-GP"></a>WGAN-GP</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torchvision.utils <span class="keyword">as</span> vutils</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line">noise_dim = <span class="number">100</span></span><br><span class="line">epochs = <span class="number">20</span></span><br><span class="line">channel_size = <span class="number">1</span></span><br><span class="line">lambda_gp = <span class="number">10</span>  <span class="comment"># 梯度惩罚系数</span></span><br><span class="line">critic_iterations = <span class="number">5</span>  <span class="comment"># 判别器训练次数</span></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">os.makedirs(<span class="string">&quot;output_wgan_gp&quot;</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Grayscale(num_output_channels=<span class="number">1</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">dataset = datasets.ImageFolder(root=<span class="string">&#x27;data/mnist_jpg&#x27;</span>, transform=transform)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    基于卷积层的生成器</span></span><br><span class="line"><span class="string">    和 DCGAN 相同</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, noise_dim, channel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.main = nn.Sequential(</span><br><span class="line">            nn.ConvTranspose2d(noise_dim, <span class="number">128</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.ConvTranspose2d(<span class="number">128</span>, <span class="number">64</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.ConvTranspose2d(<span class="number">64</span>, channel_size, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.main(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Critic</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    基于卷积层的判别器</span></span><br><span class="line"><span class="string">    和 DCGAN 相同</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.main = nn.Sequential(</span><br><span class="line">            nn.Conv2d(channel_size, <span class="number">64</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.flatten = nn.Flatten()</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(<span class="number">128</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.main(<span class="built_in">input</span>)</span><br><span class="line">        x = <span class="variable language_">self</span>.flatten(x)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_gradient_penalty</span>(<span class="params">critic, real_samples, fake_samples</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算梯度惩罚</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 随机数 alpha作为插值的权重, interpolates 是在真实样本和假样本之间的插值数据</span></span><br><span class="line">    alpha = torch.rand(real_samples.size(<span class="number">0</span>), <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">    interpolates = (alpha * real_samples + (<span class="number">1</span> - alpha) * fake_samples).requires_grad_(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 得到判别器的结果</span></span><br><span class="line">    critic_interpolates = critic(interpolates)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># torch.autograd.grad用于计算导数，将梯度计算的结果存储在 gradients 变量中</span></span><br><span class="line">    gradients = torch.autograd.grad(outputs=critic_interpolates, inputs=interpolates,</span><br><span class="line">                                    grad_outputs=torch.ones_like(critic_interpolates),</span><br><span class="line">                                    create_graph=<span class="literal">True</span>, retain_graph=<span class="literal">True</span>, only_inputs=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将梯度展平 (Batch, channels, height, width) -&gt; (Batch, channels * height * width)</span></span><br><span class="line">    gradients = gradients.view(gradients.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算梯度惩罚, 公式为((梯度的L2范数 - 1) ^ 2)的均值</span></span><br><span class="line">        <span class="comment"># 理想情况下，Lipschitz常数应该为1，因此梯度的L2范数（gradients.norm(2, dim=1)）应该接近1。</span></span><br><span class="line">        <span class="comment"># 如果它大于1或小于1，都会给模型带来惩罚，以倒逼判别器的梯度符合要求。</span></span><br><span class="line">    gradient_penalty = ((gradients.norm(<span class="number">2</span>, dim=<span class="number">1</span>) - <span class="number">1</span>) ** <span class="number">2</span>).mean()</span><br><span class="line">    <span class="keyword">return</span> gradient_penalty</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型,优化器</span></span><br><span class="line">netG = Generator(noise_dim, channel_size).to(device)</span><br><span class="line">netC = Critic(channel_size).to(device)</span><br><span class="line"></span><br><span class="line">optimizerC = optim.Adam(netC.parameters(), lr=lr, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line">optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> i, (data, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        real_imgs = data.to(device)</span><br><span class="line">        batch_size = real_imgs.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练判别器</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(critic_iterations):</span><br><span class="line">            netC.zero_grad()</span><br><span class="line">            noise = torch.randn(batch_size, noise_dim, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">            fake_imgs = netG(noise)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算 Wasserstein 损失</span></span><br><span class="line">            lossC_real = -netC(real_imgs).mean() + netC(fake_imgs.detach()).mean()</span><br><span class="line">            <span class="comment"># 计算梯度惩罚</span></span><br><span class="line">            gradient_penalty = compute_gradient_penalty(netC, real_imgs, fake_imgs.detach())</span><br><span class="line">            lossC = lossC_real + lambda_gp * gradient_penalty <span class="comment"># 增加梯度惩罚</span></span><br><span class="line"></span><br><span class="line">            lossC.backward()</span><br><span class="line">            optimizerC.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练生成器</span></span><br><span class="line">        netG.zero_grad()</span><br><span class="line">        fake_imgs = netG(noise)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算 Wasserstein 损失</span></span><br><span class="line">        lossG = -netC(fake_imgs).mean()</span><br><span class="line">        lossG.backward()</span><br><span class="line">        optimizerG.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>] Batch <span class="subst">&#123;i&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(dataloader)&#125;</span> Loss_C: <span class="subst">&#123;lossC.item():<span class="number">.4</span>f&#125;</span> Loss_G: <span class="subst">&#123;lossG.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存每个 epoch 的生成结果</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        fixed_noise = torch.randn(<span class="number">16</span>, noise_dim, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">        fake = netG(fixed_noise)</span><br><span class="line">    vutils.save_image(fake, <span class="string">f&quot;output_wgan_gp/fake_samples_epoch_<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>.png&quot;</span>, normalize=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>10 epoches:<br><img src="/img/wgan_mnist.png" alt="alt text"></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/project/">project</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GAN/" rel="tag">GAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/WGAN/" rel="tag">WGAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/WGAN-GP/" rel="tag">WGAN-GP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/code/" rel="tag">code</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/3/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2025 John Doe
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="A fast, simple &amp; powerful blog framework">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="Another simple and elegant theme for Hexo  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="Site Visitors"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="Page Hits"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





<div class="scroll" id="scroll">
    <a href="#" title="Back to Top"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="Comments"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="Go to Bottom"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>