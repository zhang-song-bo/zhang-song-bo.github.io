<!DOCTYPE html>
<html lang="en">
<head>

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no" />
<meta name="author" content="John Doe" />


    
    


<meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">

<link rel="apple-touch-icon" href= "/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">



<link rel="stylesheet" href="/css/style.css">



    <style> .article { opacity: 0;} </style>


<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>Hexo</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: 
    }
</script>


    <script> yiliaConfig.jquery_ui = [false]; </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






<meta name="generator" content="Hexo 7.3.0"></head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/avatar.jpg" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/"></a></h1>
        </hgroup>

        

        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>Menu</li>
                        <li>Tags</li>
                        
                        <li>Friends</li>
                        
                        
                        <li>About Me</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                            <li><a href="/categories/%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/">模型学习</a></li>
                        
                            <li><a href="/categories/%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/">优化技术</a></li>
                        
                            <li><a href="/categories/project/">小试牛刀</a></li>
                        
                            <li><a href="/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E8%AE%B0/">生活随记</a></li>
                        
                            <li><a href="/archives/">归档</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ACGAN/" rel="tag">ACGAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/" rel="tag">BERT</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CycleGAN/" rel="tag">CycleGAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DCGAN/" rel="tag">DCGAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GAN/" rel="tag">GAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/WGAN/" rel="tag">WGAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/WGAN-GP/" rel="tag">WGAN-GP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/code/" rel="tag">code</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A8%A1%E5%9E%8B/" rel="tag">模型</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="tag">线性回归</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9A%8F%E8%AE%B0/" rel="tag">随记</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="http://moxfive.xyz/">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">专注于前端</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页"></a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/avatar.jpg" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页"></a></h1>
            </hgroup>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/about/">关于我</a></li>
                
                    <li><a href="/categories/%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/">模型学习</a></li>
                
                    <li><a href="/categories/%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/">优化技术</a></li>
                
                    <li><a href="/categories/project/">小试牛刀</a></li>
                
                    <li><a href="/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E8%AE%B0/">生活随记</a></li>
                
                    <li><a href="/archives/">归档</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="Tags" friends="Friends" about="About Me"/>
</nav>
      <div class="body-wrap">
  
    <article id="post-WGAN-WGAN-GP-实现手写数字生成" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/13/WGAN-WGAN-GP-%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E7%94%9F%E6%88%90/" class="article-date">
      <time datetime="2025-03-13T14:26:19.000Z" itemprop="datePublished">2025-03-13</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/13/WGAN-WGAN-GP-%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E7%94%9F%E6%88%90/">WGAN/WGAN-GP 实现手写数字生成</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><ol>
<li>本次挑战使用的MNIST手写数字数据集，包含60,000张28x28的灰度图像，分为10个类别（数字0-9）。此数据集将用于训练你的生成对抗网络。</li>
<li>你的任务是使用DCGAN模型，对该数据集进行图像生成。具体要求如下：<ol>
<li>数据集下载：请下载MNIST数据集，并确保数据集中包含训练集和测试集。</li>
<li>数据预处理：将图像数据进行必要的预处理，使其适合于DCGAN模型的训练。</li>
<li>模型训练：搭建DCGAN模型，并利用训练数据集进行训练，调整模型参数，尝试生成高质量的数字图像。</li>
<li>模型评估：在训练过程中，监控生成图像的质量，并可视化不同训练阶段生成的图像。</li>
</ol>
</li>
</ol>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><h4 id="WGAN"><a href="#WGAN" class="headerlink" title="WGAN"></a>WGAN</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torchvision.utils <span class="keyword">as</span> vutils</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">lr = <span class="number">0.00005</span>  <span class="comment"># WGAN采用较小的学习率</span></span><br><span class="line">noise_dim = <span class="number">100</span></span><br><span class="line">epochs = <span class="number">20</span></span><br><span class="line">channel_size = <span class="number">1</span></span><br><span class="line">critic_iter = <span class="number">5</span>  <span class="comment"># 判别器训练次数</span></span><br><span class="line">weight_clip = <span class="number">0.01</span>  <span class="comment"># 权重裁剪范围</span></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">os.makedirs(<span class="string">&quot;output_wgan&quot;</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Grayscale(num_output_channels=<span class="number">1</span>),  <span class="comment"># 修改默认的图像通道数</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 ImageFolder 读取数据</span></span><br><span class="line">dataset = datasets.ImageFolder(root=<span class="string">&#x27;data/mnist_jpg&#x27;</span>, transform=transform)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    基于卷积层的生成器</span></span><br><span class="line"><span class="string">    和 DCGAN 相同</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, noise_dim, channel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.main = nn.Sequential(</span><br><span class="line">            nn.ConvTranspose2d(noise_dim, <span class="number">128</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.ConvTranspose2d(<span class="number">128</span>, <span class="number">64</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.ConvTranspose2d(<span class="number">64</span>, channel_size, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.main(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Discriminator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    基于卷积层的判别器</span></span><br><span class="line"><span class="string">    和 DCGAN 相同</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.main = nn.Sequential(</span><br><span class="line">            nn.Conv2d(channel_size, <span class="number">64</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.flatten = nn.Flatten()</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(<span class="number">128</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.main(<span class="built_in">input</span>)</span><br><span class="line">        x = <span class="variable language_">self</span>.flatten(x)</span><br><span class="line">        output = <span class="variable language_">self</span>.fc(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型,优化器</span></span><br><span class="line">netG = Generator(noise_dim, channel_size).to(device)</span><br><span class="line">netD = Discriminator(channel_size).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 原论文建议使用 RMSprop 优化器</span></span><br><span class="line">optimizerD = optim.RMSprop(netD.parameters(), lr=lr)</span><br><span class="line">optimizerG = optim.RMSprop(netG.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> i, (data, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        real_imgs = data.to(device)</span><br><span class="line">        batch_size = real_imgs.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 判别器训练</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(critic_iter):</span><br><span class="line">            netD.zero_grad()</span><br><span class="line">            noise = torch.randn(batch_size, noise_dim, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">            fake_imgs = netG(noise)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算 Wasserstein 损失</span></span><br><span class="line">            lossD = -netD(real_imgs).mean() + netD(fake_imgs.detach()).mean()</span><br><span class="line">            lossD.backward()</span><br><span class="line">            optimizerD.step()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 权重裁剪</span></span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> netD.parameters():</span><br><span class="line">                p.data.clamp_(-weight_clip, weight_clip)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成器训练</span></span><br><span class="line">        netG.zero_grad()</span><br><span class="line">        fake_imgs = netG(noise)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算 Wasserstein 损失</span></span><br><span class="line">        lossG = -netD(fake_imgs).mean()</span><br><span class="line">        lossG.backward()</span><br><span class="line">        optimizerG.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>] Batch <span class="subst">&#123;i&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(dataloader)&#125;</span> Loss_D: <span class="subst">&#123;lossD.item():<span class="number">.4</span>f&#125;</span> Loss_G: <span class="subst">&#123;lossG.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存生成结果</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        fixed_noise = torch.randn(<span class="number">16</span>, noise_dim, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">        fake = netG(fixed_noise)</span><br><span class="line">    vutils.save_image(fake, <span class="string">f&quot;output_wgan/fake_samples_epoch_<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>.png&quot;</span>, normalize=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h4 id="WGAN-GP"><a href="#WGAN-GP" class="headerlink" title="WGAN-GP"></a>WGAN-GP</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torchvision.utils <span class="keyword">as</span> vutils</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line">noise_dim = <span class="number">100</span></span><br><span class="line">epochs = <span class="number">20</span></span><br><span class="line">channel_size = <span class="number">1</span></span><br><span class="line">lambda_gp = <span class="number">10</span>  <span class="comment"># 梯度惩罚系数</span></span><br><span class="line">critic_iterations = <span class="number">5</span>  <span class="comment"># 判别器训练次数</span></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">os.makedirs(<span class="string">&quot;output_wgan_gp&quot;</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Grayscale(num_output_channels=<span class="number">1</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">dataset = datasets.ImageFolder(root=<span class="string">&#x27;data/mnist_jpg&#x27;</span>, transform=transform)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    基于卷积层的生成器</span></span><br><span class="line"><span class="string">    和 DCGAN 相同</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, noise_dim, channel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.main = nn.Sequential(</span><br><span class="line">            nn.ConvTranspose2d(noise_dim, <span class="number">128</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.ConvTranspose2d(<span class="number">128</span>, <span class="number">64</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.ConvTranspose2d(<span class="number">64</span>, channel_size, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.main(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Critic</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    基于卷积层的判别器</span></span><br><span class="line"><span class="string">    和 DCGAN 相同</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.main = nn.Sequential(</span><br><span class="line">            nn.Conv2d(channel_size, <span class="number">64</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.flatten = nn.Flatten()</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(<span class="number">128</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.main(<span class="built_in">input</span>)</span><br><span class="line">        x = <span class="variable language_">self</span>.flatten(x)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_gradient_penalty</span>(<span class="params">critic, real_samples, fake_samples</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算梯度惩罚</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 随机数 alpha作为插值的权重, interpolates 是在真实样本和假样本之间的插值数据</span></span><br><span class="line">    alpha = torch.rand(real_samples.size(<span class="number">0</span>), <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">    interpolates = (alpha * real_samples + (<span class="number">1</span> - alpha) * fake_samples).requires_grad_(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 得到判别器的结果</span></span><br><span class="line">    critic_interpolates = critic(interpolates)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># torch.autograd.grad用于计算导数，将梯度计算的结果存储在 gradients 变量中</span></span><br><span class="line">    gradients = torch.autograd.grad(outputs=critic_interpolates, inputs=interpolates,</span><br><span class="line">                                    grad_outputs=torch.ones_like(critic_interpolates),</span><br><span class="line">                                    create_graph=<span class="literal">True</span>, retain_graph=<span class="literal">True</span>, only_inputs=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将梯度展平 (Batch, channels, height, width) -&gt; (Batch, channels * height * width)</span></span><br><span class="line">    gradients = gradients.view(gradients.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算梯度惩罚, 公式为((梯度的L2范数 - 1) ^ 2)的均值</span></span><br><span class="line">        <span class="comment"># 理想情况下，Lipschitz常数应该为1，因此梯度的L2范数（gradients.norm(2, dim=1)）应该接近1。</span></span><br><span class="line">        <span class="comment"># 如果它大于1或小于1，都会给模型带来惩罚，以倒逼判别器的梯度符合要求。</span></span><br><span class="line">    gradient_penalty = ((gradients.norm(<span class="number">2</span>, dim=<span class="number">1</span>) - <span class="number">1</span>) ** <span class="number">2</span>).mean()</span><br><span class="line">    <span class="keyword">return</span> gradient_penalty</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型,优化器</span></span><br><span class="line">netG = Generator(noise_dim, channel_size).to(device)</span><br><span class="line">netC = Critic(channel_size).to(device)</span><br><span class="line"></span><br><span class="line">optimizerC = optim.Adam(netC.parameters(), lr=lr, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line">optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> i, (data, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        real_imgs = data.to(device)</span><br><span class="line">        batch_size = real_imgs.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练判别器</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(critic_iterations):</span><br><span class="line">            netC.zero_grad()</span><br><span class="line">            noise = torch.randn(batch_size, noise_dim, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">            fake_imgs = netG(noise)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算 Wasserstein 损失</span></span><br><span class="line">            lossC_real = -netC(real_imgs).mean() + netC(fake_imgs.detach()).mean()</span><br><span class="line">            <span class="comment"># 计算梯度惩罚</span></span><br><span class="line">            gradient_penalty = compute_gradient_penalty(netC, real_imgs, fake_imgs.detach())</span><br><span class="line">            lossC = lossC_real + lambda_gp * gradient_penalty <span class="comment"># 增加梯度惩罚</span></span><br><span class="line"></span><br><span class="line">            lossC.backward()</span><br><span class="line">            optimizerC.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练生成器</span></span><br><span class="line">        netG.zero_grad()</span><br><span class="line">        fake_imgs = netG(noise)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算 Wasserstein 损失</span></span><br><span class="line">        lossG = -netC(fake_imgs).mean()</span><br><span class="line">        lossG.backward()</span><br><span class="line">        optimizerG.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>] Batch <span class="subst">&#123;i&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(dataloader)&#125;</span> Loss_C: <span class="subst">&#123;lossC.item():<span class="number">.4</span>f&#125;</span> Loss_G: <span class="subst">&#123;lossG.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存每个 epoch 的生成结果</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        fixed_noise = torch.randn(<span class="number">16</span>, noise_dim, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">        fake = netG(fixed_noise)</span><br><span class="line">    vutils.save_image(fake, <span class="string">f&quot;output_wgan_gp/fake_samples_epoch_<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>.png&quot;</span>, normalize=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>10 epoches:<br><img src="/img/wgan_mnist.png" alt="alt text"></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/project/">project</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GAN/" rel="tag">GAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/WGAN/" rel="tag">WGAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/WGAN-GP/" rel="tag">WGAN-GP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/code/" rel="tag">code</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Wasserstein距离" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/13/Wasserstein%E8%B7%9D%E7%A6%BB/" class="article-date">
      <time datetime="2025-03-13T10:31:23.000Z" itemprop="datePublished">2025-03-13</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/13/Wasserstein%E8%B7%9D%E7%A6%BB/">Wasserstein距离</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h3 id="Wasserstein距离（Wasserstein-Distance）"><a href="#Wasserstein距离（Wasserstein-Distance）" class="headerlink" title="Wasserstein距离（Wasserstein Distance）"></a><strong>Wasserstein距离（Wasserstein Distance）</strong></h3><p><strong>Wasserstein距离</strong>，又被称为<strong>地球移动者距离（Earth Mover’s Distance, EMD）</strong>，是一种衡量两个概率分布之间“距离”的方法。直观来说，它可以看作是从一个分布到另一个分布移动“质量”的最小工作量。</p>
<h4 id="直观解释："><a href="#直观解释：" class="headerlink" title="直观解释："></a><strong>直观解释</strong>：</h4><p>假设你有两个概率分布，分别表示两个不同的“山脊”（或者“堆积”）。Wasserstein距离试图计算，把一堆质量从一个山脊移动到另一个山脊所需要的最小“成本”。其中，每个“成本”都是根据移动质量的距离来计算的。可以把它看作是把一堆土从一个地方搬到另一个地方所需要的最少工作量。</p>
<h4 id="公式："><a href="#公式：" class="headerlink" title="公式："></a><strong>公式</strong>：</h4><p>Wasserstein距离通常定义为<strong>最小传输成本</strong>。对于两个分布 (P) 和 (Q)，它的计算可以用下面的公式表示：</p>
<p>[<br>W(P, Q) &#x3D; \inf_{\gamma \in \Gamma(P, Q)} \mathbb{E}_{(x,y) \sim \gamma} [| x - y |]<br>]</p>
<p>其中：</p>
<ul>
<li>( \Gamma(P, Q) ) 是所有满足边际条件 (P) 和 (Q) 的联合分布（即对每个 (x) 和 (y) 在分布 (P) 和 (Q) 下的配对情况）。</li>
<li>( | x - y | ) 是计算在空间中从 (x) 到 (y) 的距离。</li>
</ul>
<p><strong>Wasserstein距离</strong>实际上衡量的是从一个分布到另一个分布的最小“成本”或“运输工作量”。</p>
<h4 id="优势："><a href="#优势：" class="headerlink" title="优势："></a><strong>优势</strong>：</h4><ul>
<li><strong>良好的梯度性质</strong>：与Kullback-Leibler散度等传统方法不同，Wasserstein距离在分布之间有更平滑的过渡。它不容易出现梯度消失问题，因此它在训练过程中更加稳定。</li>
<li><strong>可用性</strong>：Wasserstein距离不仅适用于连续分布，还可以适用于离散分布，这使得它在处理一些复杂的数据分布时特别有用。</li>
</ul>
<h4 id="在GAN中的应用："><a href="#在GAN中的应用：" class="headerlink" title="在GAN中的应用："></a><strong>在GAN中的应用</strong>：</h4><p>在生成对抗网络中，**Wasserstein GAN（WGAN）**引入了Wasserstein距离作为优化目标，解决了传统GAN中的训练不稳定问题。传统GAN在训练时存在判别器输出饱和和梯度消失的问题，而WGAN通过Wasserstein距离的引入，使得生成器和判别器的学习更加稳定，并且生成效果更好。</p>
<hr>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/">优化技术</a>
    </div>


      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-KL散度" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/13/KL%E6%95%A3%E5%BA%A6/" class="article-date">
      <time datetime="2025-03-13T10:31:11.000Z" itemprop="datePublished">2025-03-13</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/13/KL%E6%95%A3%E5%BA%A6/">KL散度</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h3 id="KL散度（Kullback-Leibler-Divergence）"><a href="#KL散度（Kullback-Leibler-Divergence）" class="headerlink" title="KL散度（Kullback-Leibler Divergence）"></a><strong>KL散度（Kullback-Leibler Divergence）</strong></h3><p><strong>KL散度</strong>，又叫<strong>Kullback-Leibler信息量散度</strong>，是一种衡量两个概率分布之间差异的度量。它定义了从分布 (Q) 到分布 (P) 的信息损失，或者说，如果我们用分布 (Q) 来近似真实分布 (P)，则产生的误差。</p>
<h4 id="直观解释："><a href="#直观解释：" class="headerlink" title="直观解释："></a><strong>直观解释</strong>：</h4><p>KL散度衡量的是使用概率分布 (Q) 来代替概率分布 (P) 时，所丧失的信息量。它并不是对称的，即 (D_{KL}(P \parallel Q)) 不等于 (D_{KL}(Q \parallel P))。可以把KL散度理解为，给定一个真实的分布 (P)，如果我们用分布 (Q) 来近似它，那么KL散度就是衡量这种近似误差的大小。</p>
<h4 id="公式："><a href="#公式：" class="headerlink" title="公式："></a><strong>公式</strong>：</h4><p>对于离散的概率分布 (P) 和 (Q)，KL散度的公式为：</p>
<p>[<br>D_{KL}(P \parallel Q) &#x3D; \sum_{i} P(i) \log \frac{P(i)}{Q(i)}<br>]</p>
<p>对于连续的概率分布 (P) 和 (Q)，KL散度的公式为：</p>
<p>[<br>D_{KL}(P \parallel Q) &#x3D; \int p(x) \log \frac{p(x)}{q(x)} , dx<br>]</p>
<p>其中：</p>
<ul>
<li>(P(i)) 和 (Q(i)) 分别是离散分布 (P) 和 (Q) 在样本点 (i) 上的概率。</li>
<li>(p(x)) 和 (q(x)) 是连续分布 (P) 和 (Q) 在样本点 (x) 上的概率密度。</li>
</ul>
<h4 id="优势："><a href="#优势：" class="headerlink" title="优势："></a><strong>优势</strong>：</h4><ul>
<li><strong>易于理解和计算</strong>：KL散度的计算相对简单，并且能够直接衡量分布之间的信息丧失。</li>
<li><strong>适用于实际应用</strong>：在许多实际机器学习任务中（例如变分推理和自编码器），KL散度经常作为目标函数来进行优化。</li>
</ul>
<h4 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a><strong>缺点</strong>：</h4><ul>
<li><strong>不对称</strong>：KL散度不对称，即 (D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P))，这可能会导致某些情况下的评估偏差。</li>
<li><strong>无法处理零概率问题</strong>：如果分布 (Q) 中某些事件的概率为零，而分布 (P) 中该事件的概率不为零，KL散度会出现无穷大，这在某些情况下会带来计算上的困难。</li>
</ul>
<h4 id="在GAN中的应用："><a href="#在GAN中的应用：" class="headerlink" title="在GAN中的应用："></a><strong>在GAN中的应用</strong>：</h4><p>KL散度在原始GAN的训练过程中并没有直接作为损失函数，但它与生成模型中的**变分自编码器（VAE）**相关。VAE在训练时使用KL散度来衡量潜在变量的分布与标准正态分布之间的差异，因此KL散度在生成模型中经常被用来优化潜在空间的结构。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/">优化技术</a>
    </div>


      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-ACGAN-实现手写数字生成" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/12/ACGAN-%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E7%94%9F%E6%88%90/" class="article-date">
      <time datetime="2025-03-12T14:26:19.000Z" itemprop="datePublished">2025-03-12</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/12/ACGAN-%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E7%94%9F%E6%88%90/">ACGAN 实现手写数字生成</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><ol>
<li>本次挑战使用的MNIST手写数字数据集，包含60,000张28x28的灰度图像，分为10个类别（数字0-9）。此数据集将用于训练你的生成对抗网络。</li>
<li>你的任务是使用DCGAN模型，对该数据集进行图像生成。具体要求如下：<ol>
<li>数据集下载：请下载MNIST数据集，并确保数据集中包含训练集和测试集。</li>
<li>数据预处理：将图像数据进行必要的预处理，使其适合于DCGAN模型的训练。</li>
<li>模型训练：搭建DCGAN模型，并利用训练数据集进行训练，调整模型参数，尝试生成高质量的数字图像。</li>
<li>模型评估：在训练过程中，监控生成图像的质量，并可视化不同训练阶段生成的图像。</li>
</ol>
</li>
</ol>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torchvision.utils <span class="keyword">as</span> vutils</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line">noise_dim = <span class="number">100</span></span><br><span class="line">epochs = <span class="number">20</span></span><br><span class="line">channel_size = <span class="number">1</span></span><br><span class="line">num_classes = <span class="number">10</span>  <span class="comment"># 数据集类别数</span></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">os.makedirs(<span class="string">&quot;output_acgan&quot;</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理和加载</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Grayscale(num_output_channels=<span class="number">1</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">dataset = datasets.ImageFolder(root=<span class="string">&#x27;data/mnist_jpg&#x27;</span>, transform=transform)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, noise_dim, num_classes, channel_size</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        基于卷积层的生成器</span></span><br><span class="line"><span class="string">        卷积层的部分和DCGAN完全相同，只是增加了类别嵌入，以学习到类别信息</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将离散类别标签映射到连续向量空间</span></span><br><span class="line">        <span class="variable language_">self</span>.label_emb = nn.Embedding(num_classes, noise_dim)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.main = nn.Sequential(</span><br><span class="line">            nn.ConvTranspose2d(noise_dim * <span class="number">2</span>, <span class="number">128</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.ConvTranspose2d(<span class="number">128</span>, <span class="number">64</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.ConvTranspose2d(<span class="number">64</span>, channel_size, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, noise, labels</span>):</span><br><span class="line">        <span class="comment"># 将类别嵌入</span></span><br><span class="line">        label_embedding = <span class="variable language_">self</span>.label_emb(labels).unsqueeze(<span class="number">2</span>).unsqueeze(<span class="number">3</span>) <span class="comment"># 变形以匹配噪声维度</span></span><br><span class="line">        <span class="built_in">input</span> = torch.cat([noise, label_embedding], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.main(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Discriminator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channel_size, num_classes</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        基于卷积层的判别器</span></span><br><span class="line"><span class="string">        卷积层的部分和DCGAN完全相同</span></span><br><span class="line"><span class="string">        新增加类别分类头</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.main = nn.Sequential(</span><br><span class="line">            nn.Conv2d(channel_size, <span class="number">64</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.flatten = nn.Flatten()</span><br><span class="line">        <span class="variable language_">self</span>.fc_real_fake = nn.Linear(<span class="number">128</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">1</span>)  <span class="comment"># 真假分类</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_class = nn.Linear(<span class="number">128</span> * <span class="number">7</span> * <span class="number">7</span>, num_classes)  <span class="comment"># 分类</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.main(<span class="built_in">input</span>)</span><br><span class="line">        x = <span class="variable language_">self</span>.flatten(x)</span><br><span class="line"></span><br><span class="line">        real_fake = torch.sigmoid(<span class="variable language_">self</span>.fc_real_fake(x)) <span class="comment"># 需要添加 softmax，因为 BCELoss 需要输入概率值</span></span><br><span class="line">        class_output = <span class="variable language_">self</span>.fc_class(x)  <span class="comment"># 分类输出（不加 softmax，交叉熵损失自带sigmoid）</span></span><br><span class="line">        <span class="keyword">return</span> real_fake, class_output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型,优化器,损失函数</span></span><br><span class="line">netG = Generator(noise_dim, num_classes, channel_size).to(device)</span><br><span class="line">netD = Discriminator(channel_size, num_classes).to(device)</span><br><span class="line"></span><br><span class="line">criterion_gan = nn.BCELoss()</span><br><span class="line">criterion_class = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line">optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> i, (data, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        batch_size = data.size(<span class="number">0</span>)</span><br><span class="line">        real_imgs, labels = data.to(device), labels.to(device)</span><br><span class="line"></span><br><span class="line">        real_labels = torch.ones(batch_size, <span class="number">1</span>, device=device)</span><br><span class="line">        fake_labels = torch.zeros(batch_size, <span class="number">1</span>, device=device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练判别器</span></span><br><span class="line">        netD.zero_grad()</span><br><span class="line">            <span class="comment"># 真实数据损失</span></span><br><span class="line">        real_out, real_class = netD(real_imgs)</span><br><span class="line">        lossD_real = criterion_gan(real_out, real_labels)</span><br><span class="line">        lossD_real_class = criterion_class(real_class, labels)</span><br><span class="line">            <span class="comment"># 计算虚假数据损失</span></span><br><span class="line">                <span class="comment"># 生成假样本</span></span><br><span class="line">        noise = torch.randn(batch_size, noise_dim, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">        fake_labels_input = torch.randint(<span class="number">0</span>, num_classes, (batch_size,), device=device)</span><br><span class="line">        fake_imgs = netG(noise, fake_labels_input)</span><br><span class="line">                <span class="comment"># 假样本损失</span></span><br><span class="line">        fake_out, fake_class = netD(fake_imgs.detach())</span><br><span class="line">        lossD_fake = criterion_gan(fake_out, fake_labels)</span><br><span class="line">        lossD_fake_class = criterion_class(fake_class, fake_labels_input)</span><br><span class="line">            <span class="comment"># 总判别器损失</span></span><br><span class="line">        lossD = lossD_real + lossD_fake + lossD_real_class + lossD_fake_class</span><br><span class="line">        lossD.backward()</span><br><span class="line">        optimizerD.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练生成器</span></span><br><span class="line">        netG.zero_grad()</span><br><span class="line">        fake_out, fake_class = netD(fake_imgs)</span><br><span class="line">            <span class="comment"># 生成器希望判别器把假样本判为真实</span></span><br><span class="line">        lossG_fake = criterion_gan(fake_out, real_labels)</span><br><span class="line">        lossG_classification = criterion_class(fake_class, fake_labels_input)</span><br><span class="line">            <span class="comment"># 总生成器损失</span></span><br><span class="line">        lossG = lossG_fake + lossG_classification</span><br><span class="line">        lossG.backward()</span><br><span class="line">        optimizerG.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>] Batch <span class="subst">&#123;i&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(dataloader)&#125;</span> &quot;</span></span><br><span class="line">                  <span class="string">f&quot;Loss_D: <span class="subst">&#123;lossD.item():<span class="number">.4</span>f&#125;</span> Loss_G: <span class="subst">&#123;lossG.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存生成结果</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        fixed_noise = torch.randn(<span class="number">16</span>, noise_dim, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">        fixed_labels = torch.randint(<span class="number">0</span>, num_classes, (<span class="number">16</span>,), device=device)</span><br><span class="line">        fake = netG(fixed_noise, fixed_labels)</span><br><span class="line">    vutils.save_image(fake, <span class="string">f&quot;output_acgan/fake_samples_epoch_<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>.png&quot;</span>, normalize=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>7 epoches:<br><img src="/img/acgan_mnist.png" alt="alt text"></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/project/">project</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ACGAN/" rel="tag">ACGAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GAN/" rel="tag">GAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/code/" rel="tag">code</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-批归一化" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/10/%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96/" class="article-date">
      <time datetime="2025-03-10T10:37:14.000Z" itemprop="datePublished">2025-03-10</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/10/%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96/">批归一化</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h3 id="批量正则化（Batch-Normalization）"><a href="#批量正则化（Batch-Normalization）" class="headerlink" title="批量正则化（Batch Normalization）"></a><strong>批量正则化（Batch Normalization）</strong></h3><p><strong>批量正则化（Batch Normalization，简称BN）</strong> 是一种对神经网络的输入数据进行规范化处理的方法，旨在加速网络训练、提高稳定性，并减少模型对初始化的依赖。在GAN中，批量正则化主要应用于生成器和判别器的训练过程中，以提高其收敛速度和训练的稳定性。</p>
<h4 id="原理："><a href="#原理：" class="headerlink" title="原理："></a><strong>原理</strong>：</h4><p>在深度神经网络的训练中，随着层数的增加，网络的输入可能会经历很多次的线性变换和非线性激活。这样会导致每一层的输入分布发生变化，这个现象叫做<strong>内部协变量偏移（Internal Covariate Shift）</strong>。为了避免这种偏移，<strong>批量正则化</strong>通过对每一层的输入进行标准化，使其均值为0，方差为1，从而加速训练并减少训练的不稳定性。</p>
<p>批量正则化的主要步骤是：</p>
<ol>
<li><strong>计算当前批次数据的均值和方差</strong>。</li>
<li><strong>用均值和方差对数据进行标准化</strong>，使得每个输入特征的均值为0，方差为1。</li>
<li><strong>使用可学习的缩放因子和偏移量</strong>，将标准化后的数据恢复到一定的分布范围内。</li>
</ol>
<p>公式上，批量正则化的过程是：</p>
<p>[<br>\hat{x} &#x3D; \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}<br>]<br>其中：</p>
<ul>
<li>( x ) 是当前批次的数据。</li>
<li>( \mu_B ) 和 ( \sigma_B^2 ) 分别是当前批次数据的均值和方差。</li>
<li>( \epsilon ) 是一个很小的常数，用于避免除以零的情况。</li>
</ul>
<p>然后，标准化后的数据会经过两个可学习的参数：<strong>缩放因子</strong>（( \gamma )）和<strong>偏移量</strong>（( \beta )），以便网络能够根据需要恢复原来的数据分布。</p>
<h4 id="在GAN中的作用："><a href="#在GAN中的作用：" class="headerlink" title="在GAN中的作用："></a><strong>在GAN中的作用</strong>：</h4><ul>
<li><strong>提高训练稳定性</strong>：由于批量正则化对输入数据进行了标准化，网络训练过程中的梯度更新更加稳定，避免了梯度消失或爆炸的问题。</li>
<li><strong>加速收敛</strong>：通过标准化输入数据，使得训练过程更快速地收敛，减少了学习率调节的难度。</li>
<li><strong>缓解模式崩溃</strong>：批量正则化有助于防止生成器只生成某一类样本的模式崩溃问题，因为它提供了更为平滑的训练过程，有助于生成器学习更多样的样本。</li>
</ul>
<p>然而，批量正则化在生成器中的使用会带来一些额外的挑战，尤其是在生成高分辨率图像时，它的效果并不总是理想。因此，研究者们提出了其他的优化方法，像**谱归一化（Spectral Normalization）**等。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/">优化技术</a>
    </div>


      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-DCGAN-实现手写数字生成" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/09/DCGAN-%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E7%94%9F%E6%88%90/" class="article-date">
      <time datetime="2025-03-09T14:26:19.000Z" itemprop="datePublished">2025-03-09</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/09/DCGAN-%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E7%94%9F%E6%88%90/">DCGAN 实现手写数字生成</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><ol>
<li>本次挑战使用的MNIST手写数字数据集，包含60,000张28x28的灰度图像，分为10个类别（数字0-9）。此数据集将用于训练你的生成对抗网络。</li>
<li>你的任务是使用DCGAN模型，对该数据集进行图像生成。具体要求如下：<ol>
<li>数据集下载：请下载MNIST数据集，并确保数据集中包含训练集和测试集。</li>
<li>数据预处理：将图像数据进行必要的预处理，使其适合于DCGAN模型的训练。</li>
<li>模型训练：搭建DCGAN模型，并利用训练数据集进行训练，调整模型参数，尝试生成高质量的数字图像。</li>
<li>模型评估：在训练过程中，监控生成图像的质量，并可视化不同训练阶段生成的图像。</li>
</ol>
</li>
</ol>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torchvision.utils <span class="keyword">as</span> vutils</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line">noise_dim = <span class="number">100</span></span><br><span class="line">epochs = <span class="number">20</span></span><br><span class="line">channel_size = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">os.makedirs(<span class="string">&quot;output_dcgan&quot;</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Grayscale(num_output_channels=<span class="number">1</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 ImageFolder 读取数据</span></span><br><span class="line">dataset = datasets.ImageFolder(root=<span class="string">&#x27;data/mnist_jpg&#x27;</span>, transform=transform)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, noise_dim, channel_size</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        基于卷积层的生成器</span></span><br><span class="line"><span class="string">        实现生成器的若干卷积层的叠加</span></span><br><span class="line"><span class="string">        :param noise_dim: 输入的噪音维度</span></span><br><span class="line"><span class="string">        :param channel_size: 目标图像的通道数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.main = nn.Sequential(</span><br><span class="line">            nn.ConvTranspose2d(noise_dim, <span class="number">64</span> * <span class="number">2</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>), <span class="comment"># (batch_size, noise_dim, 1, 1) -&gt; (batch_size, 128, 7, 7)</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span> * <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.ConvTranspose2d(<span class="number">64</span> * <span class="number">2</span>, <span class="number">64</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>), <span class="comment"># (batch_size, 128, 7, 7) -&gt; (batch_size, 64, 14, 14)</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.ConvTranspose2d(<span class="number">64</span>, channel_size, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>), <span class="comment"># (batch_size, 64, 14, 14) -&gt; (batch_size, channel_size, 28, 28)</span></span><br><span class="line">            nn.Tanh()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;完成前向传播&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.main(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Discriminator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channel_size</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        基于卷积层的判别器</span></span><br><span class="line"><span class="string">        实现判别器的若干卷积层的叠加</span></span><br><span class="line"><span class="string">        :param channel_size: 欲判别的图像通道数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.main = nn.Sequential(</span><br><span class="line">            nn.Conv2d(channel_size, <span class="number">64</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>), <span class="comment"># (batch_size, channel_size, 28, 28)-&gt;(batch_size, 64, 14, 14)</span></span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">64</span> * <span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>), <span class="comment"># (batch_size, 64, 14, 14)-&gt;(batch_size, 128, 7, 7)</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span> * <span class="number">2</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.flatten = nn.Flatten()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">128</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">1</span>),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        完成前向传播</span></span><br><span class="line"><span class="string">        :param input: 欲判别的图像数据</span></span><br><span class="line"><span class="string">        :return: 返回分类结果</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        x = <span class="variable language_">self</span>.main(<span class="built_in">input</span>)</span><br><span class="line">        x = <span class="variable language_">self</span>.flatten(x)</span><br><span class="line">        output = <span class="variable language_">self</span>.fc(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型,优化器,损失函数</span></span><br><span class="line">netG = Generator(noise_dim, channel_size).to(device)</span><br><span class="line">netD = Discriminator(channel_size).to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.BCELoss()</span><br><span class="line"></span><br><span class="line">optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line">optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> i, (data, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        <span class="comment"># 训练判别器</span></span><br><span class="line">            <span class="comment"># 使 D_model 对真实数据集里的真实图像进行分类判断，将 label 视作 1</span></span><br><span class="line">        netD.zero_grad()</span><br><span class="line">        real_imgs = data.to(device)</span><br><span class="line">        batch_size = real_imgs.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        label_real = torch.full((batch_size, <span class="number">1</span>), <span class="number">1.0</span>, device=device)</span><br><span class="line">        output_real = netD(real_imgs)</span><br><span class="line">        lossD_real = criterion(output_real, label_real)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 使 D_model 对 G_model 生成的虚假图像进行分类判断，将 label 视作 0</span></span><br><span class="line">        noise = torch.randn(batch_size, noise_dim, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">        fake_imgs = netG(noise)</span><br><span class="line">        label_fake = torch.full((batch_size, <span class="number">1</span>), <span class="number">0.0</span>, device=device)</span><br><span class="line">        output_fake = netD(fake_imgs.detach())</span><br><span class="line">        lossD_fake = criterion(output_fake, label_fake)</span><br><span class="line">            <span class="comment"># 通过真实图像上的损失和虚假图像上的损失相加，得到原论文中的损失表达，可以衡量模型在真假图形分类上的表现</span></span><br><span class="line">        lossD = lossD_real + lossD_fake</span><br><span class="line">        lossD.backward()</span><br><span class="line">        optimizerD.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练生成器</span></span><br><span class="line">        netG.zero_grad()</span><br><span class="line">        label_gen = torch.full((batch_size, <span class="number">1</span>), <span class="number">1.0</span>, device=device)   <span class="comment"># 生成器希望判别器将假样本判为真实,故标签设置为 1</span></span><br><span class="line">        output_gen = netD(fake_imgs)</span><br><span class="line">        lossG = criterion(output_gen, label_gen)</span><br><span class="line">        lossG.backward()</span><br><span class="line">        optimizerG.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>] Batch <span class="subst">&#123;i&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(dataloader)&#125;</span> Loss_D: <span class="subst">&#123;(lossD_real + lossD_fake).item():<span class="number">.4</span>f&#125;</span> Loss_G: <span class="subst">&#123;lossG.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存每个 epoch 的生成结果</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        fixed_noise = torch.randn(<span class="number">16</span>, noise_dim, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">        fake = netG(fixed_noise)</span><br><span class="line">    vutils.save_image(fake, <span class="string">f&quot;output_dcgan/fake_samples_epoch_<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>.png&quot;</span>, normalize=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>30 epoches:<br><img src="/img/dcgan_mnist.png" alt="alt text"></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/project/">project</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DCGAN/" rel="tag">DCGAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GAN/" rel="tag">GAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/code/" rel="tag">code</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-GAN-实现手写数字生成" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/09/GAN-%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E7%94%9F%E6%88%90/" class="article-date">
      <time datetime="2025-03-09T14:26:19.000Z" itemprop="datePublished">2025-03-09</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/09/GAN-%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E7%94%9F%E6%88%90/">GAN 实现手写数字生成</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><ol>
<li>本次挑战使用的MNIST手写数字数据集，包含60,000张28x28的灰度图像，分为10个类别（数字0-9）。此数据集将用于训练你的生成对抗网络。</li>
<li>你的任务是使用DCGAN模型，对该数据集进行图像生成。具体要求如下：<ol>
<li>数据集下载：请下载MNIST数据集，并确保数据集中包含训练集和测试集。</li>
<li>数据预处理：将图像数据进行必要的预处理，使其适合于DCGAN模型的训练。</li>
<li>模型训练：搭建DCGAN模型，并利用训练数据集进行训练，调整模型参数，尝试生成高质量的数字图像。</li>
<li>模型评估：在训练过程中，监控生成图像的质量，并可视化不同训练阶段生成的图像。</li>
</ol>
</li>
</ol>
<h2 id="代码（增强数据版）"><a href="#代码（增强数据版）" class="headerlink" title="代码（增强数据版）"></a>代码（增强数据版）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torchvision.utils <span class="keyword">as</span> vutils</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置参数</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line">noise_dim = <span class="number">100</span></span><br><span class="line">epochs = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">os.makedirs(<span class="string">&quot;output_ganpro&quot;</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Grayscale(num_output_channels=<span class="number">1</span>),  <span class="comment"># 修改处理的默认的图像通道数</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))  <span class="comment"># 将图像归一化到 [-1, 1]</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 ImageFolder 读取数据</span></span><br><span class="line">dataset = datasets.ImageFolder(root=<span class="string">&#x27;data/mnist_jpg&#x27;</span>, transform=transform)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, noise_dim</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        生成器，将输入的噪声通过 MLP</span></span><br><span class="line"><span class="string">        :param noise_dim: 输入的噪声维度</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.main = nn.Sequential(</span><br><span class="line">            <span class="comment"># MLP</span></span><br><span class="line">            nn.Linear(noise_dim, <span class="number">256</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">256</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, <span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">256</span>, <span class="number">512</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">512</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, <span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">1024</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, <span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">28</span> * <span class="number">28</span>),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param input: 输入的噪声数据</span></span><br><span class="line"><span class="string">        :return: 通过 MLP 生成的图像</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        output = <span class="variable language_">self</span>.main(<span class="built_in">input</span>)</span><br><span class="line">        output = output.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Discriminator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        判别器，将输入的图像通过 MLP 进行二分类</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.main = nn.Sequential(</span><br><span class="line">            <span class="comment"># MLP</span></span><br><span class="line">            nn.Linear(<span class="number">28</span> * <span class="number">28</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">512</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">256</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 使用 Sigmoid 映射到 [0,1]</span></span><br><span class="line">            nn.Linear(<span class="number">256</span>, <span class="number">1</span>),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = <span class="variable language_">self</span>.main(<span class="built_in">input</span>.view(-<span class="number">1</span>, <span class="number">28</span> * <span class="number">28</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型,优化器,损失函数</span></span><br><span class="line">netG = Generator(noise_dim).to(device)</span><br><span class="line">netD = Discriminator().to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.BCELoss()</span><br><span class="line"></span><br><span class="line">optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line">optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> i, (data, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        <span class="comment"># 训练辨别器</span></span><br><span class="line">            <span class="comment"># 使 D_model 对真实数据集里的真实图像进行分类判断，将 label 视作 1</span></span><br><span class="line">        netD.zero_grad()</span><br><span class="line">        real_images = data.to(device)</span><br><span class="line">        batch_size = real_images.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        label_real = torch.full((batch_size, <span class="number">1</span>), <span class="number">1.0</span>, device=device)  <span class="comment"># 真实标签为 1</span></span><br><span class="line">        output_real = netD(real_images)</span><br><span class="line">        lossD_real = criterion(output_real, label_real)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 使 D_model 对 G_model 生成的虚假图像进行分类判断，将 label 视作 0</span></span><br><span class="line">        noise = torch.randn(batch_size, noise_dim, device=device)</span><br><span class="line">        fake_images = netG(noise)</span><br><span class="line">        label_fake = torch.full((batch_size, <span class="number">1</span>), <span class="number">0.0</span>, device=device)  <span class="comment"># 假图像标签为 0</span></span><br><span class="line">        output_fake = netD(fake_images.detach())  <span class="comment"># detach 防止梯度流向生成器</span></span><br><span class="line">        lossD_fake = criterion(output_fake, label_fake)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新判别器参数</span></span><br><span class="line">        lossD = lossD_real + lossD_fake</span><br><span class="line">        lossD.backward()</span><br><span class="line">        optimizerD.step()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练生成器</span></span><br><span class="line">        netG.zero_grad()</span><br><span class="line">        label_gen = torch.full((batch_size, <span class="number">1</span>), <span class="number">1.0</span>, device=device)</span><br><span class="line">        output_gen = netD(fake_images)</span><br><span class="line">        lossG = criterion(output_gen, label_gen)</span><br><span class="line">        lossG.backward()</span><br><span class="line">        optimizerG.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出训练信息</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>] Batch <span class="subst">&#123;i&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(dataloader)&#125;</span> &quot;</span></span><br><span class="line">                  <span class="string">f&quot;Loss_D: <span class="subst">&#123;(lossD_real + lossD_fake).item():<span class="number">.4</span>f&#125;</span> Loss_G: <span class="subst">&#123;lossG.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每个 epoch 结束后保存一批生成的图片</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        fixed_noise = torch.randn(<span class="number">16</span>, noise_dim, device=device)</span><br><span class="line">        fake = netG(fixed_noise).detach()</span><br><span class="line">    vutils.save_image(fake, <span class="string">f&quot;output_ganpro/fake_samples_epoch_<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>.png&quot;</span>, normalize=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>30 epoches:<br><img src="/img/gan_mnist.png" alt="alt text"></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/project/">project</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GAN/" rel="tag">GAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/code/" rel="tag">code</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-颇有感触的引言" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/06/%E9%A2%87%E6%9C%89%E6%84%9F%E8%A7%A6%E7%9A%84%E5%BC%95%E8%A8%80/" class="article-date">
      <time datetime="2025-03-06T14:45:23.000Z" itemprop="datePublished">2025-03-06</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/06/%E9%A2%87%E6%9C%89%E6%84%9F%E8%A7%A6%E7%9A%84%E5%BC%95%E8%A8%80/">颇有感触的引言/台词</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <ul>
<li>“男人前进的道路，岂能只有欢笑声，决心要行的路，怎能没有暴风拦路，阁下，走吧！ ” ————《南山的部长们》</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E8%AE%B0/">生活随记</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%9A%8F%E8%AE%B0/" rel="tag">随记</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-BERT-实现-IMDb-情感分类" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/06/BERT-%E5%AE%9E%E7%8E%B0-IMDb-%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/" class="article-date">
      <time datetime="2025-03-06T14:25:52.000Z" itemprop="datePublished">2025-03-06</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/06/BERT-%E5%AE%9E%E7%8E%B0-IMDb-%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/">BERT 实现 IMDb 情感分类</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><ol>
<li>在本次挑战中，你将使用IMDb电影评论数据集，该数据集包含50,000条影评，其中25,000条用于训练，25,000条用于测试。每条评论都被标记为正面或负面情感。</li>
<li>你的任务是使用BERT模型，对该数据集进行情感分类。具体要求如下：<ol>
<li>数据集下载：请下载IMDb数据集，确保数据集中包含train和test两个文件夹，分别用于训练和测试。</li>
<li>数据预处理：对文本数据进行必要的预处理，包括分词、去除停用词、填充等，以便用于BERT模型的训练。</li>
<li>模型训练：利用BERT模型对训练数据集进行训练，调整模型参数，使其在测试集上取得尽可能高的分类准确度。</li>
<li>模型评估：在训练过程中，及时监控模型在测试集上的性能，并记录模型在测试集上的分类准确率，将结果可视化（如损失函数，预测准确率等）。</li>
</ol>
</li>
</ol>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> OneCycleLR  <span class="comment"># type: ignore</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertForSequenceClassification</span><br><span class="line"><span class="keyword">from</span> torchtext.datasets <span class="keyword">import</span> IMDB</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设备设置</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载 BERT 分词器</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">text_pipeline</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param text: 原始文本</span></span><br><span class="line"><span class="string">    :return: 处理后的 token id 和 attention mask</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> tokenizer(text, padding=<span class="string">&quot;max_length&quot;</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">100</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">label_pipeline</span>(<span class="params">label</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param label: 电影评论标签（pos/neg）</span></span><br><span class="line"><span class="string">    :return: 1 表示正面，0 表示负面</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">if</span> label == <span class="string">&quot;pos&quot;</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">IMDBDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;自定义 IMDB 数据集类&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_iter</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param data_iter: IMDB 数据集迭代器</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.data = []</span><br><span class="line">        <span class="keyword">for</span> label, text <span class="keyword">in</span> data_iter:</span><br><span class="line">            encoding = text_pipeline(text)</span><br><span class="line">            <span class="variable language_">self</span>.data.append((</span><br><span class="line">                encoding[<span class="string">&quot;input_ids&quot;</span>].squeeze(<span class="number">0</span>),</span><br><span class="line">                encoding[<span class="string">&quot;attention_mask&quot;</span>].squeeze(<span class="number">0</span>),</span><br><span class="line">                label_pipeline(label)</span><br><span class="line">            ))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回数据集大小&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;获取数据集中的单个样本&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.data[idx]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载 IMDB 数据集</span></span><br><span class="line">train_iter, test_iter = IMDB(split=<span class="string">&quot;train&quot;</span>), IMDB(split=<span class="string">&quot;test&quot;</span>)</span><br><span class="line">train_data, test_data = IMDBDataset(train_iter), IMDBDataset(test_iter)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    处理批量数据</span></span><br><span class="line"><span class="string">    :param batch: 输入数据</span></span><br><span class="line"><span class="string">    :return: 处理后的 input_ids, attention_masks, labels</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    input_ids, attention_masks, labels = <span class="built_in">zip</span>(*batch)</span><br><span class="line">    <span class="keyword">return</span> torch.stack(input_ids), torch.stack(attention_masks), torch.tensor(labels, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据加载器</span></span><br><span class="line">train_loader = DataLoader(train_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>, collate_fn=collate_fn)</span><br><span class="line">test_loader = DataLoader(test_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>, collate_fn=collate_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载预训练 BERT 模型</span></span><br><span class="line">model = BertForSequenceClassification.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>, num_labels=<span class="number">1</span>).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只训练 BERT 的最后 4 层</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.bert.encoder.layer[:-<span class="number">4</span>].parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器、损失函数和学习率调度器</span></span><br><span class="line">optimizer = optim.AdamW(model.parameters(), lr=<span class="number">5e-5</span>, weight_decay=<span class="number">1e-2</span>)</span><br><span class="line">criterion = nn.BCEWithLogitsLoss().to(device)</span><br><span class="line"></span><br><span class="line">EPOCHS = <span class="number">3</span></span><br><span class="line">scheduler = OneCycleLR(</span><br><span class="line">    optimizer,</span><br><span class="line">    max_lr=<span class="number">5e-5</span>,</span><br><span class="line">    total_steps=EPOCHS * <span class="built_in">len</span>(train_loader),</span><br><span class="line">    pct_start=<span class="number">0.3</span>,</span><br><span class="line">    anneal_strategy=<span class="string">&#x27;cos&#x27;</span>,</span><br><span class="line">    div_factor=<span class="number">10</span>,</span><br><span class="line">    final_div_factor=<span class="number">100</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model, loader, optimizer, criterion, scheduler</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> input_ids, attention_masks, labels <span class="keyword">in</span> loader:</span><br><span class="line">        input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = model(input_ids, attention_mask=attention_masks)</span><br><span class="line">        logits = outputs.logits.squeeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        loss = criterion(logits, labels.<span class="built_in">float</span>())</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        scheduler.step()</span><br><span class="line"></span><br><span class="line">        epoch_loss += loss.item()</span><br><span class="line">    <span class="keyword">return</span> epoch_loss / <span class="built_in">len</span>(loader)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">model, loader, criterion</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    epoch_loss, correct, total = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> input_ids, attention_masks, labels <span class="keyword">in</span> loader:</span><br><span class="line">            input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)</span><br><span class="line"></span><br><span class="line">            outputs = model(input_ids, attention_mask=attention_masks)</span><br><span class="line">            logits = outputs.logits.squeeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            loss = criterion(logits, labels.<span class="built_in">float</span>())</span><br><span class="line">            epoch_loss += loss.item()</span><br><span class="line"></span><br><span class="line">            preds = torch.sigmoid(logits) &gt; <span class="number">0.5</span></span><br><span class="line">            correct += (preds == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">            total += labels.size(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> epoch_loss / <span class="built_in">len</span>(loader), correct / total</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line">train_losses, test_losses, test_accs = [], [], []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):</span><br><span class="line">    train_loss = train(model, train_loader, optimizer, criterion, scheduler)</span><br><span class="line">    test_loss, test_acc = evaluate(model, test_loader, criterion)</span><br><span class="line">    train_losses.append(train_loss)</span><br><span class="line">    test_losses.append(test_loss)</span><br><span class="line">    test_accs.append(test_acc)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch: <span class="subst">&#123;epoch + <span class="number">1</span>:02&#125;</span>, Train Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span>, Test Loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span>, Test Acc: <span class="subst">&#123;test_acc:<span class="number">.2</span>%&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">torch.save(model.state_dict(), <span class="string">&quot;bert_imdb.pth&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制训练曲线</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(train_losses, label=<span class="string">&quot;Train Loss&quot;</span>)</span><br><span class="line">plt.plot(test_losses, label=<span class="string">&quot;Test Loss&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">&quot;Loss Curve&quot;</span>)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(test_accs, label=<span class="string">&quot;Test Accuracy&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">&quot;Accuracy Curve&quot;</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p><img src="/img/BERT_imdb.png" alt="alt text"></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/project/">project</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BERT/" rel="tag">BERT</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/code/" rel="tag">code</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-想看的电影" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/02/%E6%83%B3%E7%9C%8B%E7%9A%84%E7%94%B5%E5%BD%B1/" class="article-date">
      <time datetime="2025-03-02T14:44:47.000Z" itemprop="datePublished">2025-03-02</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/02/%E6%83%B3%E7%9C%8B%E7%9A%84%E7%94%B5%E5%BD%B1/">想看的电影</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <ul>
<li><p>中国</p>
<ul>
<li>《高山下的花环》</li>
</ul>
</li>
<li><p>韩国：</p>
<ul>
<li>《首尔之春》</li>
<li>《南山的部长们》</li>
<li>《共同警备区》</li>
<li>《辩护人》</li>
</ul>
</li>
<li></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E8%AE%B0/">生活随记</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%9A%8F%E8%AE%B0/" rel="tag">随记</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2025 John Doe
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="A fast, simple &amp; powerful blog framework">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="Another simple and elegant theme for Hexo  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="Site Visitors"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="Page Hits"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





<div class="scroll" id="scroll">
    <a href="#" title="Back to Top"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="Comments"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="Go to Bottom"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>