<!DOCTYPE html>
<html lang="en">
<head>

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no" />
<meta name="author" content="John Doe" />



<meta name="description" content="关于 Transformer上下文理解大师 人类理解一句话里的一个词的时候，绝对不会脱离了文本，一定是结合上下文，才能分得清主谓宾，定状补，一词多义，熟词生义，阅读理解，完形填空……这种全局性的理解，正是 Transformer 架构的精髓所在。 Transformer 模型由编码器和解码器两部分组成，核心在于完全基于 自注意力机制。它让模型在处理某个词汇时，关注输入序列中的所有其他词汇，从而捕捉">
<meta property="og:type" content="article">
<meta property="og:title" content="关于 Transformer 的笔记与思考">
<meta property="og:url" content="http://example.com/2025/03/20/%E5%85%B3%E4%BA%8E-Transformer-%E7%9A%84%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%80%9D%E8%80%83/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="关于 Transformer上下文理解大师 人类理解一句话里的一个词的时候，绝对不会脱离了文本，一定是结合上下文，才能分得清主谓宾，定状补，一词多义，熟词生义，阅读理解，完形填空……这种全局性的理解，正是 Transformer 架构的精髓所在。 Transformer 模型由编码器和解码器两部分组成，核心在于完全基于 自注意力机制。它让模型在处理某个词汇时，关注输入序列中的所有其他词汇，从而捕捉">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/Transformer.png">
<meta property="article:published_time" content="2025-03-20T09:59:01.000Z">
<meta property="article:modified_time" content="2025-03-24T14:40:57.586Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Transformer">
<meta property="article:tag" content="模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/Transformer.png">

<link rel="apple-touch-icon" href= "/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">



<link rel="stylesheet" href="/css/style.css">




<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>关于 Transformer 的笔记与思考 | Hexo</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: false,
        isPost: true,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: 
    }
</script>


    <script> yiliaConfig.jquery_ui = [false]; </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






<meta name="generator" content="Hexo 7.3.0"></head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/avatar.jpg" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/"></a></h1>
        </hgroup>

        

        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>Menu</li>
                        <li>Tags</li>
                        
                        <li>Friends</li>
                        
                        
                        <li>About Me</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                            <li><a href="/categories/%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/">模型学习</a></li>
                        
                            <li><a href="/categories/%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/">优化技术</a></li>
                        
                            <li><a href="/categories/project/">小试牛刀</a></li>
                        
                            <li><a href="/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E8%AE%B0/">生活随记</a></li>
                        
                            <li><a href="/archives/">归档</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ACGAN/" rel="tag">ACGAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/" rel="tag">BERT</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CycleGAN/" rel="tag">CycleGAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DCGAN/" rel="tag">DCGAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GAN/" rel="tag">GAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/WGAN/" rel="tag">WGAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/WGAN-GP/" rel="tag">WGAN-GP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/code/" rel="tag">code</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A8%A1%E5%9E%8B/" rel="tag">模型</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="tag">线性回归</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9A%8F%E8%AE%B0/" rel="tag">随记</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="http://moxfive.xyz/">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">专注于前端</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页"></a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/avatar.jpg" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页"></a></h1>
            </hgroup>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/about/">关于我</a></li>
                
                    <li><a href="/categories/%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/">模型学习</a></li>
                
                    <li><a href="/categories/%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/">优化技术</a></li>
                
                    <li><a href="/categories/project/">小试牛刀</a></li>
                
                    <li><a href="/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E8%AE%B0/">生活随记</a></li>
                
                    <li><a href="/archives/">归档</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="Tags" friends="Friends" about="About Me"/>
</nav>
      <div class="body-wrap"><article id="post-关于-Transformer-的笔记与思考" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/03/20/%E5%85%B3%E4%BA%8E-Transformer-%E7%9A%84%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%80%9D%E8%80%83/" class="article-date">
      <time datetime="2025-03-20T09:59:01.000Z" itemprop="datePublished">2025-03-20</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      关于 Transformer 的笔记与思考
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/">模型学习</a>
    </div>


        
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%A8%A1%E5%9E%8B/" rel="tag">模型</a></li></ul>
    </div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="关于-Transformer"><a href="#关于-Transformer" class="headerlink" title="关于 Transformer"></a>关于 Transformer</h1><p><em>上下文理解大师</em></p>
<p>人类理解一句话里的一个词的时候，绝对不会脱离了文本，一定是结合上下文，才能分得清主谓宾，定状补，一词多义，熟词生义，阅读理解，完形填空……这种全局性的理解，正是 Transformer 架构的精髓所在。</p>
<p>Transformer 模型由编码器和解码器两部分组成，核心在于完全基于 <strong>自注意力机制</strong>。它让模型在处理某个词汇时，关注输入序列中的所有其他词汇，从而捕捉全局依赖关系。这就像是在读小说时，你不仅关注当前章节，还时刻留意其他章节的剧情，以获得全面的理解。</p>
<p>摒弃了传统的 CNN、RNN 的结构（虽然后续研究发现 Transformer 和 CNN、RNN 从某种意义上是等价的），Transformer 可以利用可以并行计算的特性，大幅度提升了计算处理的速度。</p>
<h4 id="Transformer架构设计"><a href="#Transformer架构设计" class="headerlink" title="Transformer架构设计"></a>Transformer架构设计</h4><p><img src="/img/Transformer.png" alt="Transformer的架构"></p>
<p>Transformer 的特殊架构（如上图）主要由以下几个重要组成部分：</p>
<ol>
<li><p><strong>输入嵌入（Input Embedding）</strong><br>传统的模型无法直接处理离散数据（如单词或字符 ID），因此需要将输入的每个 token（例如单词）映射为连续的向量。这个映射过程通过 <strong>输入嵌入矩阵</strong> 实现，最终让模型能够理解每个单词的语义。通过训练，嵌入矩阵会自动调整，使得词向量之间的相似度反映其语义上的接近程度。</p>
</li>
<li><p><strong>位置编码（Positional Encoding）</strong><br>由于 Transformer 完全摒弃了 RNN 和 CNN 的序列结构，它无法像传统模型那样自然而然地获得序列中单词的位置信息。因此，Transformer 引入了位置编码，通过正弦和余弦函数为每个单词的向量加上唯一的位置信息。这种方式可以帮助模型识别词汇在序列中的位置顺序，使得模型能够理解句子的结构。</p>
</li>
<li><p><strong>自注意力机制（Self-Attention）</strong><br>自注意力机制是 Transformer 的核心，它使得模型在处理每个单词时，能够同时考虑到输入序列中其他所有单词的影响。在每个 token 的表示中，不仅包含了当前位置的单词信息，还包括了整个句子中相关词汇的信息。通过自注意力机制，模型能够学习到更丰富的上下文关系。</p>
</li>
<li><p><strong>多头注意力机制（Multi-Head Attention）</strong><br>多头注意力的引入使得 Transformer 能够从多个子空间（多个角度）并行地进行注意力计算，增强了模型在学习不同类型依赖关系方面的能力。它让模型不仅仅关注到局部的关系，而是能够从多个层次去理解输入序列。</p>
</li>
<li><p><strong>前馈神经网络（Feed-Forward Network）</strong><br>在每个编码器和解码器的层中，除了自注意力机制外，还会有一个前馈神经网络，它主要由两个线性变换和一个非线性激活函数（如 ReLU）构成。通过这种方式，模型能够捕捉到更复杂的特征表示。</p>
</li>
<li><p><strong>残差连接和层归一化（Residual Connection &amp; Layer Normalization）</strong><br>为了防止深层网络中的梯度消失或爆炸问题，Transformer 在每一层都采用了 <strong>残差连接</strong> 和 <strong>层归一化</strong>。残差连接确保信息在网络中的有效传递，而层归一化帮助模型的训练更稳定、收敛更快。</p>
</li>
</ol>
<h6 id="对于翻译任务"><a href="#对于翻译任务" class="headerlink" title="对于翻译任务"></a>对于翻译任务</h6><p>对于学习的数据，应该是一一配对的句子对(两种不同的语言)<br>对于模型而言，需要分别将句子对的两个视作 <strong>源语言</strong> 和 <strong>目标语言</strong></p>
<p>因为模型无法学习非数值的数据，所以需要对于文本数据进行词嵌入处理，把句子变成tokens，再把tokens变成可学习的向量(维度为embed_dim)。</p>
<ul>
<li><p>对于编码器</p>
<ol>
<li>分词处理：<br> 首先，使用分词技术将输入句子拆分为若干个token（分割策略有讲究，可以是基于词的划分，也可以是基于子词或字符的划分等）。分词后的结果是一个形状为 (batch_size, seq_len) 的张量，其中每个token都被映射为其对应的ID。</li>
<li>词嵌入和位置编码：<br> 接下来，通过词嵌入（Input Embedding）将token ID映射为对应的高维稠密向量，得到形状为 (batch_size, seq_len, embed_dim) 的数据。这些嵌入向量是通过查找表学习得到的。<br> 然后，使用位置编码（Positional Encoding）对输入嵌入进行增强。位置编码通过正余弦函数生成，形状为 (batch_size, seq_len, embed_dim)，用于注入序列中每个token的位置信息。位置编码会加到输入嵌入中，从而让模型能够感知每个token在序列中的相对位置。</li>
<li>数据转置：<br> 为了与Transformer中的矩阵运算兼容，输入数据会被转置成 (seq_len, batch_size, embed_dim) 的形状。这样做有助于提高计算效率，并使得后续的矩阵运算能够并行执行。</li>
<li>进入自注意力（Self-Attention）层：<br> 数据进入自注意力层后，首先通过三个全连接层分别计算得到查询（Q）、键（K）、值（V）矩阵。这些矩阵的参数会通过后续的梯度下降优化学习得到。<br> 对每个token的查询向量Q与其他token的键向量K进行点积计算，得到注意力得分。通过缩放因子（根号下 d_model）对得分进行缩放，再通过softmax操作将得分转换为概率分布。<br> 接着，使用该概率分布对值向量V进行加权求和，得到一个融合了上下文信息的新的表示。</li>
<li>残差连接与层归一化：<br> 将注意力层的输出与输入数据进行残差连接，然后进行层归一化（Layer Normalization）。残差连接有助于缓解深层网络中的梯度消失问题，而层归一化则有助于提升训练的稳定性。</li>
<li>前馈神经网络（MLP）：<br> 接着，经过一个前馈神经网络（MLP）。这个网络通常由两层全连接层组成，其中中间层的大小通常是输入的4倍。前馈神经网络的作用是进一步非线性转换每个token的表示。</li>
<li>再次残差连接与层归一化：<br> 前馈神经网络的输出会再次与输入数据进行残差连接，并进行层归一化处理，确保模型在深层网络中保持有效的信息流动和稳定的训练过程。</li>
<li>完成一次Encoder处理：<br> 这样，我们就完成了一个Transformer编码器层的处理。</li>
<li>重复堆叠多个编码器层：<br> 上述的编码器层结构会堆叠若干次，每一层都包括自注意力、前馈神经网络和层归一化等组件。通过多层堆叠，模型能够逐步捕捉更复杂的特征和语义信息。</li>
</ol>
</li>
<li><p>对于解码器</p>
<ol>
<li>输入处理<br>在解码器中，首先输入的是目标序列（通常是已经翻译过的部分，或者在训练时是实际的目标序列）。目标序列通过<strong>分词处理</strong>（tokenization）分割成若干个token，得到形状为 (batch_size, target_seq_len) 的数据，其中每个token被映射为对应的ID。</li>
<li>词嵌入和位置编码<br>和编码器类似，解码器的目标序列经过<strong>词嵌入</strong>（Input Embedding），将token ID映射为高维稠密向量，得到形状为 (batch_size, target_seq_len, embed_dim) 的数据。<br>之后，解码器会对目标序列应用<strong>位置编码</strong>（Positional Encoding）。位置编码采用正余弦函数生成，并加到输入的词嵌入中，使得每个token的表示不仅包含词汇信息，还包含位置信息，形状为 (batch_size, target_seq_len, embed_dim)</li>
<li>数据转置<br>为了和后续矩阵运算兼容，解码器输入数据会被转置成 (target_seq_len, batch_size, embed_dim) 的形状。这样处理是为了使得计算过程能够高效并行。</li>
<li>进入解码器的自注意力层（Masked Self-Attention）：<br>在解码器的自注意力层中，首先通过全连接层计算得到 查询（Q）、键（K）、值（V）矩阵。这些矩阵的参数通过训练进行优化。<br>在解码器中，<strong>自注意力</strong>机制是<strong>掩蔽的</strong>（masked），即当前token只能看到之前生成的token，无法看到后续的token。这种掩蔽机制确保了解码过程的自回归性质，使得每个token只能依赖于先前生成的部分。</li>
<li>计算注意力得分<br>类似于编码器，解码器中的每个token对应的查询向量Q与所有其他token的键向量K进行点积计算。得分会经过缩放（根号下 d_model）后，通过<strong>softmax</strong>得到注意力权重，最后使用这些权重对值向量V进行加权求和，从而得到每个token的上下文表示。</li>
<li>残差连接与层归一化<br>解码器中的自注意力输出会与输入进行<strong>残差连接</strong>，然后通过<strong>层归一化</strong>（Layer Normalization）。这帮助模型在深度网络中保持稳定的训练过程。</li>
<li><strong>交叉注意力层（Cross-Attention）</strong></li>
</ol>
<ul>
<li>解码器的一个关键组件是<strong>交叉注意力层</strong>，它的作用是将解码器的输入（通过自注意力计算的表示）与编码器的输出进行交互。这个层通过查询（Q）来自解码器，键（K）和值（V）来自编码器的输出。</li>
<li>交叉注意力层通过计算解码器的查询和编码器的键的点积，得到解码器与编码器之间的依赖关系。然后，利用softmax计算权重，并根据这些权重对编码器的值向量进行加权求和，从而得到融合了编码器上下文信息的输出。</li>
</ul>
<ol start="8">
<li><strong>残差连接与层归一化（交叉注意力部分）</strong>：</li>
</ol>
<ul>
<li>类似于自注意力层，交叉注意力层的输出会与输入数据进行<strong>残差连接</strong>，然后进行<strong>层归一化</strong>处理。</li>
</ul>
<ol start="9">
<li><strong>前馈神经网络（MLP）</strong>：</li>
</ol>
<ul>
<li>通过一个<strong>前馈神经网络</strong>（MLP），通常包含两层全连接层，其中中间层的大小通常是输入的四倍。前馈网络的作用是进一步对每个token的表示进行非线性变换。</li>
</ul>
<ol start="10">
<li><strong>残差连接与层归一化（MLP部分）</strong>：</li>
</ol>
<ul>
<li>前馈神经网络的输出会再次与输入进行<strong>残差连接</strong>，并进行<strong>层归一化</strong>处理。这样可以有效地缓解梯度消失问题，并确保训练过程中信息的流动。</li>
</ul>
<ol start="11">
<li><strong>完成一个解码器处理</strong>：</li>
</ol>
<ul>
<li>这样，一次解码器层的处理就完成了。通过多个堆叠的解码器层，模型能够逐层深入地理解目标序列的上下文信息，并生成目标序列的预测。</li>
</ul>
<ol start="12">
<li><strong>堆叠多个解码器层</strong>：</li>
</ol>
<ul>
<li>Transformer解码器通常由多个相同的层堆叠组成。每一层都包括自注意力、交叉注意力、前馈神经网络和层归一化等组件。通过堆叠多个解码器层，模型可以捕捉到更加复杂和丰富的特征，从而生成更精确的预测。</li>
</ul>
<ol start="13">
<li><strong>生成最终输出</strong>：</li>
</ol>
<ul>
<li>在解码器的最后一层，经过所有处理后的表示会通过一个线性层和Softmax层，生成对目标词汇表中每个token的预测概率分布。最终的输出是一个形状为 <code>(batch_size, target_seq_len, vocab_size)</code> 的张量，其中 <code>vocab_size</code> 是词汇表的大小。</li>
</ul>
</li>
</ul>
<h6 id="对于文本情感分类任务"><a href="#对于文本情感分类任务" class="headerlink" title="对于文本情感分类任务"></a>对于文本情感分类任务</h6><p>事实上这个任务完全不需要解码器的存在，因为是分类任务而不是生成任务</p>
<p>所以只需要编码器的堆叠，最终通过全连接层进行二分类即可</p>
<h4 id="Transformer的优势"><a href="#Transformer的优势" class="headerlink" title="Transformer的优势"></a>Transformer的优势</h4><ol>
<li><strong>并行计算</strong>：与传统的 RNN 和 CNN 不同，Transformer 可以一次性处理整个序列，大大提高了计算效率。</li>
<li><strong>长距离依赖建模</strong>：通过自注意力机制，Transformer 能够捕捉到序列中任意两位置之间的依赖关系，从而解决了 RNN 无法有效建模长距离依赖的问题。</li>
<li><strong>高效训练</strong>：由于 Transformer 结构的简洁性和并行性，它能够大幅提高训练的速度，尤其是在处理大规模数据时。</li>
</ol>
<p>Transformer 模型的核心优势在于能够通过自注意力机制捕捉长距离的依赖关系，并且通过并行计算极大地提升了处理速度。这使得它成为了自然语言处理（NLP）领域的革命性突破，并成为许多现代预训练语言模型（如 BERT、GPT）架构的基础。</p>
<h1 id="关于-Transformer-的变体"><a href="#关于-Transformer-的变体" class="headerlink" title="关于 Transformer 的变体"></a>关于 Transformer 的变体</h1><h3 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h3><p><em>你做完型填空的时候也不会只看一边，对吧</em></p>
<p>假如现在的任务目标不再是翻译，而是纯粹的语言理解（语义情感分析，问答），那么完全不需要 <strong>Decoder</strong> ，只需要 <strong>Encoder</strong> 就已经能够充分完成任务了。</p>
<p><strong>BERT</strong> 基于 <strong>Transformer</strong> 的编码器部分，完全摒弃了解码器，主要用于理解输入序列的上下文信息。BERT 的输入是一个句子或一对句子，它通过以下两种方式来获取丰富的上下文信息：</p>
<ol>
<li><strong>Masked Language Model（MLM）：</strong><br> 为了进行预训练，<strong>BERT</strong> 会随机遮蔽输入中的一部分单词，通过上下文信息来预测这些被遮蔽的单词。这种训练方式确保模型能有效学习到每个词在上下文中的角色。主要目标是学会词元之间的关系。</li>
<li><strong>Next Sentence Prediction（NSP）：</strong><br> <strong>BERT</strong> 还通过预测句子间的关系来进一步增强理解能力。它随机选取两个句子，判断第二个句子是否是第一个句子的下一句，从而训练模型捕捉句子间的语义关系。主要目标是学会句间的关系。</li>
</ol>
<p>BERT的独特之处还在于其 <strong>双向编码能力</strong>。</p>
<p>既然是纯碎的语言理解，那么就 <strong>没必要单向阅读</strong> 了吧，所以这里引入了 <strong>双向理解</strong> 的思路，这样显然而且试验证明的确能提高模型性能。</p>
<p>传统的语言模型（如 GPT）是单向的，只能从左到右或从右到左理解文本，而 BERT 通过双向的方式同时捕捉前后文的关系，从而更好地理解词汇的含义。</p>
<h2 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h2><p><em>你说话的时候总得想着你上一句是什么吧</em></p>
<p>理解一句话的意思不仅仅是拼凑单词的意义，而是要在理解词汇的同时，还能生成合乎逻辑且富有创意的内容。这正是 <strong>GPT（Generative Pretrained Transformer）</strong> 模型的核心能力。它不仅能理解输入的文本，还能在此基础上进行创造性地生成输出。</p>
<p>GPT 是基于 Transformer 架构的一个衍生模型，最重要的特点是完全依赖 <strong>自回归模型</strong>，即通过前文的单词逐步生成后续单词，这使得它在生成连贯且自然的文本时具有无与伦比的优势。</p>
<p>和 <strong>BERT</strong> 相对，<strong>GPT</strong> 反而是只聚焦于 <strong>解码器</strong>，它通过连续生成预测下一个词汇来实现文本生成。在每一个词生成之前，都要把上文已经有的文本进行处理，经过若干解码器的堆叠处理，然后得到最可能的下一个词，如此循环往复，最终得到完整的生成文本。</p>
<p>GPT 的关键特点是自回归生成过程。它从一个初始的输入开始，逐步预测下一个词汇，然后把这个词汇作为新的输入加入到序列中继续预测下一个词汇，直到生成完整的句子或段落。</p>
<p>GPT 的成功在于通过大量数据的预训练，使得模型能够在生成文本时，不仅理解词汇，还能够创作出合理且自然的语言。这让它成为当前自然语言处理领域的重要突破之一。</p>
<h2 id="Reformer"><a href="#Reformer" class="headerlink" title="Reformer"></a>Reformer</h2><p><em>深度压缩与效率的化身</em></p>
<p>当句子一长，token增多，那么如果对于每一个 token 都需要对其他所有的 token 计算注意力得分，这显然是一个相当低效的过程。</p>
<p>其实想一下，对于每一个 token 而言，对于它比较重要的 token 也没多少，其实不需要对于其他 <strong>每一个</strong> token <strong>都</strong> 计算注意力（计算出来的注意力得分权重为0.0000000000001要你有啥用）。如果能将相似注意力的 token 放在一起，只在他们之间计算注意力得分，然后加权求和，就很好的缩小了问题的规模，这就是 <strong>Reformer</strong> 了</p>
<p><strong>Reformer架构设计</strong><br>Reformer 的设计理念是将 <strong>注意力机制</strong> 和 <strong>压缩存储</strong> 结合起来，从而使得计算效率和内存使用得到了极大的提升。它的关键创新点包括：</p>
<ol>
<li>局部敏感哈希<br>Reformer用 <strong>局部敏感哈希（LSH）</strong> 来替代传统的 <strong>全局</strong> 自注意力计算。这种方式将注意力计算限制在邻近的词汇之间，避免了传统自注意力计算中计算量过大的问题，从而有效减少了计算复杂度。</li>
<li>可逆残差网络<br>为了降低内存占用，Reformer 引入了 <strong>可逆残差网络</strong> 的概念。这意味着在计算时，<strong>不需要保留中间层的所有输出</strong>，而是通过反向传递过程来恢复它们，这大大减少了内存的使用。</li>
<li>分块计算<br>Reformer 对输入数据进行分块处理，每次只处理小块的局部信息，而不是一次性处理整个序列。这样可以进一步降低计算开销，尤其是在处理超大规模数据时。</li>
</ol>
<p>有了如上的改进，可以在保证原 Transformer 的性能的基础上：</p>
<ul>
<li>更低的内存消耗：通过局部敏感哈希和可逆残差网络，Reformer 在处理大规模数据时显著减少了内存使用，提升了训练效率。</li>
<li>高效处理长序列：Reformer 在处理长序列时，通过分块计算避免了全局计算的瓶颈，使得它能够在有限资源下处理更长的文本。</li>
</ul>

      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>Title:</span><a href="/2025/03/20/%E5%85%B3%E4%BA%8E-Transformer-%E7%9A%84%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%80%9D%E8%80%83/">关于 Transformer 的笔记与思考</a></p>
        <p><span>Author:</span><a href="/" title="Back to Homepage"></a></p>
        <p><span>Created:</span>2025-03-20, 17:59:01</p>
        <p><span>Updated:</span>2025-03-24, 22:40:57</p>
        <p>
            <span>Full URL:</span><a class="post-url" href="/2025/03/20/%E5%85%B3%E4%BA%8E-Transformer-%E7%9A%84%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%80%9D%E8%80%83/" title="关于 Transformer 的笔记与思考">http://example.com/2025/03/20/%E5%85%B3%E4%BA%8E-Transformer-%E7%9A%84%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%80%9D%E8%80%83/</a>
            <span class="copy-path" data-clipboard-text="From http://example.com/2025/03/20/%E5%85%B3%E4%BA%8E-Transformer-%E7%9A%84%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%80%9D%E8%80%83/　　By " title="Copy Article&#39;s Link &amp; Author"><i class="fa fa-clipboard"></i></span>
            <script> var clipboard = new Clipboard('.copy-path'); </script>
        </p>
        <p>
            <span>License:</span><i class="fa fa-creative-commons"></i> <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target = "_blank">"CC BY-NC-SA 4.0"</a> Keep Link &amp; Author if Distribute.
        </p>
    </div>



    <nav id="article-nav">
        
            <div id="article-nav-newer" class="article-nav-title">
                <a href="/2025/03/20/FID/">
                    FID
                </a>
            </div>
        
        
            <div id="article-nav-older" class="article-nav-title">
                <a href="/2025/03/18/CycleGAN-%E5%AE%9E%E7%8E%B0%E8%8E%AB%E5%A5%88%E9%A3%8E%E6%A0%BC%E7%94%BB%E4%BD%9C%E8%BD%AC%E5%8C%96%E4%B8%BA%E7%9C%9F%E5%AE%9E%E7%85%A7%E7%89%87/">
                    CycleGAN 实现莫奈风格画作转化为真实照片
                </a>
            </div>
        
    </nav>

  
</article>

    <div id="toc" class="toc-article">
        <strong class="toc-title">Contents</strong>
        
            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E-Transformer"><span class="toc-number">1.</span> <span class="toc-text">关于 Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Transformer%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="toc-number">1.0.0.1.</span> <span class="toc-text">Transformer架构设计</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%AF%B9%E4%BA%8E%E7%BF%BB%E8%AF%91%E4%BB%BB%E5%8A%A1"><span class="toc-number">1.0.0.1.0.1.</span> <span class="toc-text">对于翻译任务</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%AF%B9%E4%BA%8E%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1"><span class="toc-number">1.0.0.1.0.2.</span> <span class="toc-text">对于文本情感分类任务</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Transformer%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="toc-number">1.0.0.2.</span> <span class="toc-text">Transformer的优势</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E-Transformer-%E7%9A%84%E5%8F%98%E4%BD%93"><span class="toc-number">2.</span> <span class="toc-text">关于 Transformer 的变体</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#BERT"><span class="toc-number">2.0.1.</span> <span class="toc-text">BERT</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GPT"><span class="toc-number">2.1.</span> <span class="toc-text">GPT</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reformer"><span class="toc-number">2.2.</span> <span class="toc-text">Reformer</span></a></li></ol></li></ol>
        
    </div>
    <style>
        .left-col .switch-btn,
        .left-col .switch-area {
            display: none;
        }
        .toc-level-3 i,
        .toc-level-3 ol {
            display: none !important;
        }
    </style>

    <input type="button" id="tocButton" value="Hide"  title="Show or Hide Table of Contents">

    <script>
        yiliaConfig.toc = ["Hide", "Show", !!"false"];
    </script>



    
<div class="share">
    
        <div class="bdsharebuttonbox">
            <a href="#" class="fa fa-twitter bds_twi" data-cmd="twi" title="分享到推特"></a>
            <a href="#" class="fa fa-weibo bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
            <a href="#" class="fa fa-qq bds_sqq" data-cmd="sqq" title="分享给 QQ 好友"></a>
            <a href="#" class="fa fa-files-o bds_copy" data-cmd="copy" title="复制网址"></a>
            <a href="#" class="fa fa fa-envelope-o bds_mail" data-cmd="mail" title="通过邮件分享"></a>
            <a href="#" class="fa fa-weixin bds_weixin" data-cmd="weixin" title="生成文章二维码"></a>
            <a href="#" class="fa fa-share-alt bds_more" data-cmd="more"></i></a>
        </div>
        <script>
            window._bd_share_config={
                "common":{"bdSnsKey":{},"bdText":"关于 Transformer 的笔记与思考　| Hexo　","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
        </script>
    

    
</div>







    




    <div class="scroll" id="post-nav-button">
        
            <a href="/2025/03/20/FID/" title="Pre: FID">
                <i class="fa fa-angle-left"></i>
            </a>
        

        <a title="Mini Archives"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>

        
            <a href="/2025/03/18/CycleGAN-%E5%AE%9E%E7%8E%B0%E8%8E%AB%E5%A5%88%E9%A3%8E%E6%A0%BC%E7%94%BB%E4%BD%9C%E8%BD%AC%E5%8C%96%E4%B8%BA%E7%9C%9F%E5%AE%9E%E7%85%A7%E7%89%87/" title="Next: CycleGAN 实现莫奈风格画作转化为真实照片">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>

    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2025/03/24/%E9%93%BE%E6%97%B6%E4%BB%A3/">链时代招新</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/23/%E5%A4%9A%E7%94%9F%E6%88%90%E5%99%A8%E6%9E%B6%E6%9E%84/">多生成器架构</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/23/%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96/">层归一化</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/23/Inception-Score/">Inception Score</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/23/hello-world/">Hello World</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/21/%E5%85%B3%E4%BA%8E-GAN-%E5%8F%8A%E5%85%B6%E8%A1%8D%E7%94%9F%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%80%9D%E8%80%83/">关于 GAN 及其衍生模型的笔记与思考</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/20/FID/">FID</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/20/%E5%85%B3%E4%BA%8E-Transformer-%E7%9A%84%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%80%9D%E8%80%83/">关于 Transformer 的笔记与思考</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/18/CycleGAN-%E5%AE%9E%E7%8E%B0%E8%8E%AB%E5%A5%88%E9%A3%8E%E6%A0%BC%E7%94%BB%E4%BD%9C%E8%BD%AC%E5%8C%96%E4%B8%BA%E7%9C%9F%E5%AE%9E%E7%85%A7%E7%89%87/">CycleGAN 实现莫奈风格画作转化为真实照片</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/18/%E5%85%B3%E4%BA%8E-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E7%9A%84%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%80%9D%E8%80%83/">关于 线性回归 的笔记与思考</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/13/WGAN-WGAN-GP-%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E7%94%9F%E6%88%90/">WGAN/WGAN-GP 实现手写数字生成</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/13/Wasserstein%E8%B7%9D%E7%A6%BB/">Wasserstein距离</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/13/KL%E6%95%A3%E5%BA%A6/">KL散度</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/12/ACGAN-%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E7%94%9F%E6%88%90/">ACGAN 实现手写数字生成</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/10/%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96/">批归一化</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/09/DCGAN-%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E7%94%9F%E6%88%90/">DCGAN 实现手写数字生成</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/09/GAN-%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E7%94%9F%E6%88%90/">GAN 实现手写数字生成</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/06/BERT-%E5%AE%9E%E7%8E%B0-IMDb-%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/">BERT 实现 IMDb 情感分类</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/02/%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B/">房价预测</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/03/01/Transformer-%E5%AE%9E%E7%8E%B0-IMDb-%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/">Transformer 实现 IMDb 情感分类</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/27/%E9%A2%84%E6%B5%8B%E7%B3%BB%E5%A4%96%E8%A1%8C%E6%98%9F%E8%BD%A8%E9%81%93%E5%91%A8%E6%9C%9F/">预测系外行星轨道周期</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/23/%E6%B5%8B%E8%AF%95%E5%8D%9A%E5%AE%A2/">测试博客</a></li></ul>




    <script>
        
    </script>
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2025 John Doe
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="A fast, simple &amp; powerful blog framework">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="Another simple and elegant theme for Hexo  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="Site Visitors"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="Page Hits"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





<div class="scroll" id="scroll">
    <a href="#" title="Back to Top"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="Comments"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="Go to Bottom"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>